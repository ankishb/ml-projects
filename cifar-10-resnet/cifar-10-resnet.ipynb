{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "y_train shape: (50000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (50000, 10), (10000, 32, 32, 3), (10000, 10))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Trains a ResNet on the CIFAR10 dataset.\n",
    "\n",
    "ResNet v1:\n",
    "[Deep Residual Learning for Image Recognition\n",
    "](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "\n",
    "ResNet v2:\n",
    "[Identity Mappings in Deep Residual Networks\n",
    "](https://arxiv.org/pdf/1603.05027.pdf)\n",
    "\n",
    "\n",
    "Model|n|200-epoch accuracy|Original paper accuracy |sec/epoch GTX1080Ti\n",
    ":------------|--:|-------:|-----------------------:|---:\n",
    "ResNet20   v1|  3| 92.16 %|                 91.25 %|35\n",
    "ResNet32   v1|  5| 92.46 %|                 92.49 %|50\n",
    "ResNet44   v1|  7| 92.50 %|                 92.83 %|70\n",
    "ResNet56   v1|  9| 92.71 %|                 93.03 %|90\n",
    "ResNet110  v1| 18| 92.65 %|            93.39+-.16 %|165\n",
    "ResNet164  v1| 27|     - %|                 94.07 %|  -\n",
    "ResNet1001 v1|N/A|     - %|                 92.39 %|  -\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Model|n|200-epoch accuracy|Original paper accuracy |sec/epoch GTX1080Ti\n",
    ":------------|--:|-------:|-----------------------:|---:\n",
    "ResNet20   v2|  2|     - %|                     - %|---\n",
    "ResNet32   v2|N/A| NA    %|            NA         %| NA\n",
    "ResNet44   v2|N/A| NA    %|            NA         %| NA\n",
    "ResNet56   v2|  6| 93.01 %|            NA         %|100\n",
    "ResNet110  v2| 12| 93.15 %|            93.63      %|180\n",
    "ResNet164  v2| 18|     - %|            94.54      %|  -\n",
    "ResNet1001 v2|111|     - %|            95.08+-.14 %|  -\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 128  # orig paper trained all networks with batch_size=128\n",
    "epochs = 120\n",
    "data_augmentation = True\n",
    "num_classes = 10\n",
    "\n",
    "# Subtracting pixel mean improves accuracy\n",
    "subtract_pixel_mean = True\n",
    "\n",
    "# Model parameter\n",
    "# ----------------------------------------------------------------------------\n",
    "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
    "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
    "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
    "# ----------------------------------------------------------------------------\n",
    "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
    "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
    "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
    "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
    "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
    "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
    "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
    "# ---------------------------------------------------------------------------\n",
    "n = 3\n",
    "\n",
    "# Model version\n",
    "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
    "version = 2\n",
    "\n",
    "# Computed depth from supplied model parameter n\n",
    "if version == 1:\n",
    "    depth = n * 6 + 2\n",
    "elif version == 2:\n",
    "    depth = n * 9 + 2\n",
    "\n",
    "# Model name, depth and version\n",
    "model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "# The data, split between train and test sets:\n",
    "x_train = np.load('data/x_train.npy')\n",
    "y_train = np.load('data/y_train.npy')\n",
    "x_test = np.load('data/x_test.npy')\n",
    "y_test = np.load('data/y_test.npy')\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "\n",
    "# # Load the CIFAR10 data.\n",
    "# (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# # Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# # Normalize data.\n",
    "# x_train = x_train.astype('float32') / 255\n",
    "# x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# If subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "# # Convert class vectors to binary class matrices.\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ResNet29v2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   272         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 64)   1088        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 64)   1088        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 64)   0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   1040        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 64)   1088        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 64)   0           add_1[0][0]                      \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 16)   1040        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 64)   1088        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 64)   0           add_2[0][0]                      \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 64)   256         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 64)   4160        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 64)   36928       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 128)  8320        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 128)  8320        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 128)  0           conv2d_15[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 64)   8256        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 64)   36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 64)   256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 128)  8320        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 128)  0           add_4[0][0]                      \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 128)  512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 64)   8256        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 64)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 64)   36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 64)   256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 128)  8320        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 128)  0           add_5[0][0]                      \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 128)  512         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 128)    16512       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 128)    512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 128)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 8, 8, 128)    147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 128)    512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 128)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 8, 8, 256)    33024       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 256)    33024       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 256)    0           conv2d_25[0][0]                  \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 256)    1024        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 256)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 128)    32896       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 128)    512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 128)    0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 128)    147584      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 128)    512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 128)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 256)    33024       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 256)    0           add_7[0][0]                      \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 256)    1024        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 256)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 128)    32896       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 128)    512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 128)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 128)    147584      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 128)    512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 128)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 256)    33024       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 256)    0           add_8[0][0]                      \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 256)    1024        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 256)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 256)    0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 256)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           2570        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 849,002\n",
      "Trainable params: 843,786\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# epochs = 120\n",
    "\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet_v2(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    # out-size = (None, 8, 8, 256)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "if version == 2:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "else:\n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "# print(model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 34s 89ms/step - loss: 2.0231 - acc: 0.4488 - val_loss: 1.9742 - val_acc: 0.4512\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.45120, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.001.h5\n",
      "Epoch 2/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 1.5501 - acc: 0.5942 - val_loss: 1.6276 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.45120 to 0.55800, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.002.h5\n",
      "Epoch 3/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 1.3303 - acc: 0.6589 - val_loss: 1.9159 - val_acc: 0.5146\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.55800\n",
      "Epoch 4/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 1.1735 - acc: 0.7105 - val_loss: 1.2833 - val_acc: 0.6591\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.55800 to 0.65910, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.004.h5\n",
      "Epoch 5/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 1.0655 - acc: 0.7434 - val_loss: 1.5374 - val_acc: 0.6184\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.65910\n",
      "Epoch 6/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.9916 - acc: 0.7656 - val_loss: 1.1793 - val_acc: 0.7103\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.65910 to 0.71030, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.006.h5\n",
      "Epoch 7/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.9474 - acc: 0.7771 - val_loss: 1.0662 - val_acc: 0.7404\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.71030 to 0.74040, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.007.h5\n",
      "Epoch 8/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.8864 - acc: 0.7964 - val_loss: 0.9381 - val_acc: 0.7787\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.74040 to 0.77870, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.008.h5\n",
      "Epoch 9/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.8498 - acc: 0.8052 - val_loss: 1.3200 - val_acc: 0.6958\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.77870\n",
      "Epoch 10/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.8172 - acc: 0.8136 - val_loss: 1.0900 - val_acc: 0.7252\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.77870\n",
      "Epoch 11/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.7897 - acc: 0.8228 - val_loss: 0.9835 - val_acc: 0.7690\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.77870\n",
      "Epoch 12/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.7683 - acc: 0.8301 - val_loss: 1.2966 - val_acc: 0.6845\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.77870\n",
      "Epoch 13/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.7366 - acc: 0.8395 - val_loss: 1.1556 - val_acc: 0.7196\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.77870\n",
      "Epoch 14/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.7224 - acc: 0.8416 - val_loss: 0.9442 - val_acc: 0.7775\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.77870\n",
      "Epoch 15/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.7062 - acc: 0.8448 - val_loss: 0.9017 - val_acc: 0.7809\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.77870 to 0.78090, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.015.h5\n",
      "Epoch 16/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.6881 - acc: 0.8500 - val_loss: 1.0154 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.78090\n",
      "Epoch 17/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.6699 - acc: 0.8566 - val_loss: 0.9362 - val_acc: 0.7880\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.78090 to 0.78800, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.017.h5\n",
      "Epoch 18/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.6606 - acc: 0.8581 - val_loss: 0.8456 - val_acc: 0.8025\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.78800 to 0.80250, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.018.h5\n",
      "Epoch 19/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.6377 - acc: 0.8672 - val_loss: 0.8425 - val_acc: 0.8067\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.80250 to 0.80670, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.019.h5\n",
      "Epoch 20/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.6362 - acc: 0.8644 - val_loss: 0.9277 - val_acc: 0.7792\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.80670\n",
      "Epoch 21/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.6241 - acc: 0.8694 - val_loss: 1.0641 - val_acc: 0.7435\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.80670\n",
      "Epoch 22/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.6132 - acc: 0.8730 - val_loss: 0.9090 - val_acc: 0.7824\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.80670\n",
      "Epoch 23/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.6059 - acc: 0.8739 - val_loss: 0.8138 - val_acc: 0.8124\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.80670 to 0.81240, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.023.h5\n",
      "Epoch 24/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.5958 - acc: 0.8774 - val_loss: 0.7877 - val_acc: 0.8151\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.81240 to 0.81510, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.024.h5\n",
      "Epoch 25/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.5861 - acc: 0.8806 - val_loss: 0.8818 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.81510\n",
      "Epoch 26/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.5798 - acc: 0.8814 - val_loss: 0.7625 - val_acc: 0.8296\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.81510 to 0.82960, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.026.h5\n",
      "Epoch 27/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.5672 - acc: 0.8868 - val_loss: 0.8768 - val_acc: 0.8006\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.82960\n",
      "Epoch 28/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.5651 - acc: 0.8863 - val_loss: 0.8602 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.82960\n",
      "Epoch 29/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.5553 - acc: 0.8898 - val_loss: 0.8547 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.82960\n",
      "Epoch 30/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.5396 - acc: 0.8935 - val_loss: 0.7345 - val_acc: 0.8344\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.82960 to 0.83440, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.030.h5\n",
      "Epoch 31/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.5435 - acc: 0.8941 - val_loss: 0.6723 - val_acc: 0.8476\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.83440 to 0.84760, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.031.h5\n",
      "Epoch 32/120\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 25s 66ms/step - loss: 0.5417 - acc: 0.8935 - val_loss: 0.8371 - val_acc: 0.8049\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.84760\n",
      "Epoch 33/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.5276 - acc: 0.8969 - val_loss: 0.8034 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.84760\n",
      "Epoch 34/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.5283 - acc: 0.8992 - val_loss: 0.7692 - val_acc: 0.8251\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.84760\n",
      "Epoch 35/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.5181 - acc: 0.9022 - val_loss: 0.7207 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.84760\n",
      "Epoch 36/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.5216 - acc: 0.9002 - val_loss: 0.8878 - val_acc: 0.7994\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.84760\n",
      "Epoch 37/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.5143 - acc: 0.8997 - val_loss: 0.7090 - val_acc: 0.8431\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.84760\n",
      "Epoch 38/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.5059 - acc: 0.9055 - val_loss: 0.9223 - val_acc: 0.7913\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.84760\n",
      "Epoch 39/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.5023 - acc: 0.9047 - val_loss: 0.6986 - val_acc: 0.8523\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.84760 to 0.85230, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.039.h5\n",
      "Epoch 40/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.5020 - acc: 0.9053 - val_loss: 0.8061 - val_acc: 0.8371\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.85230\n",
      "Epoch 41/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4978 - acc: 0.9067 - val_loss: 0.6823 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.85230\n",
      "Epoch 42/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.4919 - acc: 0.9084 - val_loss: 1.0096 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.85230\n",
      "Epoch 43/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4913 - acc: 0.9076 - val_loss: 0.8086 - val_acc: 0.8213\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.85230\n",
      "Epoch 44/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.4846 - acc: 0.9104 - val_loss: 0.7208 - val_acc: 0.8428\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.85230\n",
      "Epoch 45/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.4786 - acc: 0.9130 - val_loss: 0.7428 - val_acc: 0.8393\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.85230\n",
      "Epoch 46/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4862 - acc: 0.9093 - val_loss: 0.6757 - val_acc: 0.8547\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.85230 to 0.85470, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.046.h5\n",
      "Epoch 47/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.4742 - acc: 0.9133 - val_loss: 0.7973 - val_acc: 0.8170\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.85470\n",
      "Epoch 48/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.4756 - acc: 0.9123 - val_loss: 0.9140 - val_acc: 0.7965\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.85470\n",
      "Epoch 49/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4713 - acc: 0.9146 - val_loss: 0.7331 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.85470\n",
      "Epoch 50/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.4693 - acc: 0.9147 - val_loss: 0.7359 - val_acc: 0.8444\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.85470\n",
      "Epoch 51/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.4603 - acc: 0.9185 - val_loss: 0.7563 - val_acc: 0.8361\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.85470\n",
      "Epoch 52/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.4615 - acc: 0.9166 - val_loss: 0.8650 - val_acc: 0.8074\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.85470\n",
      "Epoch 53/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.4611 - acc: 0.9170 - val_loss: 0.8236 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.85470\n",
      "Epoch 54/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.4587 - acc: 0.9183 - val_loss: 0.7134 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.85470\n",
      "Epoch 55/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.4568 - acc: 0.9184 - val_loss: 0.7629 - val_acc: 0.8431\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.85470\n",
      "Epoch 56/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.4532 - acc: 0.9191 - val_loss: 0.7447 - val_acc: 0.8419\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.85470\n",
      "Epoch 57/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.4500 - acc: 0.9222 - val_loss: 0.6320 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00057: val_acc improved from 0.85470 to 0.86150, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.057.h5\n",
      "Epoch 58/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4436 - acc: 0.9228 - val_loss: 0.7885 - val_acc: 0.8330\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.86150\n",
      "Epoch 59/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4504 - acc: 0.9200 - val_loss: 0.8355 - val_acc: 0.8185\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.86150\n",
      "Epoch 60/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.4482 - acc: 0.9208 - val_loss: 0.6649 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.86150\n",
      "Epoch 61/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.4398 - acc: 0.9238 - val_loss: 0.8182 - val_acc: 0.8320\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.86150\n",
      "Epoch 62/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4438 - acc: 0.9228 - val_loss: 0.7186 - val_acc: 0.8536\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.86150\n",
      "Epoch 63/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.4369 - acc: 0.9244 - val_loss: 0.8618 - val_acc: 0.8237\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.86150\n",
      "Epoch 64/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.4385 - acc: 0.9242 - val_loss: 0.7842 - val_acc: 0.8363\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.86150\n",
      "Epoch 65/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.4342 - acc: 0.9247 - val_loss: 0.6446 - val_acc: 0.8728\n",
      "\n",
      "Epoch 00065: val_acc improved from 0.86150 to 0.87280, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.065.h5\n",
      "Epoch 66/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4360 - acc: 0.9263 - val_loss: 0.6585 - val_acc: 0.8626\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.87280\n",
      "Epoch 67/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.4308 - acc: 0.9260 - val_loss: 0.8592 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.87280\n",
      "Epoch 68/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.4313 - acc: 0.9258 - val_loss: 0.6650 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.87280\n",
      "Epoch 69/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4244 - acc: 0.9281 - val_loss: 0.6507 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.87280\n",
      "Epoch 70/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.4290 - acc: 0.9267 - val_loss: 0.7537 - val_acc: 0.8420\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.87280\n",
      "Epoch 71/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4267 - acc: 0.9272 - val_loss: 0.7485 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.87280\n",
      "Epoch 72/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.4248 - acc: 0.9271 - val_loss: 0.6327 - val_acc: 0.8684\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.87280\n",
      "Epoch 73/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4262 - acc: 0.9257 - val_loss: 0.9252 - val_acc: 0.8078\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.87280\n",
      "Epoch 74/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4215 - acc: 0.9296 - val_loss: 0.6783 - val_acc: 0.8587\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.87280\n",
      "Epoch 75/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4111 - acc: 0.9313 - val_loss: 0.7001 - val_acc: 0.8602\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.87280\n",
      "Epoch 76/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4196 - acc: 0.9293 - val_loss: 0.6233 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.87280\n",
      "Epoch 77/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4151 - acc: 0.9298 - val_loss: 0.6825 - val_acc: 0.8607\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.87280\n",
      "Epoch 78/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.4172 - acc: 0.9302 - val_loss: 0.7525 - val_acc: 0.8433\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.87280\n",
      "Epoch 79/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.4102 - acc: 0.9317 - val_loss: 0.7659 - val_acc: 0.8478\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.87280\n",
      "Epoch 80/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.4147 - acc: 0.9296 - val_loss: 0.7856 - val_acc: 0.8399\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.87280\n",
      "Epoch 81/120\n",
      "Learning rate:  0.001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.4099 - acc: 0.9307 - val_loss: 0.7356 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.87280\n",
      "Epoch 82/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.3429 - acc: 0.9559 - val_loss: 0.5045 - val_acc: 0.9098\n",
      "\n",
      "Epoch 00082: val_acc improved from 0.87280 to 0.90980, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.082.h5\n",
      "Epoch 83/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.3130 - acc: 0.9671 - val_loss: 0.5029 - val_acc: 0.9105\n",
      "\n",
      "Epoch 00083: val_acc improved from 0.90980 to 0.91050, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.083.h5\n",
      "Epoch 84/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2998 - acc: 0.9711 - val_loss: 0.4994 - val_acc: 0.9130\n",
      "\n",
      "Epoch 00084: val_acc improved from 0.91050 to 0.91300, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.084.h5\n",
      "Epoch 85/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2916 - acc: 0.9726 - val_loss: 0.4986 - val_acc: 0.9133\n",
      "\n",
      "Epoch 00085: val_acc improved from 0.91300 to 0.91330, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.085.h5\n",
      "Epoch 86/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2795 - acc: 0.9770 - val_loss: 0.4895 - val_acc: 0.9181\n",
      "\n",
      "Epoch 00086: val_acc improved from 0.91330 to 0.91810, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.086.h5\n",
      "Epoch 87/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2742 - acc: 0.9771 - val_loss: 0.4917 - val_acc: 0.9161\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.91810\n",
      "Epoch 88/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2701 - acc: 0.9780 - val_loss: 0.5014 - val_acc: 0.9129\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.91810\n",
      "Epoch 89/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2613 - acc: 0.9809 - val_loss: 0.5048 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.91810\n",
      "Epoch 90/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2577 - acc: 0.9811 - val_loss: 0.4955 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.91810\n",
      "Epoch 91/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2539 - acc: 0.9814 - val_loss: 0.5086 - val_acc: 0.9127\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.91810\n",
      "Epoch 92/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.2482 - acc: 0.9822 - val_loss: 0.4997 - val_acc: 0.9154\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.91810\n",
      "Epoch 93/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2442 - acc: 0.9828 - val_loss: 0.4934 - val_acc: 0.9146\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.91810\n",
      "Epoch 94/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2414 - acc: 0.9840 - val_loss: 0.5045 - val_acc: 0.9152\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.91810\n",
      "Epoch 95/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2348 - acc: 0.9851 - val_loss: 0.4944 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00095: val_acc improved from 0.91810 to 0.91870, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.095.h5\n",
      "Epoch 96/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2337 - acc: 0.9843 - val_loss: 0.4983 - val_acc: 0.9153\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.91870\n",
      "Epoch 97/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.2297 - acc: 0.9854 - val_loss: 0.5084 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.91870\n",
      "Epoch 98/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2259 - acc: 0.9859 - val_loss: 0.5119 - val_acc: 0.9122\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.91870\n",
      "Epoch 99/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.2231 - acc: 0.9869 - val_loss: 0.5094 - val_acc: 0.9153\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.91870\n",
      "Epoch 100/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.2191 - acc: 0.9872 - val_loss: 0.4954 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00100: val_acc improved from 0.91870 to 0.91920, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.100.h5\n",
      "Epoch 101/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2169 - acc: 0.9875 - val_loss: 0.5082 - val_acc: 0.9151\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.91920\n",
      "Epoch 102/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2138 - acc: 0.9876 - val_loss: 0.5053 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.91920\n",
      "Epoch 103/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.2101 - acc: 0.9886 - val_loss: 0.5030 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00103: val_acc improved from 0.91920 to 0.91980, saving model to /home/ankish1/ankish_save/conditional-feature/saved_models/cifar10_ResNet29v2_model.103.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2076 - acc: 0.9893 - val_loss: 0.5060 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.91980\n",
      "Epoch 105/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.2068 - acc: 0.9885 - val_loss: 0.5086 - val_acc: 0.9162\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.91980\n",
      "Epoch 106/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.2033 - acc: 0.9888 - val_loss: 0.5114 - val_acc: 0.9159\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.91980\n",
      "Epoch 107/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2020 - acc: 0.9891 - val_loss: 0.5222 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.91980\n",
      "Epoch 108/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.2007 - acc: 0.9891 - val_loss: 0.5005 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.91980\n",
      "Epoch 109/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.1966 - acc: 0.9900 - val_loss: 0.5066 - val_acc: 0.9184\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.91980\n",
      "Epoch 110/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.1942 - acc: 0.9909 - val_loss: 0.5253 - val_acc: 0.9140\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.91980\n",
      "Epoch 111/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.1956 - acc: 0.9896 - val_loss: 0.5183 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.91980\n",
      "Epoch 112/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.1899 - acc: 0.9911 - val_loss: 0.5156 - val_acc: 0.9157\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.91980\n",
      "Epoch 113/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.1902 - acc: 0.9906 - val_loss: 0.5233 - val_acc: 0.9127\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.91980\n",
      "Epoch 114/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.1855 - acc: 0.9920 - val_loss: 0.5068 - val_acc: 0.9175\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.91980\n",
      "Epoch 115/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.1870 - acc: 0.9910 - val_loss: 0.5000 - val_acc: 0.9189\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.91980\n",
      "Epoch 116/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 66ms/step - loss: 0.1853 - acc: 0.9905 - val_loss: 0.5295 - val_acc: 0.9133\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.91980\n",
      "Epoch 117/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.1818 - acc: 0.9915 - val_loss: 0.5211 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.91980\n",
      "Epoch 118/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.1808 - acc: 0.9915 - val_loss: 0.5221 - val_acc: 0.9172\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.91980\n",
      "Epoch 119/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 24s 65ms/step - loss: 0.1786 - acc: 0.9922 - val_loss: 0.5110 - val_acc: 0.9177\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.91980\n",
      "Epoch 120/120\n",
      "Learning rate:  0.0001\n",
      "375/375 [==============================] - 25s 65ms/step - loss: 0.1771 - acc: 0.9923 - val_loss: 0.5226 - val_acc: 0.9143\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.91980\n"
     ]
    }
   ],
   "source": [
    "# Prepare model model saving directory.\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "\n",
    "# Run training, with or without data augmentation.\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # epsilon for ZCA whitening\n",
    "        zca_epsilon=1e-06,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # set range for random shear\n",
    "        shear_range=0.,\n",
    "        # set range for random zoom\n",
    "        zoom_range=0.,\n",
    "        # set range for random channel shifts\n",
    "        channel_shift_range=0.,\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        # value used for fill_mode = \"constant\"\n",
    "        cval=0.,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False,\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        epochs=epochs, verbose=1, workers=4,steps_per_epoch=int(1500/4),\n",
    "                        callbacks=callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.4488333333333333,\n",
       "  0.5942817817817818,\n",
       "  0.6588463463264582,\n",
       "  0.7104813146479814,\n",
       "  0.7433683683484803,\n",
       "  0.7656406406605287,\n",
       "  0.7771312980044075,\n",
       "  0.7963588588787469,\n",
       "  0.8051801802000683,\n",
       "  0.8135844177709726,\n",
       "  0.8228436769705675,\n",
       "  0.8301426426625308,\n",
       "  0.8394853186122091,\n",
       "  0.8415498831966618,\n",
       "  0.8448031364896912,\n",
       "  0.8500583917051703,\n",
       "  0.8566691691890572,\n",
       "  0.8581081081081081,\n",
       "  0.8672213880746095,\n",
       "  0.8644269269269269,\n",
       "  0.869411077724523,\n",
       "  0.8730814147083052,\n",
       "  0.8739364364563246,\n",
       "  0.8773982315648983,\n",
       "  0.8806041666666666,\n",
       "  0.8813813814211576,\n",
       "  0.8868034701169154,\n",
       "  0.8863238238238238,\n",
       "  0.8898481815148482,\n",
       "  0.8934976643707738,\n",
       "  0.8940815815815816,\n",
       "  0.8935602268935602,\n",
       "  0.8969177511043058,\n",
       "  0.8992534200668654,\n",
       "  0.90219386056697,\n",
       "  0.9001918585053038,\n",
       "  0.8997122122321003,\n",
       "  0.9054888221554889,\n",
       "  0.9046129462398368,\n",
       "  0.9053636970104756,\n",
       "  0.9066983650118102,\n",
       "  0.9085335335335335,\n",
       "  0.9076159492428397,\n",
       "  0.910472972972973,\n",
       "  0.9129754754754755,\n",
       "  0.9092842843041724,\n",
       "  0.9133299966235537,\n",
       "  0.9122872872673992,\n",
       "  0.914625,\n",
       "  0.9146646647044409,\n",
       "  0.9184392725661631,\n",
       "  0.9165832499364713,\n",
       "  0.9170003336471122,\n",
       "  0.9183350017081112,\n",
       "  0.9184809809412048,\n",
       "  0.9190231898366351,\n",
       "  0.9221930263994692,\n",
       "  0.9228812145279931,\n",
       "  0.9200450450649331,\n",
       "  0.9208166499435404,\n",
       "  0.9237779446112779,\n",
       "  0.9227769435705008,\n",
       "  0.9244035702369036,\n",
       "  0.9242158825293277,\n",
       "  0.9245912579444794,\n",
       "  0.9263013012615251,\n",
       "  0.9260719051987957,\n",
       "  0.9258425092156187,\n",
       "  0.9281364698031365,\n",
       "  0.9266558224891558,\n",
       "  0.927218885572107,\n",
       "  0.9271146146345027,\n",
       "  0.9257173840706054,\n",
       "  0.9295833333333333,\n",
       "  0.9312854520790093,\n",
       "  0.9293043042645281,\n",
       "  0.9298256589724375,\n",
       "  0.9301593259926594,\n",
       "  0.9317025358692025,\n",
       "  0.929617117097229,\n",
       "  0.9306598264931598,\n",
       "  0.9558516850581279,\n",
       "  0.9671338004472457,\n",
       "  0.9710960960563199,\n",
       "  0.9726184517453422,\n",
       "  0.9769978312042741,\n",
       "  0.9770395395395396,\n",
       "  0.9780196863530197,\n",
       "  0.9808558558558559,\n",
       "  0.9810643977708406,\n",
       "  0.981398064731398,\n",
       "  0.9821696696298935,\n",
       "  0.9828578578180817,\n",
       "  0.9839631297964632,\n",
       "  0.9851101100703339,\n",
       "  0.984296796836573,\n",
       "  0.98544377715022,\n",
       "  0.9858958333333333,\n",
       "  0.9869244244443125,\n",
       "  0.9871746746746747,\n",
       "  0.9874874875073756,\n",
       "  0.9876334668001334,\n",
       "  0.9886344678409107,\n",
       "  0.9893018018216899,\n",
       "  0.9884884884884885,\n",
       "  0.9888013013013013,\n",
       "  0.9891349683414111,\n",
       "  0.9891349683414111,\n",
       "  0.9900525525923287,\n",
       "  0.9908867200533867,\n",
       "  0.989572906259461,\n",
       "  0.9910744077808507,\n",
       "  0.9905739072405739,\n",
       "  0.9920337003869218,\n",
       "  0.9909909909512148,\n",
       "  0.9904696363427459,\n",
       "  0.9914706373039707,\n",
       "  0.9914914915113796,\n",
       "  0.9921796796796797,\n",
       "  0.9922839506172839],\n",
       " 'loss': [2.0231266508102417,\n",
       "  1.5497367664858386,\n",
       "  1.3303481756467441,\n",
       "  1.1736083104684427,\n",
       "  1.0655886849284688,\n",
       "  0.9914397168724307,\n",
       "  0.9474529113855448,\n",
       "  0.8864199815092383,\n",
       "  0.8498347134720615,\n",
       "  0.8172355235557697,\n",
       "  0.7895827219810014,\n",
       "  0.7681525971120224,\n",
       "  0.7366021502045819,\n",
       "  0.7224521511468961,\n",
       "  0.7062804146850352,\n",
       "  0.6879846283225644,\n",
       "  0.6697370705303846,\n",
       "  0.6605735525394385,\n",
       "  0.6374713413151336,\n",
       "  0.6362021010360361,\n",
       "  0.6240757527174773,\n",
       "  0.6132045548519851,\n",
       "  0.6058597483354924,\n",
       "  0.5958137689410029,\n",
       "  0.5860702783266704,\n",
       "  0.5797494010086811,\n",
       "  0.5671856861572724,\n",
       "  0.5650684402551419,\n",
       "  0.5552364864029564,\n",
       "  0.5396863223116598,\n",
       "  0.5434242277333129,\n",
       "  0.5418283265055439,\n",
       "  0.5274668014721748,\n",
       "  0.5281622003307731,\n",
       "  0.5181165817263607,\n",
       "  0.5216288224712864,\n",
       "  0.5142414416119699,\n",
       "  0.5057277343135537,\n",
       "  0.5024204461086103,\n",
       "  0.5019038459401072,\n",
       "  0.4977572784648166,\n",
       "  0.49172022593669745,\n",
       "  0.4912972884711958,\n",
       "  0.4845662881900837,\n",
       "  0.4786631989506908,\n",
       "  0.4862100301204143,\n",
       "  0.47416871849759484,\n",
       "  0.4754818728418003,\n",
       "  0.4713224542140961,\n",
       "  0.4693025442950917,\n",
       "  0.46036345040078236,\n",
       "  0.46147588405523216,\n",
       "  0.4611176236255749,\n",
       "  0.45864261774806764,\n",
       "  0.4567451573706962,\n",
       "  0.45322460437322165,\n",
       "  0.44998815687529276,\n",
       "  0.4434523448213801,\n",
       "  0.45046514905131496,\n",
       "  0.44812319423502434,\n",
       "  0.4398257357798937,\n",
       "  0.4437837427582071,\n",
       "  0.4367986198938565,\n",
       "  0.4384496201763242,\n",
       "  0.4343285029993321,\n",
       "  0.43597376042300157,\n",
       "  0.43080831523692564,\n",
       "  0.4313259588944184,\n",
       "  0.42437936814578325,\n",
       "  0.4289479117951156,\n",
       "  0.4267620987243003,\n",
       "  0.424678082978443,\n",
       "  0.4262324289098994,\n",
       "  0.4214605604012807,\n",
       "  0.4110066037815254,\n",
       "  0.4196388314058274,\n",
       "  0.4150629488078204,\n",
       "  0.41725898090306224,\n",
       "  0.41012603016666543,\n",
       "  0.41456435712767237,\n",
       "  0.40985050033759307,\n",
       "  0.34292668703798057,\n",
       "  0.3129795679140775,\n",
       "  0.2997444467441933,\n",
       "  0.2916085875487781,\n",
       "  0.27947378234144127,\n",
       "  0.27427045807545686,\n",
       "  0.27012095786568957,\n",
       "  0.261375859509041,\n",
       "  0.2577050145681914,\n",
       "  0.2538962662965805,\n",
       "  0.24815986236612678,\n",
       "  0.24407486636956055,\n",
       "  0.24141440968394956,\n",
       "  0.23479293875985438,\n",
       "  0.23366904577932082,\n",
       "  0.22974839499782712,\n",
       "  0.22590422574679056,\n",
       "  0.2230546736780866,\n",
       "  0.21910535458886787,\n",
       "  0.21690342228512866,\n",
       "  0.21384692458081014,\n",
       "  0.21001851389636428,\n",
       "  0.2075573479309852,\n",
       "  0.20681907417060616,\n",
       "  0.20331092966390443,\n",
       "  0.20187531167521333,\n",
       "  0.2006662551774794,\n",
       "  0.1965830432581114,\n",
       "  0.1941932456559804,\n",
       "  0.1956178919052656,\n",
       "  0.18992957342534134,\n",
       "  0.19019378520346977,\n",
       "  0.185520574863171,\n",
       "  0.18693840613574397,\n",
       "  0.1852374977376567,\n",
       "  0.1818454671311784,\n",
       "  0.18077087838887612,\n",
       "  0.17864894101748596,\n",
       "  0.17705263720479136],\n",
       " 'lr': [0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0003162278,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0003162278,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0003162278,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0003162278,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0003162278,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0003162278,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0003162278,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0003162278,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0003162278,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0003162278,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  3.1622774e-05,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  3.1622774e-05,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  3.1622774e-05,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  3.1622774e-05,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  3.1622774e-05,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  3.1622774e-05,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04,\n",
       "  1e-04],\n",
       " 'val_acc': [0.4512,\n",
       "  0.558,\n",
       "  0.5146,\n",
       "  0.6591,\n",
       "  0.6184,\n",
       "  0.7103,\n",
       "  0.7404,\n",
       "  0.7787,\n",
       "  0.6958,\n",
       "  0.7252,\n",
       "  0.769,\n",
       "  0.6845,\n",
       "  0.7196,\n",
       "  0.7775,\n",
       "  0.7809,\n",
       "  0.7655,\n",
       "  0.788,\n",
       "  0.8025,\n",
       "  0.8067,\n",
       "  0.7792,\n",
       "  0.7435,\n",
       "  0.7824,\n",
       "  0.8124,\n",
       "  0.8151,\n",
       "  0.7925,\n",
       "  0.8296,\n",
       "  0.8006,\n",
       "  0.8042,\n",
       "  0.8033,\n",
       "  0.8344,\n",
       "  0.8476,\n",
       "  0.8049,\n",
       "  0.8144,\n",
       "  0.8251,\n",
       "  0.8383,\n",
       "  0.7994,\n",
       "  0.8431,\n",
       "  0.7913,\n",
       "  0.8523,\n",
       "  0.8371,\n",
       "  0.8507,\n",
       "  0.7656,\n",
       "  0.8213,\n",
       "  0.8428,\n",
       "  0.8393,\n",
       "  0.8547,\n",
       "  0.817,\n",
       "  0.7965,\n",
       "  0.8425,\n",
       "  0.8444,\n",
       "  0.8361,\n",
       "  0.8074,\n",
       "  0.8106,\n",
       "  0.8417,\n",
       "  0.8431,\n",
       "  0.8419,\n",
       "  0.8615,\n",
       "  0.833,\n",
       "  0.8185,\n",
       "  0.8615,\n",
       "  0.832,\n",
       "  0.8536,\n",
       "  0.8237,\n",
       "  0.8363,\n",
       "  0.8728,\n",
       "  0.8626,\n",
       "  0.8167,\n",
       "  0.859,\n",
       "  0.8671,\n",
       "  0.842,\n",
       "  0.85,\n",
       "  0.8684,\n",
       "  0.8078,\n",
       "  0.8587,\n",
       "  0.8602,\n",
       "  0.8725,\n",
       "  0.8607,\n",
       "  0.8433,\n",
       "  0.8478,\n",
       "  0.8399,\n",
       "  0.8512,\n",
       "  0.9098,\n",
       "  0.9105,\n",
       "  0.913,\n",
       "  0.9133,\n",
       "  0.9181,\n",
       "  0.9161,\n",
       "  0.9129,\n",
       "  0.9135,\n",
       "  0.9154,\n",
       "  0.9127,\n",
       "  0.9154,\n",
       "  0.9146,\n",
       "  0.9152,\n",
       "  0.9187,\n",
       "  0.9153,\n",
       "  0.9136,\n",
       "  0.9122,\n",
       "  0.9153,\n",
       "  0.9192,\n",
       "  0.9151,\n",
       "  0.9158,\n",
       "  0.9198,\n",
       "  0.9166,\n",
       "  0.9162,\n",
       "  0.9159,\n",
       "  0.9141,\n",
       "  0.9198,\n",
       "  0.9184,\n",
       "  0.914,\n",
       "  0.9145,\n",
       "  0.9157,\n",
       "  0.9127,\n",
       "  0.9175,\n",
       "  0.9189,\n",
       "  0.9133,\n",
       "  0.9137,\n",
       "  0.9172,\n",
       "  0.9177,\n",
       "  0.9143],\n",
       " 'val_loss': [1.9742008758544922,\n",
       "  1.6275889318466186,\n",
       "  1.9159384214401245,\n",
       "  1.2832901262283325,\n",
       "  1.5374419471740723,\n",
       "  1.1792990692138672,\n",
       "  1.0662222124099732,\n",
       "  0.9380812585830689,\n",
       "  1.32002953414917,\n",
       "  1.0899700588226318,\n",
       "  0.9834986033439637,\n",
       "  1.2966240930557251,\n",
       "  1.1556272840499877,\n",
       "  0.9442348690986633,\n",
       "  0.9016872562408448,\n",
       "  1.0153935359954833,\n",
       "  0.9362041946411133,\n",
       "  0.8456424606323242,\n",
       "  0.8425469201087952,\n",
       "  0.9276600670814514,\n",
       "  1.0641323935508729,\n",
       "  0.9090133412361145,\n",
       "  0.8138298846244812,\n",
       "  0.7877408852577209,\n",
       "  0.8818083012580872,\n",
       "  0.7625448427200318,\n",
       "  0.8768039169311523,\n",
       "  0.8602410598754883,\n",
       "  0.854733565235138,\n",
       "  0.7344560872077942,\n",
       "  0.6723265970230102,\n",
       "  0.837104174232483,\n",
       "  0.8033629453659058,\n",
       "  0.7691612268924714,\n",
       "  0.7206518524169921,\n",
       "  0.8877853693962097,\n",
       "  0.7089952816963195,\n",
       "  0.9222603055477142,\n",
       "  0.6986288426399231,\n",
       "  0.8060817509174347,\n",
       "  0.682289764213562,\n",
       "  1.009591718196869,\n",
       "  0.8086231889724731,\n",
       "  0.7207880823135376,\n",
       "  0.7428314752578735,\n",
       "  0.6757287113189697,\n",
       "  0.7973006946563721,\n",
       "  0.9140133244514466,\n",
       "  0.7330590837478638,\n",
       "  0.7358919492721557,\n",
       "  0.7562826734542847,\n",
       "  0.8649570950508118,\n",
       "  0.8236403618812561,\n",
       "  0.7133600660324096,\n",
       "  0.7629197607994079,\n",
       "  0.7446933191299439,\n",
       "  0.632029418182373,\n",
       "  0.7885135999202728,\n",
       "  0.8355123805999756,\n",
       "  0.6648637441635132,\n",
       "  0.8181772772789001,\n",
       "  0.7185760165214539,\n",
       "  0.8618320404052734,\n",
       "  0.7842306435108185,\n",
       "  0.6446206084251404,\n",
       "  0.6585146026611328,\n",
       "  0.8591756593704224,\n",
       "  0.6650412045478821,\n",
       "  0.6506941736221313,\n",
       "  0.7537412568092346,\n",
       "  0.7485041986465454,\n",
       "  0.6326821261882782,\n",
       "  0.9251513829231263,\n",
       "  0.6783160581588745,\n",
       "  0.7000644751548767,\n",
       "  0.6232710853099823,\n",
       "  0.6824563697814942,\n",
       "  0.7524777807712555,\n",
       "  0.7658678905487061,\n",
       "  0.7856441387176514,\n",
       "  0.7356035587787628,\n",
       "  0.5044750347137451,\n",
       "  0.5028979661941528,\n",
       "  0.49940107769966124,\n",
       "  0.49857479887008666,\n",
       "  0.48946366944313047,\n",
       "  0.491659680891037,\n",
       "  0.5013817922592163,\n",
       "  0.5047936882019043,\n",
       "  0.49550390572547914,\n",
       "  0.5085592190742493,\n",
       "  0.4996910503387451,\n",
       "  0.493394984292984,\n",
       "  0.5045426853656769,\n",
       "  0.4943679696559906,\n",
       "  0.4983192997455597,\n",
       "  0.5083797122955322,\n",
       "  0.511932479095459,\n",
       "  0.5093541767120361,\n",
       "  0.4954479258060455,\n",
       "  0.5081724985599518,\n",
       "  0.5053313967227936,\n",
       "  0.5030083994865417,\n",
       "  0.5060481800079346,\n",
       "  0.508609428358078,\n",
       "  0.511354559803009,\n",
       "  0.522188205909729,\n",
       "  0.5005037321090698,\n",
       "  0.5066369156837464,\n",
       "  0.5253071592330932,\n",
       "  0.5183402532577515,\n",
       "  0.5156153860092163,\n",
       "  0.5233386806488037,\n",
       "  0.5068309512138367,\n",
       "  0.5000041328430176,\n",
       "  0.5294980830192566,\n",
       "  0.521079461479187,\n",
       "  0.5220742968082428,\n",
       "  0.510963489484787,\n",
       "  0.5226242725849152]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f96782667d0>,\n",
       " <matplotlib.lines.Line2D at 0x7f9678222e10>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(16,4))\n",
    "\n",
    "ax[0].plot(range(120),model.history.history['loss'], range(120),model.history.history['val_loss'])\n",
    "ax[1].plot(range(120),model.history.history['acc'], range(120),model.history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f967812b1d0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztnXu0HNV15r9dVX2vkHQFeoGEBEgYgSSwMSyZ2HHiEDMOAjtWVoLHYjwOTvCQlcByxs6aGMZejkPCSkgywckEnGEMtuNxLBhsbMWWTWxI/Jhg4AI2RggZRQIjkEAggSRAV/3Y80c9+lR1Vdc+dft2V3ft31pa6q6uU4+uvmef8+3HIWaGoiiKomThDPoCFEVRlHKjhkJRFEXpihoKRVEUpStqKBRFUZSuqKFQFEVRuqKGQlEURemKGgpFURSlK2ooFEVRlK6ooVAURVG64g36AnrBokWLeMWKFYO+DEVRlKHiwQcffIGZF+ftNxKGYsWKFZicnBz0ZSiKogwVRPSUZD+VnhRFUZSuqKFQFEVRuqKGQlEURemKGgpFURSlK2ooFEVRlK6IDAURrSei7US0g4iuTvl8nIhuCz6/j4hWGJ9dE2zfTkQXGttvJaLniejRxLEWENG3ieiJ4P/5xW9PURRFmS65hoKIXAA3ArgIwFoAlxLR2sRulwM4wMynAbgBwPVB27UANgI4E8B6ADcFxwOAzwXbklwN4G5mXgXg7uC9oiiKMiAkeRTnAdjBzDsBgIg2AdgA4DFjnw0APhm8vgPA3xERBds3MfMUgF1EtCM43r3M/D1z5pE41vnB688D+FcAHxXfkQV3Prwbu/a9MhOHzmTtifOw/qyl0fvnDh7BpvufRrPV6ut19INfPH0x3rRiQfR++95D+MYjz2buT0T4jXOX4+SFs6Nt3/3pPjz45P7MNuM1F7/5llMwMasWbbvtgZ/hmQOvZbZZNDGO97/5FPg/UeBIvYnP/duTeHWqkdnmrGXH4lfOXBK93/Pya7jtgafRauUvJTx73MNvvXUFxj1/jMTM+Id7n8KLh6cy2yxfMBv/cd1J0fuDR+r4wr1PYarezGyzbsUCvO30du7Uky+8gq88/AzQg+WOzz1lPs4/4/hpH0cZTiSGYhmAp433uwH8XNY+zNwgopcBLAy2/zDRdlnO+U5g5j3BsfYQUeqvk4iuAHAFAJx88smC2+jkn368B/+y/flCbYvADMyfXYsZiq/96Bnc8J2fAgCCfmskYAbu27Uft/3OW6JtN39vJ7780O7M+2T2O+1rLl4Tbbv2n7bi3/e9ktom7P9WLpqDi1/vf6eHjtTx0S//BED69xm2uWDNCVh23DEAgAefOoA//+bjXdssnhiPGYqvPPQMPvWdJzLbJM939vLj8JbXLQQA7D7wGv5o89bca3zXG5Zi9pj/J/rd7fvwl3dt79rm9BPm4p9P/6Vo2//54VP4zA92Tft3xQycuniOGooKIzEUaT+z5BAlax9J20Iw880AbgaAdevWFTrmrR94Uy8uRcyffv0xfOn+n8W2TdX9mcQT112Emjs6sQXvv+U+HE6M0KcaTZy6eA7u+YPzU9u8/pN3YarRSrRp4dfPWYa/fu8bO/bfue8w3v4/voujRpvw9R+/+0xc9vMrOtrc+fBufPi2H8faTDX8UfpXr3wr3njScR1tPrl5K+58+JmO6wKAXX92cTQzSePBp/bjNz59L442zfP5r//20nPw7rNP7Ghzyw924U++/hjqDQbG4m2+999+OTbjCvnQlx7GI7tf6rjG+bNrePgTv5J5fRI+cvuPcP+u7FmdMvpIeqbdAE4y3i8HkNQPon2IyANwLID9wrZJniOipcGxlgLo35B/hnFdQiMhVYTvPWeEphPw76eZuNdmi7veZ1YbN6ON5/g/X/M7DdtntXGDNqbU12h2fwZu6nW14DnU1Uhkna+Z88zD7Y1YG/+162a3SfttheefDmnPRakWkl/RAwBWEdFKIhqD75zenNhnM4DLgteXALiHmTnYvjGIiloJYBWA+3POZx7rMgBfE1zjUFBznJQ/5hZcQYczbHiug3ozfq/1Jkede1abRsJXU28yvIyZlhd0mg1jtF4Pvt9aRodaCzph89oiY53VCbuEejN+XY0mZ+4fa5tyvvBYmYYivK+W2YZj15/WppH4vhvNVub3YEPas1SqRa6hYOYGgKsA3AVgG4DbmXkrEV1LRO8OdrsFwMLAWf0RBJFKzLwVwO3wHd/fAnAlMzcBgIi+BOBeAGcQ0W4iujw41p8DeAcRPQHgHcH7kcBz/ZEZG87FRrP7KHtYqbkU68AB3yh267hqDnV0SN3ahB1qvWV+n2EnnGVcgllIased3ibNwNebjJpgtF5LOV8jMmbZ5zOvy2/fil1/kjQj22jJjFkeNYc6jq1UC1H1WGbeAmBLYtsnjNdHALwno+11AK5L2X5pxv4vArhAcl3Dhjm6HPPar0fRULhps6dmtowEBNJcysg9V3oyZxTNnNlB+AwKSE/MHM38Gq1WpgyUbBvu3z5fK/ZZZpsU45L9XXQa2Xqz1XUGJ8V1nI7ZilItRsd7OgSEo8FmTFNvZY4Sh5m0UWgj516zpLmskXdoDJopPorsGUWXNllyVYoU1Gh1l9E62lpKXWnnM4/X0cZxrH1CUmquziiqzuj1UCUmbTRbb3FPdOSyka6Zd7/XrDZZnV1bokmRkbp0qOZ+QPt5ZBukFPlIqP9HbVNmMJnSU2qbHHksxY/Szb9jQ9pzUaqFGoo+4qVJCs1WVzlmWHGdFGd2ThSOL1e1OztmDkbueRKN0aHmRRSljfCb+bKOf/3xzl7y3FKd2S176akukMfSZmO9mFGEMiL3IHFPGU7UUPSR1NGlUMIYNmoudWSbN1utzKidsE1aqGvWqDhNEgrPmen0dTqlp0jWyZKrwjYJ+UiS95J2vvA4WedLvy+GQ4CTaQCdjkCJZg+d2eHxlGoyej1UiUnVq3PkmGHFS3GA5oWUeg5ZaflEFIykO53ZWQYp7NzTI4qyO2EgMaMQjta9tPO1ZPJYIyGPdffvpIXUtkSRWXm0BzhqKKqKGoo+4jopWndrNKUnz6VYxwrkR+F4jhP3HeTkGwCB5GIhI7kpHaokosg8tn9tdtJTWk5EXsJdPXFfXb+HjEFIL35b7etRh3ZVUUPRR9qSQlzrHqXSHSGZmdk5zmybCCYgjK6KG17/WHJZJ8+5nB6tJpSeukZZ2Z2vm6EIZw4dsmZPEu5Ueqo6o9dDlZi0khO9+mMuG2E2r6mZSzKz6ykj7+6RUk7cmZ3TJk3WabRaIMqeUYTHSs52JM8tLXlOmpkdi45rZocJm22Ss9VeDELa8pkaiqqihqKPuClT+F4lRZWNdKdxd12/5lBHBw6ga6RUzaV4ZnarexhpVkRRNy0/Va4SJko6DsEhuzyKWppEmSMjZUVm9SLqKa32lFItRq+HKjFZzuxRzMxOTRrLcWZn+Rvy26SEx2Ymz6XM6nJClNNyL3yjJ/vz8Rwn0YFLjVn8vrrPKDrlKumsJ480H41SLdRQ9JG06JFehTCWjVqGzNats6sl6hXlZSMDQXRVir8hv+CeXMuvpWj0NpKh51JHaK153KzzJX0veRFjQLwzb/Yo9DrNuCrVQg1FH6mljBTrPdKRy0ZaZddGs7v05CXyKPJG3kBYfLAzMzu/4J5cy0/T6G2CEJIlwNszpZxMcAsZKS3sty6scJtH2rNUqsXo9VAlJkvrHsnw2NSMZO5aSK9DehKs1ZHMo8hdjyKaHcizrL00A2+RUe879jtLhliFx+ZIXeG12PiEpKRdj1It1FD0kTTpaVQzszNDSruGujodo2jzWKlt3HhiXz3HR5HeCXPXjPHU7GqLGl3JUOGmUB5rJgYUEnmsnpSeehH15HQ+S6VajF4PVWJqGXLMaGZmxyO8mFmURxHv9LtnMEdtUuSq7PIY6QX+unWo7czsYga+llj4p54z60nPzO7e6XspeRR+NFfvpKdkAqVSHdRQ9BE3YzQ7ktJTwiGbl40cfmZKNHlLhgJh8cEU6SnDuISHMqWnumCJVqBTepLKOkl5LPTVZK1qmDbraebISG6aA7zZ6tFSqJ3GVakWaij6SC1FjulVUlTZaMsVreD/fBkpLGwXkrfyHOAHCMTbdC+4R0QduRfNvBpUGUX6bKKebNqmSU95C1yl5l70qIR9WqSYUi1Gr4cqMWmJS6OaR5HUzPMcuEBYHyqttIZcrsor8OdfQyKbO8dRnCZX2az1UEucLy/BL4pgaiUlSklmdu9LeKTl/yjVQg1FH/FSQjN7tbhM2UgWQMzLbwA6O9S8CCYg0P9Tci/ypKRkkb5uxih1OVOLiKIOY5azjGpWToQoMiu4r8gn1KOlUMPrVqrJ6PVQJaYtKZidYW9CGMtGUq7IK9YH+B1yi4FW5NfonhMRtklKeW4X/T+8NptOOE3WaebUrYqdr6NwYfe2aWHUecYsmXshMZhSNDNbUUPRR7yEHAPkhz0OK8nMbImMlMxIzivHAYSlye1KoniJDPB6btRT54zCT5SU51Ekndnd2hJRsDaHXB5LOsAlocVSNDNbUUPRR7IWpBlFZ3ayAGJ7nYj8MNSwU5WsR+FnZif0/5zv04+ukjt9s9aHECfcJc8naNsZ9ts9WTFZPba9Dvj0ByFpxSyVajF6PVSJySqUN4rhsUkHqKTjSnbIkvUokmtFNwULQSXXvcgLI40MmJET0rBIZvOSxkywlkVyEaf8pMCE9JSzgJMN6sxW1FD0kaQcE3Y4vUiKKhvJzGxJp5/MgJZUj00WEqwLQkJrtp1wwsCH1yd9bp4TD/uV+KXSjFn3DPV4Zy7xCUlJy7JXqoUaij6SDGGU5BYMK8nM7LokbDUxcm/PQrobl44s6xwnc2dIbU55jKS/xfK51VyyjnRL+l7yDGCHMztn7XAb0ta6UKrF6PVQJabD4SgI/xxWkqNwSXhsu0OKj9y7hoW6nf6GvO/TTZQmrwsL7jWSRm+amdndSDqzbcNjeyk9adSTooaijxBRLJxTst7CsJJc9lUyCo8kjtCvkZNlHZ4nWQk2V3pyOyvOSgruJaUneWZ2Ys0MQSJcUnrKWwkx2Zk3BLMxKWnFLJVqoYaiz3gORVN4yXoLw0qyAGK7WJ+gAmry+7HJzG511/KBNLmqe15DaOAbCQMmz8zuPF834weEiYSJNgLpKYoyszRm3a+lM+tbqRaj10OVHC+lwxnFGUVybWqJzJYsPidp05GZLcmjcDrzKPKegWsaeEE5ktj53M6SIbmRWanSkzwQQFInS0paAqBSLdRQ9Bmz8F1bgx+9x5BM0pJIT8lSGe0kPZvMbJmsY5OZDQTFBy3KkcTOl5aZLTBMST9KXs2r8NiArPKulLTMdKVajF4PVXL8CBh5JNCwkiyAGElPEl+AobMT5cwogmQ25vZIOj/qKS7r1HMK7oVtkkZPvBRqSvJcXtuaMQtpthjMeVV049JTWx6b/m/LcQgOaa2nKiP6pRPReiLaTkQ7iOjqlM/Hiei24PP7iGiF8dk1wfbtRHRh3jGJ6AIieoiIfkREPyCi06Z3i+XC1Lqr4MyuJ2S27hFMnfWKJOU4gHi+Rt736a84l6iyKpCC6gl/izwzO7EUqmAZVdO4tHMists4DoHIzEHpnTMb6AzXVapF7q+IiFwANwK4CMBaAJcS0drEbpcDOMDMpwG4AcD1Qdu1ADYCOBPAegA3EZGbc8xPA3gfM78RwD8C+Pj0brFceEZoZthZjaL0lCyA2BSMwjsidwSzg6R+LiqPkXQut7qXxwDiUUi2Br5jKVShMbOpvAuEiYTy0GIb/PvXGUVVkfRQ5wHYwcw7mfkogE0ANiT22QDg88HrOwBcQH75zg0ANjHzFDPvArAjOF63YzKAecHrYwE8W+zWyokZmtkO/xzBGUWiAKLEAZzMBagLCiYmQ1cltbP8pUkTRfry5CqjE2533FLpyekwTLnymOFwlxb4M0uF1Hs8W03Wq1KqhSfYZxmAp433uwH8XNY+zNwgopcBLAy2/zDRdlnwOuuYHwSwhYheA3AQwJsF1zg0uGkjxZHMzI47QOuCzjUZ4tkUSULxbG5JG9MB3moxWpyv5XumgQ9ngsJO2F9Rz24ZVc8lvFaXG1kg7gDvdeh1sgKuUi0kv6K0X2dyaJG1j+12APgwgIuZeTmAzwL469SLIrqCiCaJaHLfvn2pF15GzNGsZNW3YcUNNPOkM7t7yfBOZ7ZkFA3EfSF5Up6ZzS19BmlSUN4sJMR1CMzxGla5hsnI5pbmRJh1ryQ+IRuScp1SLSS/9N0ATjLeL0enHBTtQ0QefMlof5e2qduJaDGAs5n5vmD7bQB+Pu2imPlmZl7HzOsWL14suI1yYGrdthm+w0bN6YwUslmb2l8yVDajMJ24kqKAjYTvJM8gmZ2wxLmcbGu2k1SeNeWqaAGnXLmq3ZlLfEI21BLZ5Uq1kPyKHgCwiohWEtEYfOf05sQ+mwFcFry+BMA97McrbgawMYiKWglgFYD7uxzzAIBjiej04FjvALCt+O2VD9dph2b2MimqjLhG0phECkl2qE2hkxlIyFU5naNrGOu60FGcKhla5FGY7STLqNaMqCepY9rM17A1ZnmYz1KpHrk+isDncBWAuwC4AG5l5q1EdC2ASWbeDOAWAF8goh3wZxIbg7Zbieh2AI8BaAC4kpmbAJB2zGD7fwHwZSJqwTccv93TOx4wtVjnOdozClPikcwoktncdZGTudOZnR8dZFyXsMCfmXthW8K7I0tdsIyqa6wfLs2JMDPApQZQiudSLPdEqRYSZzaYeQuALYltnzBeHwHwnoy21wG4TnLMYPudAO6UXNcw4qWMFEfRRwH4M4SOkFJB0ljDRsuP8ijaxle0FKplqfeakXthO6PolJ4k8ljn7CA/KZCMyru9W4/Cvx4nykxXqsdoah4lphYb9fU2KapsxByywgJ/5r5560QD7dBi05ktcYBH8p8wjDRtdmSTmW22kxnAYnkUyRlFr0KvzagvpXqMZg9VYmIhjCPuzDZj7yVSSORvML4f6Yyi7cTNH62bCXDSMFLP6TTw0ogis7xGuKphfmSWA9uoJzftvnqWma15FFVGDUWf8TuchKEYUekpWQDRD5nNrx7bbFrISCmFBPOdvk5QP4nFnXBatJo0mS28nmaLxcuoxhzTQmNWS5n19C4zO76cq1It1FD0GTMze5TXowBCuaadMyJJMgPM8Fh5HoVdZnZbrpJmWadmZltKT/WmaZgkM5jEbEzizE7MQnqbma3SU1UZzR6qxKRnZo/ojCJxrxKNHWh3jDaZ2TbZ3K6ReyGt4Gv6W2wTJU1ntnQZVbPKcDvoIb/ulVknS9JGSrICrlIt1FD0GTNxqSHsAIaVeAFEQX5DJNG06xVJZxShlFSXGCRjJT1p5Nl0otXM8FhpkqXpb6gLcyLM3Itey5rms1Sqx2j2UCXGXLms10lRZSNeAFEQEposJNhsibR8f18Wh7qabaQ5ETU3TQoSLoVqyGPStuH6F8xteUxSuNDM0XHILz/eC2quJtxVGTUUfSYWmmlZM2jYSMpseY7VaG1qMzM7d3bQlp7E0UExKUjmXE7PMpdmZrcLF0oL/IXX02ixUY5e7gCvC+pk2WA+S6V6jGYPVWLioz67KqTDhpcogCiR2MwOSbLynLkehVRuqZkzCmHxvJph4G3DmsPriTvP84xZ+xqla6sncy96GU3nJdYmV6qFGoo+482gjlw2aomQUkkETjwj2W49CnFOhJF7IZWewpDasJ1/rfL1KAD/O5Am67Wz1FvyazQ6c4lT34ZaYvElpVqooegzqeWqRzQz2yyAKJGegHh5DWlOhL9vC9KQUDP3oiEcrbvmUqjBWt5S/d8znOfSZVQ9t3PWI8kpaRc7zJ+N2eA6jkpPFWY0e6gSE4919zucXiVFlQ2zAKK044pLPIKigGkdqkXuRdtnkJ97YTqzbfxK0ezASkYKfC8tue8llswpKBNigxmuq1QPNRR9pkNaGVEjAXRmM0s6LtehqPicbIGfdiFBaWkNM/dCnpltZpm3rDphM+xXmhPhRW3kkprZmUuWW7XBfJZK9VBD0Wdcx4lWO2s0ZQ7eYcXPZm7nROTVN4ramAv8iLO5252wVHryn4Fc1qlHob4yGS3EDPuVLqMaD+EVOsDNWk+WxiwP81kq1WN0e6mSYi60U++xPFA2Yo57QU4EEJd4GoISHmY2t1RGMktqSCv4eqaBF5QJiZ/PcEwLHeGxsF9hBr+5zG7Po54czcyuMmoo+kw4ugwLxI209JQogCiWnowS4Pk5EW1ZR7pYT1RSo9mehYidy8HMxea5pSf45c8OgKDMiHQ9isQaFr2VnjQzu8qooegznuHYbPQ4KapsJAsgypzZ7VFxs5XvNDZzFGwzs/1OWDpaj+c12M0ozBBemTxmylVNqTxmloeZAWe2ZmZXl9HtpUqKGSrpR8+M7owilpktyLIG2nIVM1tlZjeaFgX+jHUvws4vzyC5poFvtqx8FGmZ2bnrURh5FHXprMeMMhPUybLBdQgtBlo6q6gkaij6TLTmQiA9jWpWNpAogChYJxoI4vVb8jDSsO9smmGkFrkXkfQkTuxriWW09vk6ZxTizOwo6KH7Wh6AbwDDzrwpKOtuQ7sCrhqKKqKGos/Endn5eQLDTLIAojgz2+jA80bFRBTlXojXlkgp0icpuBe2aVjmUcRyPWwzswNJTWKYzM5cUkXXhuQCUUq1GN1eqqTEHJsjHvXkmslz4sxsX66yWfMhrJ/ViJy+0hmFkaMgDFetB+exC49tJ8+Jcz2M9cPr4tmYmXHe68zsts9EqR5qKPpMLFSy1RLlFgwrNXOdaWFIaZhHIZVown3Mgns2EUx1qVzV4cy2T7gzs8eluR71Vhj0IPsegPZKer3M+DcjxZTqMbq9VEkxy0c3hIXyhpWYZi6UQsIM4PZIX2BcgjbSnAgz96IZzA7y9f+2rCNZhCnWNlYyXL4eBdAO+5XMKMzOvGFpzPIwF4hSqocaij6THF2Odh6F4QsQ6uzh2tQ2lXXD+lni1eOM3AvpMzA1+rpl1BMRRf4aqaQWD/uVOabN3Iue51EYMxyleqih6DNmxm1dkHk8zMQzkmUdVxivLy3wB/izNL88hsyZXTM6YWlORLKkhu1oPQz7lUpqZtiv1JfVXuK19/4vM1JMqR6j20uVFHMKP/qZ2XGZzSYzuy50TAP+DCFePE8oIzVbkfSUhxlRVKTgXpil3hBKT6Zjut6SGrN4yfXeLlzUfpZK9VBD0WfM6JFeJ0WVjY4IL2GHXG/JS2sAvs/BainUhCQmMkbRvYSzoyIzipbYmJmZ4FJjFs8A7+1s1YwUU6rH6PZSJaWWkGNGOTPbHLlLy5WECztFWdaSirNBSK10IajkqnjScwDt5UxtZZ0O34twPYpG5MyW+XeAIDKrx4ESZv6PUj3UUPSZZATMqC5aBCQ65JasXElSy5eN9p3YIkS2q+LZSE+Ro9hytO6vPievHhvP5haGFhudea9/WzWNeqo0aij6TLI20agugwq0O+SjjRaYhaGuUfKchTM7knWKdMKykXc76qclNnqx9uGsp9WCQ/nLqCad5zbO7DBUuJdRT1GtK83MriSj20uVlJjWbVkzaNgI7+21ehOAzN/QlpFsMrPbnbB53iwch+CQXQXfeDa3bBEmE9/3wuKcCM+IjpP6d8zOvNd5FGakmFI91FD0mbg+PtrSU9ghHgkMhXTkHkpV4XvJeUItH5AapHYGuDQREAiLD8rqVpm4gfQkXUY1tgqfMCfC7MybwhUFpXiG9KZUD9EviYjWE9F2ItpBRFenfD5ORLcFn99HRCuMz64Jtm8nogvzjkk+1xHRT4loGxF9aHq3WC7M8tGN1ogXBQw6xCN1G8e0E1VoDd9LzhMWzwPyndmA36k2C8g6xZ3ZFOVt2Bgmm9BiszOvFzBmkutRZ3Y18fJ2ICIXwI0A3gFgN4AHiGgzMz9m7HY5gAPMfBoRbQRwPYD3EtFaABsBnAngRADfIaLTgzZZx/wAgJMArGbmFhEd34sbLQtmeOyoFwUMO8RwRiHqkKO6TRbSk+vglaNNNJotEMlmFG4wc5Fq+aasU0T/r7ntwoWyJMIi0pO/T+QTmoHMbA2PrSaSX9J5AHYw805mPgpgE4ANiX02APh88PoOABeQXzxnA4BNzDzFzLsA7AiO1+2YvwvgWmZuAQAzP1/89spHpZzZwb2FPgobbX6qIfM3AO3S5HXBinhRm2AlPWtndjgLKZRHIc8nMf0o0gz+WsInNCOZ2erMriSSv6plAJ423u8OtqXuw8wNAC8DWNilbbdjvg7+bGSSiL5JRKtktzIcmFr3yIfHFphRuIk2EkMaZnPbfJ9hIUFpm3h4rH2iZNvhLl9G1QvCfpvCnIikT6i3CxdpZnaVkfxi035tyV9L1j622wFgHMARZl4H4H8DuDX1ooiuCIzJ5L59+1IvvIyYI1Npobxhpd3py2WkWsKvIe3E27WzhIYiSICTlj+fdmZ24HC3WUbVC+teCR3THT6hGViPQqWnaiL5Je2G7zMIWQ7g2ax9iMgDcCyA/V3adjvmbgBfDl7fCeANaRfFzDcz8zpmXrd48WLBbZSDZKG80XZmJ6QnizDUsI3k+zFlHfFo3ci9sDFgU40WWmwv63guWQ8OPMOPIkpWdOLSU0/DY41wXaV6SP6qHgCwiohWEtEYfOf05sQ+mwFcFry+BMA9zMzB9o1BVNRKAKsA3J9zzK8CeHvw+pcA/LTYrZUTc0bRYtmIeVhpzw7CTl8e7WMrV4Wyjni0boThykbrcd+JrW/JC+Uxi2VUzfLpNkuhHrHIW5Gi61FUm9yoJ2ZuENFVAO4C4AK4lZm3EtG1ACaZeTOAWwB8gYh2wJ9JbAzabiWi2wE8BqAB4EpmbgJA2jGDU/45gC8S0YcBHAbwwd7d7uAJDcXUDIz6yoabuFdRfkPQiU5Z6Ow1Q9aRZkxHGeBNWRhp8rnZdsJe6Dy3WEa1XfdKZsyS33cvZ6tRtJ4aikqSaygAgJm3ANiS2PYJ4/URAO/JaHsdgOskxwy2vwRvNKGtAAAcRElEQVTgnZLrGkbchDwwytVjawnpyaZekZVc5bZnFNLv07ZNUtax9VHU3FBGkmdM+74XFif4dX53vczM1vUoqszo9lIlhYhQc8nKwTustPMo5KGuHR2yRTZ33cLJ7AUlNRqWq8eF92IrPbmO016DRNg2zOZuSJdCTfh3VHpSeoUaigHgOjQjIYxlIxmuKS2t4bexS7hrNOVafnhcm6VQw+VMi+r/fiKh3TKqnkttYyZcwAkobsy6XouxzrhSPdRQDICa41RCekpKIdLSGoCZCyCTnmwK7vnHNUpqWMhVRSOKTKlLLD1FfhRhZNYM5lFEJUVUeqoko9tLlRivKtJTR62nIgl3MumpaTHy9o8bzkLkclXNcazqVpm018yQGzMzmkvqqwHsIsaktMus64yiiqihGACe62CqMfozinCEG96raEZhSE+uQ/ArwXTHC/R/6cgbaGdm29Tb8lwynlsRZ7Y8yipsE4XjWuRRFDVm3SCiyGeiVI/R7aVKjOcQXjtagfDYUHo6arceBeCPiqVavpmvIZaRQulJmJkN+LOC8F5sO2E/HNeutLznOm2fiOB3EnbmMzGjANrhukr1UEMxAEytu5ejvrKRjMKxqVf0Wr0pz4kwwnClhrddUkM+C6mZz63AjKLebKFusYyq55BVhnpHmx7PVsNwXaV6jG4vVWJ8rbv3IYxlo0Mzt6pXZDc7CNtIF+uJ5VFYyFU2vhMTs3Ch3ADazw5q7sz9tny5TqWnKqKGYgD48kAYwjjChiKhmcsys9ttxB240UaemU1GKXN5Ndf2vVhKT65jrGpofz7pdxH7bfV4tuo5pM7siqKGYgCY2vMoO7OTmrmdM7tp4WQu1sZ2tG7mUUgNUogZ9mvjzLb9ndQKzEKkhGVPlOoxur1UiYn9MY+w9ATEO1fb9ShsciJs2xR5BtMx8G5MUrOZHVheoyFr9vq3FVbpVaqHGooB4DpUuGbQsOFZ3qu5SpvtjMKmTfwZ2DuK7TOzjWu0qB5r6zyP3VePZ6sa9VRd1FAMgJrjIByYjbL0BPj3Z3OvYSfaYrkRDY1Ly2KdaM94BuJIKZes25ht29coTfBrn89mptRu0+sZhaNLoVaU0e6lSoo5Ohz1GUXN8l7Nkbo8v8H++zSvS+pcNp3D9nkUxjVa5G1E57acXfmvNY9C6Q1qKAZArGMb4agnwL4TN42DfN0G+87RLdSm+HOLdeAzaMxiBqnXUU/qo6gsaigGgNkZ9jopqmyYnZVNZrb/Wi63tF8XaSOXnqLXtgsXFTAy8e/Cvk2vQ6/9dcZVeqoio91LlRSvgFQyrISdVc2V1W0y5R3bzGzALjoo7XXXa5uGgS/S1ot9F/Zteu3/qrkqPVUVNRQDID4yHe1HEM4ipDKSG5Nb7BLuku27tikwOzCvxzbqqUhbb5ptej0ICbPLleox2r1USSmiqQ8r4ejZplZRsq1VmyJafgG5qkitp7Rzd8OLzUJmXh7Lo+Y6qGvUUyVRQzEAimjPw0p4fza1ipJt88/RpzYFpKDUtoUMk7081vNaTxr1VFnUUAyA6YRZDhvh/Uk7OrNvs83MTr7uRrERfnED7xZoWyTsN9zPE67lYUO4+JJSPUa7lyopRTqNYcXsuCQQUdSJF+vAi+RezHzoaXxwMHOzKzcyzL3/XfnObJWeqogaigFQK6CpDyu20hNgzkIKdKgWy5q22898MluRAIYihimKMpuB31VYAVepHqPdS5UUbwZ15LJh68wG2h3kTGZmmx23eCW9aUQUFcmjiGep2xkzafSXDTWHtIRHRVFDMQC8Ah3AsGIbHgu0O1J5B26fO1CsVIgTtbXV/+O5HjPnmG5Lfb3/03bVmV1Z1FAMALMj7LXDsWzYOrPNfYuEhMrXerCXkWx9JybTzcy2DRWeiQGIp0uhVhY1FAPAM0amo46ZmS3FdlQcT0ybuUgpL7oX+z+bIsas2DXO3G+rpkuhVhY1FAMgGvVVwFD0Q3qafmKavfRkS7zmVYFyHJYGcCZqiKn0VF3UUAyAsGMb9bUogGLO7HDfmez0i2TH16Yh68SKEM5g3oY3DXksD83Mri6j31OVkCJyzLASyUg20lMUUltgnYiZ1P9DA1/AURwPrbV3ZtuGCs/EIEQzs6uLGooBUESOGVZsZSR/37BDnskM5uIRRcWkJ/tw3CJJga4zczMKz/HXo2BWY1E11FAMgOmMTIcNL5KRLKSnSD6ZuZpIsRmFeA3r4jPBIiHR4X5E9rkeM5GZHX63WkG2eoj+QohoPRFtJ6IdRHR1yufjRHRb8Pl9RLTC+OyaYPt2IrrQ4pj/k4gOF7utcjMdrXvYKKKZ28pVRBR1pFL9v5hcVVzWKZJHERlZm2TFAj4h+bH970mzs6tH7q+JiFwANwK4CMBaAJcS0drEbpcDOMDMpwG4AcD1Qdu1ADYCOBPAegA3EZGbd0wiWgfguGneW2mplPRUxEdhKT0BBda9mMZaD33LoyjwO5nJ31Z4PWooqodk2HEegB3MvJOZjwLYBGBDYp8NAD4fvL4DwAXkZ5JtALCJmaeYeReAHcHxMo8ZGJG/BPCH07u18hJFAlUg6qmIzGbrzAZMycVerrJNZisi6xRZJ6I9g5GfL5LtZkJ6Cp6hFgasHpK/kGUAnjbe7w62pe7DzA0ALwNY2KVtt2NeBWAzM++R3cLwUaRQ3rBSRDO3zcwu0ibc30r/n4ZvKV6E0NYnYlMna+YGIeF3q9nZ1cMT7JP2V5T8pWTtk7U97VfMRHQigPcAOD/3ooiuAHAFAJx88sl5u5eKdmZ2dWYURYoCWs1CLCWXIjLSdJzZjkMgApjtpSebawyvbUakJ3VmVxbJX+JuACcZ75cDeDZrHyLyABwLYH+XtlnbzwFwGoAdRPQkgNlEtCPtopj5ZmZex8zrFi9eLLiN8lDJzOxCJTyKdOJ2o/WZNEZJatEAwS6E185XYz8bkx87nFGo9FQ1JH8lDwBYRUQriWgMvnN6c2KfzQAuC15fAuAe9oOtNwPYGERFrQSwCsD9Wcdk5m8w8xJmXsHMKwC8GjjIR4pKSU+uvVGsFdDmbTtV2zUvzDZFZZ3ImFmG/doVVLQ3gFLC61FndvXIlZ6YuUFEVwG4C4AL4FZm3kpE1wKYZObNAG4B8IVg9L8ffsePYL/bATwGoAHgSmZuAkDaMXt/e+WkUs7sQtVjCzizLWcUtvvHrqvgjMLWGV5WZ7YWBqweEh8FmHkLgC2JbZ8wXh+B71tIa3sdgOskx0zZZ67k+oaNSoXHFuhci2QXFw2PtSpWOE3fkm0EWBEJzjar3QYvkp50RlE1Rn9IW0JsM4+HmSIST61AZ2crV7WLFfbHmQ0UmVHY/05sw4RtCI+p9Z6qx+j3VCXEm0GHY9ko5DSehiwkLsdRoEMt4jOIty/mRykUJjwjUU/BjEKlp8qhhmIAVEp6KhLBVCgjOZCFLNej7qes4zpktaphke+h3WZmqscCGh5bRdRQDIBKObML1Ecq5MR17GYURATPIcuop2k6s10qFPJbJBBgRpZCDb5bDY+tHqPfU5WQmVxcpmwUKYA4HenJdt0LK/1/mms91BzHsopuAempgE9Ifj1BeKz6KCqHGooBMJ2aQcNGseii4hFJdhKXY/UMprvWQyg92Z7PRkZqr//R+z9tV6WnyqKGYgBUaT2KIqPwYuW17fT/sE2R8hhFDXzNJcvZgX2y4kxm/YfPUqWn6jH6PVUJmcnFZcpGOxppZjtkz3GsR/r+jKLAWg+FM7Mdq8FBET/KjC6FqpnZlUUNxQDwLMMkh5kimdlF6hV5jt3sIDy+1YyiB9KT7eDAb9O/7PGux9b1KCqLGooBEGndlYh6su+4agV0ds+161D949u1ma6PwtYw+W3sZkoz6f/S9Siqy+j3VCWkSKG8YaXYCndFjItjHRJacx3LzOzpyTq2Upd/LrvIrCIzOJtrATTqqYqooRgAM7m4TNkY8+zvdcxzY21FbVy70NMibcam6aMY84pd45gnN2bhdzY2I+GxgTNbM7Mrh6gooNJbxjwHf/Wes/GW1y0c9KXMOGuWzMPH37kGb1slXzNk/VlL0GTG8RPj4jYfeOsKXLDmeKtru+bi1Zg/e0y8/zFjLv7ikjfgF1ctsjpPyO+d/zocOtKwanPthrOwctEc8f5Lj52FP9lwJi48c4nt5eVyzJhvwF872uz5sZVyQ/6yEcPNunXreHJyctCXoSgjTavFOPW/b8GHLliFj7zj9EFfjtIDiOhBZl6Xt9/oax+KovQExyHMHfdw6Eh90Jei9Bk1FIqiiJmY5VnLZ8rwo4ZCURQxvqHQGUXVUEOhKIqYiVk1nVFUEDUUiqKImZjl4fCUGoqqoYZCURQxOqOoJmooFEURo1FP1UQNhaIoYubN8nBQZxSVQw2FoihiJmZ5ONpoYaqh2dlVQg2FoihiJmbVAED9FBVDDYWiKGImZvnl4Q6roagUaigURRGjM4pqooZCURQxc8f9GYVGPlULNRSKoogJpSeNfKoWaigURREzL5KedEZRJdRQKIoiJpxRqI+iWqihUBRFzNww6knrPVUKNRSKooipuQ6OqbkqPVUMkaEgovVEtJ2IdhDR1SmfjxPRbcHn9xHRCuOza4Lt24nowrxjEtEXg+2PEtGtRFSb3i0qitJL5uriRZUj11AQkQvgRgAXAVgL4FIiWpvY7XIAB5j5NAA3ALg+aLsWwEYAZwJYD+AmInJzjvlFAKsBvB7AMQA+OK07VBSlp+gqd9VDMqM4D8AOZt7JzEcBbAKwIbHPBgCfD17fAeACIqJg+yZmnmLmXQB2BMfLPCYzb+EAAPcDWD69W1QUpZdMzKrhoEpPlUJiKJYBeNp4vzvYlroPMzcAvAxgYZe2uccMJKf3A/iW4BoVRekT83RGUTkkhoJStrFwH9vtJjcB+B4zfz/1ooiuIKJJIprct29f2i6KoswAuspd9ZAYit0ATjLeLwfwbNY+ROQBOBbA/i5tux6TiP4IwGIAH8m6KGa+mZnXMfO6xYsXC25DUZReMDFe06iniiExFA8AWEVEK4loDL5zenNin80ALgteXwLgnsDHsBnAxiAqaiWAVfD9DpnHJKIPArgQwKXM3Jre7SmK0ms06ql6eHk7MHODiK4CcBcAF8CtzLyViK4FMMnMmwHcAuALRLQD/kxiY9B2KxHdDuAxAA0AVzJzEwDSjhmc8u8BPAXgXt8fjq8w87U9u2NFUabFxCwPrx5totFswXM1FasK5BoKwI9EArAlse0TxusjAN6T0fY6ANdJjhlsF12ToiiDISw1fniqgeNmjw34apR+oMMBRVGs0HpP1UMNhaIoVsxTQ1E51FAoimLFhJYarxxqKBRFsaK9yp3OKKqCGgpFUayIfBRTOqOoCmooFEWxoi096YyiKqihUBTFCo16qh5qKBRFsWJWzcWY66ihqBBqKBRFscZfk0J9FFVBDYWiKNZovadqoYZCURRrdEZRLdRQKIpijV9qXGcUVUENhaIo1ui62dVCDYWiKNZMzKrpKncVQkt6K4pizcQsD3sPHsG7/+4HPTum6xA++atn4uyTjou2ffSOR7Bt78HMNnPGPNz0vnMxf45f7vzQkTp+74sP4eXXsv0nKxfNwafe+0YE691g256D+PhXH0W9mb1O2q+sPQFXvX1V9H7LT/bgf3333zvWbw4hAL97/mlYf9aSaNvf3v0EvrPtucxz1FwHf/brr8fpJ0wAAJgZH9r0Izz14iuZbQDgxv90Lk5aMLvrPtNFDYWiKNa86w1L8fT+V9HirK7Snu8/8QK2/GRPZCgOvHIUt00+jTNOmMCJx83q2P/Vo03cu/NF3LvzRVz8+qUAgMmnDuD7T7yAN62YH9WkMnnu4BS+9qNn8YfrV2PZcccAAO7auhcP/ewAzj89fUnlJ54/jM/921MxQ3H75NN48sVXce7Jx6W2efCpA7jjwd2RoWBmfPb/7cLcWR5OWzy3Y38G8K/b9+Gft+6NDMXT+1/DP/34WZx54jwcPzGe9bXBdSjzs16hhkJRFGvWrViAWz6woKfHvPhvvo/H9rRnD9uC1x975xq8LaUTP1JvYu0nvoVtew5GhiJs85nffBOOnV3raDP55H5c8vf3YtuzByNDsW3PQaxcOAef/a3zUq/rM9/fiT/9xjbsOzSFxUGHvW3PQbx99fG44b1vTG1z1T8+hId/9lL0/rmDUzjwah2/f8EqfOCtK1Pb/OJf3INtew5F78Pv4k9/7Sycc/L81Db9Qn0UiqKUgjVL56V2lGuWzkvdf1bNxamL50bGAQC27TmEZccdk2okAGB1cKxkm6xzAMDa4LPHAwls/ytH8dzBKaxZOtH1Xp556bVIAtuWcy8AsGbJvMR1HQQRcMaS7PP0CzUUiqKUgjVLJ/DC4SnsOzQFAHh87yEsmjsejeLT28SNy+N7DnbtwOeOezh5wWw8vtdvc3iqgZ/tfzW30wfanf3jgk4/NC7bg/OEfpbV3QzF0nnY9eIreO1o0z/PXn+mM3ts8MKPGgpFUUpBcuS+LafTB3zjEo7cj9Sb2PnCK1078LBN2Olv35vf6c+fM4Yl82ZFBilvpmN+Fp4nmukckz7TCdswA9ufOxS1ybuXfqGGQlGUUmB2rvVmC088dzgyHnltHt9zEE88dxjNFgsMhT9yf/VoA48Fnb+Ncdm25xAWT4xj0dzsmc4J88Yxf3bNaHMw9xxrjfs/dKSeO9PpJ2ooFEUpBebIfee+V3C02bLqXCV+gPBzZl8W2rbnII49poalx3ZGVSXb7Hj+MKYaTVGnT0SBLHbQn+nsO4y1OZ3+8vnHYO64h217DkaSlc4oFEVREoQjd2mnf/zEOBbMGcO2PYfw2J6DmD3m4pScnIK2cTkUyVthTkX2dc1Do8XYvvcQdjx/WDTSX7N0HrY/55+jxfn34jiE1Uvs7r9fqKFQFKU0hCP3H+9+CWOug1MXz+m6vz9yn8C2vX7nesaSCTg5eQXL5x+DiXEPW599Gdv3yvwA4T7feGQPjjZbuZJY2OZIvYVvPbo3doy8No8HRk8y0+kXaigURSkN4cj9G4/swaoT5qLm5ndRa5bMi2QkSWdMRFi9dALffuw5vHq0KWqzctEcjHsO7nz4meg68+/Fn3Xc+fAzmD3m4mRB9vSapfNwaKqBu7c9L5rp9As1FIqilIawc33+0BRWL5HJLquXzsNUo4WDRxpYI8w5WL1kHp4PwnDXCM7jOoQzlkzg+UNTGHMdrFzUfaYDAKcdPxeeQ3j+0JRopgMAqwvcfz9QQ6EoSmlYsdAfuQMQR/yY+0k1/XA/1yGsOqGzpEZqm6Djls50xj0XrwvKdUiva/WSCYSTCIm81S/UUCiKUho814kykaUdZThyB7ontJmExuXURXMwq+ZatbFxMNu2mT3mYcXCOdbnmWnUUCiKUirCkbu0oxz3XJx2/FycvGB2aiHANM4IRu52nb7ddZn75oXGxttMWM10+sHgc8MVRVEM3v+WU7Bi0ZyodLiED7/jdBxtZJcJTzJ7zMPH37k2s/prGueeMh+/87ZT8atnLxW3+bVzlmH/q0fxhuXy81z+C6fiTSsWiGc6/YC4h2WCB8W6det4cnJy0JehKIoyVBDRg8y8Lm8/lZ4URVGUrogMBRGtJ6LtRLSDiK5O+XyciG4LPr+PiFYYn10TbN9ORBfmHZOIVgbHeCI4pnz+qSiKovScXENBRC6AGwFcBGAtgEuJaG1it8sBHGDm0wDcAOD6oO1aABsBnAlgPYCbiMjNOeb1AG5g5lUADgTHVhRFUQaEZEZxHoAdzLyTmY8C2ARgQ2KfDQA+H7y+A8AF5KcUbgCwiZmnmHkXgB3B8VKPGbR5e3AMBMf8teK3pyiKokwXiaFYBuBp4/3uYFvqPszcAPAygIVd2mZtXwjgpeAYWecCABDRFUQ0SUST+/btE9yGoiiKUgSJoUjLO0+GSmXt06vtnRuZb2bmdcy8bvHi9EXRFUVRlOkjMRS7AZxkvF8O4NmsfYjIA3AsgP1d2mZtfwHAccExss6lKIqi9BGJoXgAwKogGmkMvnN6c2KfzQAuC15fAuAe9hM0NgPYGERFrQSwCsD9WccM2vxLcAwEx/xa8dtTFEVRposo4Y6ILgbwKQAugFuZ+ToiuhbAJDNvJqJZAL4A4Bz4M4mNzLwzaPsxAL8NoAHgvzLzN7OOGWw/Fb5zewGAhwH8Z2aeyrm+fQCesr35gEXwZzKjgN5LeRml+9F7KSdF7uUUZs7V7kciM3s6ENGkJDNxGNB7KS+jdD96L+VkJu9FM7MVRVGUrqihUBRFUbqihgK4edAX0EP0XsrLKN2P3ks5mbF7qbyPQlEURemOzigURVGUrlTaUORVxS0zRHQSEf0LEW0joq1E9PvB9gVE9O2g+u63iWj+oK9VSlAw8mEi+nrwfigrCRPRcUR0BxE9HjyftwzrcyGiDwe/r0eJ6EtENGtYngsR3UpEzxPRo8a21OdAPn8b9AWPENG5g7vydDLu5y+D39kjRHQnER1nfJZaubsIlTUUwqq4ZaYB4A+YeQ2ANwO4Mrj+qwHcHVTfvTt4Pyz8PoBtxvthrST8NwC+xcyrAZwN/56G7rkQ0TIAHwKwjpnPgp/ztBHD81w+B79qtUnWc7gIfkLwKgBXAPh0n67Rhs+h836+DeAsZn4DgJ8CuAbIrtxd9MSVNRSQVcUtLcy8h5kfCl4fgt8ZLUO8ku/QVN8louUA3gngM8H7oawkTETzALwNwC0AwMxHmfklDOlzgb9c8jFBWZ3ZAPZgSJ4LM38PfgKwSdZz2ADgH9jnh/BLCcnXPO0DaffDzP9sFFH9IfyyR0B25e5CVNlQSKriDgXkLxR1DoD7AJzAzHsA35gAOH5wV2bFpwD8IYBw4WNxJeGScSqAfQA+G8honyGiORjC58LMzwD4KwA/g28gXgbwIIbzuYRkPYdR6A9+G8A3g9c9vZ8qGwpxpdoyQ0RzAXwZfnmUg4O+niIQ0bsAPM/MD5qbU3YdhufjATgXwKeZ+RwAr2AIZKY0Av1+A4CVAE4EMAe+RJNkGJ5LHsP6ewMQlUpqAPhiuCllt8L3U2VDIamKW2qIqAbfSHyRmb8SbH4unDIH/z8/qOuz4K0A3k1ET8KXAN8Of4YxjJWEdwPYzcz3Be/vgG84hvG5/AcAu5h5HzPXAXwFwM9jOJ9LSNZzGNr+gIguA/AuAO/jdr5DT++nyoZCUhW3tAQa/i0AtjHzXxsfmZV8h6L6LjNfw8zLmXkF/OdwDzO/D0NYSZiZ9wJ4mojOCDZdAOAxDOFzgS85vZmIZge/t/Behu65GGQ9h80AfjOIfnozgJdDiarMENF6AB8F8G5mftX4KKtydzGYubL/AFwMP1Lg3wF8bNDXY3ntvwB/KvkIgB8F/y6Gr+3fDeCJ4P8Fg75Wy/s6H8DXg9enBj/uHQD+L4DxQV+f8B7eCGAyeDZfBTB/WJ8LgD8G8DiAR+FXiB4flucC4EvwfSt1+CPsy7OeA3yp5sagL/gJ/Eivgd+D4H52wPdFhH3A3xv7fyy4n+0ALprOuTUzW1EURelKlaUnRVEURYAaCkVRFKUraigURVGUrqihUBRFUbqihkJRFEXpihoKRVEUpStqKBRFUZSuqKFQFEVRuvL/AauFALfFPe4bAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.history.history['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 267us/step\n",
      "Test loss: 0.5226242788314819\n",
      "Test accuracy: 0.9143\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
