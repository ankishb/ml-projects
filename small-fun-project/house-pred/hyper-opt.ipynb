{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hyperopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c2e13193501d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hyperopt'"
     ]
    }
   ],
   "source": [
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an XGBoost model with hyperparmaters that are optimized using hyperopt\n",
    "# The output of the script are the best hyperparmaters\n",
    "# The optimization part using hyperopt is partly inspired from the following script: \n",
    "# https://github.com/bamine/Kaggle-stuff/blob/master/otto/hyperopt_xgboost.py\n",
    "\n",
    "\n",
    "# Data wrangling\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Scientific \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Machine learning\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Hyperparameters tuning\n",
    "\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "\n",
    "# Some constants\n",
    "\n",
    "SEED = 314159265\n",
    "VALID_SIZE = 0.2\n",
    "TARGET = 'outcome'\n",
    "\n",
    "#-------------------------------------------------#\n",
    "\n",
    "# Utility functions\n",
    "\n",
    "def intersect(l_1, l_2):\n",
    "    return list(set(l_1) & set(l_2))\n",
    "\n",
    "\n",
    "def get_features(train, test):\n",
    "    intersecting_features = intersect(train.columns, test.columns)\n",
    "    intersecting_features.remove('people_id')\n",
    "    intersecting_features.remove('activity_id')\n",
    "    return sorted(intersecting_features)\n",
    "\n",
    "#-------------------------------------------------#\n",
    "\n",
    "# Scoring and optimization functions\n",
    "\n",
    "\n",
    "def score(params):\n",
    "    print(\"Training with params: \")\n",
    "    print(params)\n",
    "    num_round = int(params['n_estimators'])\n",
    "    del params['n_estimators']\n",
    "    dtrain = xgb.DMatrix(train_features, label=y_train)\n",
    "    dvalid = xgb.DMatrix(valid_features, label=y_valid)\n",
    "    watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "    gbm_model = xgb.train(params, dtrain, num_round,\n",
    "                          evals=watchlist,\n",
    "                          verbose_eval=True)\n",
    "    predictions = gbm_model.predict(dvalid,\n",
    "                                    ntree_limit=gbm_model.best_iteration + 1)\n",
    "    score = roc_auc_score(y_valid, predictions)\n",
    "    # TODO: Add the importance for the selected features\n",
    "    print(\"\\tScore {0}\\n\\n\".format(score))\n",
    "    # The score function should return the loss (1-score)\n",
    "    # since the optimize function looks for the minimum\n",
    "    loss = 1 - score\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def optimize(\n",
    "             #trials, \n",
    "             random_state=SEED):\n",
    "    \"\"\"\n",
    "    This is the optimization function that given a space (space here) of \n",
    "    hyperparameters and a scoring function (score here), finds the best hyperparameters.\n",
    "    \"\"\"\n",
    "    # To learn more about XGBoost parameters, head to this page: \n",
    "    # https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n",
    "    space = {\n",
    "        'n_estimators': hp.quniform('n_estimators', 100, 1000, 1),\n",
    "        'eta': hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "        # A problem with max_depth casted to float instead of int with\n",
    "        # the hp.quniform method.\n",
    "        'max_depth':  hp.choice('max_depth', np.arange(1, 14, dtype=int)),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "        'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "        'eval_metric': 'auc',\n",
    "        'objective': 'binary:logistic',\n",
    "        # Increase this number if you have more cores. Otherwise, remove it and it will default \n",
    "        # to the maxium number. \n",
    "        'nthread': 4,\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'exact',\n",
    "        'silent': 1,\n",
    "        'seed': random_state\n",
    "    }\n",
    "    # Use the fmin function from Hyperopt to find the best hyperparameters\n",
    "    best = fmin(score, space, algo=tpe.suggest, \n",
    "                # trials=trials, \n",
    "                max_evals=250)\n",
    "    return best\n",
    "\n",
    "#-------------------------------------------------#\n",
    "\n",
    "\n",
    "# Load processed data\n",
    "\n",
    "# You could use the following script to generate a well-processed train and test data sets:\n",
    "# https://www.kaggle.com/yassinealouini/predicting-red-hat-business-value/features-processing\n",
    "# I have only used the .head() of the data sets since the process takes a long time to run.\n",
    "# I have also put the act_train and act_test data sets since I don't have the processed data sets \n",
    "# loaded. \n",
    "\n",
    "train_df = pd.read_csv('../input/act_train.csv').head(100)\n",
    "test_df = pd.read_csv('../input/act_test.csv').head(100)\n",
    "\n",
    "FEATURES = get_features(train_df, test_df)\n",
    "print(FEATURES)\n",
    "\n",
    "\n",
    "#-------------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# Extract the train and valid (used for validation) dataframes from the train_df\n",
    "\n",
    "train, valid = train_test_split(train_df, test_size=VALID_SIZE,\n",
    "                                random_state=SEED)\n",
    "train_features = train[FEATURES]\n",
    "valid_features = valid[FEATURES]\n",
    "y_train = train[TARGET]\n",
    "y_valid = valid[TARGET]\n",
    "\n",
    "print('The training set is of length: ', len(train.index))\n",
    "print('The validation set is of length: ', len(valid.index))\n",
    "\n",
    "#-------------------------------------------------#\n",
    "\n",
    "# Run the optimization\n",
    "\n",
    "# Trials object where the history of search will be stored\n",
    "# For the time being, there is a bug with the following version of hyperopt.\n",
    "# You can read the error messag on the log file.\n",
    "# For the curious, you can read more about it here: https://github.com/hyperopt/hyperopt/issues/234\n",
    "# => So I am commenting it.\n",
    "# trials = Trials()\n",
    "\n",
    "best_hyperparams = optimize(\n",
    "                            #trials\n",
    "                            )\n",
    "print(\"The best hyperparameters are: \", \"\\n\")\n",
    "print(best_hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
