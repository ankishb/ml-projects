{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "\n",
    "import helpers_04\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid (Logistic) Function\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$\\sigma$ ranges from (0, 1). When the input $x$ is negative, $\\sigma$ is close to 0. When $x$ is positive, $\\sigma$ is close to 1. At $x=0$, $\\sigma=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_me(xs, ys, ylim, cross):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "\n",
    "    plt.grid(True, which='both')\n",
    "    plt.axhline(y=0, color='y')\n",
    "    plt.axvline(x=0, color='y')\n",
    "    \n",
    "    plt.plot(xs, ys)\n",
    "    plt.plot(0,cross,'ro')\n",
    "\n",
    "    plt.ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-10.0, 10.0, num=100)\n",
    "sigmoid = tf.nn.sigmoid(xs)\n",
    "ys = tf.Session().run(sigmoid)\n",
    "show_me(xs, ys, (-0.1, 1.15), .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "\n",
    "* Easy derivative\n",
    "* Function looks like we think a neuron might function: it is either off or outputing a value (up to a maximum)\n",
    "\n",
    "Cons: \n",
    "\n",
    "* Not symmetric, which causes issues when training\n",
    "* Susceptible to vanishing gradients: when input values are saturated (either positively or negatively), the derivative is close to zero.\n",
    "\n",
    "##### Derivative\n",
    "\n",
    "Derivative of the sigmoid is easy to calculate if you know the output:\n",
    "\n",
    "$$\n",
    "\\frac{d\\sigma}{dx} = \\sigma \\left(1 - \\sigma \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperbolic Tangent (Tanh)\n",
    "\n",
    "$$\n",
    "tanh(x) = \\frac{sinh(x)}{cosh(x)} = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} = \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
    "$$\n",
    "\n",
    "Pros:\n",
    "* Similar to sigmoid, but \"stretched\" to range from (-1, 1)\n",
    "* Symmetric around 0, which helps for optimization\n",
    "\n",
    "Cons:\n",
    "* Still suffers from vanishing gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-10.0, 10.0, num=100)\n",
    "tanh = tf.nn.tanh(xs)\n",
    "ys = tf.Session().run(tanh)\n",
    "show_me(xs, ys, (-1.1, 1.1), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Derivative\n",
    "\n",
    "$$\n",
    "\\frac{dtanh}{dx} = 1 - tanh^{2} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified Linear Unit (ReLU)\n",
    "\n",
    "$$\n",
    "ReLU(x) = max\\left(0,x\\right) \\\\ \\\\\n",
    "$$\n",
    "\n",
    "Equivalent to:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  ReLU(x) = \\begin{cases}\n",
    "    0 & \\text{if $x\\lt0$} \\\\\n",
    "    x & \\text{if $x\\geq0$}\n",
    "  \\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Pros:\n",
    "* Incredibly easy to calculate output and derivative\n",
    "* Doesn't suffer from vanishing gradient on positive side\n",
    "* In practice tend to be more useful than Sigmoid/Tanh for typical activation functions\n",
    "\n",
    "Cons:\n",
    "* Not symmetric\n",
    "* Can cause exploding activations if not careful\n",
    "* Gradient can \"die\" if not careful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-10.0, 10.0, num=100)\n",
    "relu = tf.nn.relu(xs)\n",
    "ys = tf.Session().run(relu)\n",
    "show_me(xs, ys, (-1.0, 10.0), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Derivative\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\frac{dReLU}{dx} = \\begin{cases}\n",
    "    0 & \\text{if $x\\lt0$} \\\\\n",
    "    1 & \\text{if $x\\geq0$}\n",
    "  \\end{cases}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU\n",
    "\n",
    "$$\n",
    "LReLU(x) = max\\left(\\alpha x,x\\right) \\\\ \\\\\n",
    "$$\n",
    "\n",
    "Equivalent to:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  ReLU(x) = \\begin{cases}\n",
    "    \\alpha x & \\text{if $x\\lt0$} \\\\\n",
    "    x & \\text{if $x\\geq0$}\n",
    "  \\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Pros:\n",
    "* Similar to ReLU, but doesn't \"die\".\n",
    "\n",
    "Cons:\n",
    "* Yet another hyper-parameter to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, scale):\n",
    "    return tf.maximum(tf.multiply(scale, z), z)\n",
    "    \n",
    "xs = np.linspace(-10.0, 10.0, num=100)\n",
    "l_relu = leaky_relu(xs, 0.1)\n",
    "ys = tf.Session().run(l_relu)\n",
    "show_me(xs, ys, (-1.0, 10.0), 0.0)\n",
    "plt.annotate(r'scale ($\\alpha$) set to 0.1', \n",
    "             (-5, -.5), # where the arrow points to\n",
    "             (-8, 1),   # where the text lives\n",
    "             size=10, \n",
    "             arrowprops={'arrowstyle': '->'});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Derivative\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\frac{dReLU}{dx} = \\begin{cases}\n",
    "    \\alpha & \\text{if $x\\lt0$} \\\\\n",
    "    1 & \\text{if $x\\geq0$}\n",
    "  \\end{cases}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling and Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to take a very short digression to look at two utility topics.\n",
    " * We want to be able to randomly shuffle arrays in lock step (that is, apply the same permutation to several different arrays)\n",
    " * We want a method that will gives us blocks (subsets) of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(*args):\n",
    "    \"Shuffles list of NumPy arrays in unison\"\n",
    "    state = np.random.get_state()\n",
    "    for array in args:\n",
    "        np.random.set_state(state)\n",
    "        np.random.shuffle(array)\n",
    "\n",
    "def grouper(iter_, n):\n",
    "    \"\"\"Collect data into fixed-length chunks or blocks\n",
    "     grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\n",
    "     from python itertools docs\"\"\"\n",
    "    args = [iter(iter_)] * n\n",
    "    return zip(*args)\n",
    "\n",
    "def batches(data, labels, batch_size, randomize=True):\n",
    "    if len(data) != len(labels):\n",
    "        raise ValueError('Image data and label data must be same size')\n",
    "    if batch_size > len(data):\n",
    "        raise ValueError('Batch size cannot be larger than size of datasets')\n",
    "    if randomize: \n",
    "        shuffle(data, labels)\n",
    "    for res in zip(grouper(data, batch_size),\n",
    "                   grouper(labels, batch_size)):\n",
    "        yield res\n",
    "\n",
    "for b in batches(list(range(10)), \n",
    "                     list(range(100,110)), \n",
    "                     3, randomize=False):\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, dropout is included in TensorFlow in the form of [`tf.nn.dropout()`](https://www.tensorflow.org/api_docs/python/tf/nn/dropout). You simply pass a `Tensor` and a desired keep probability (usually from a placeholder) into it, and it returns a `Tensor` that will have values randomly dropped at a rate of $p = 1 - keep\\_prob$.  TF automatically performs the scaling necessary to keep the expected value of the outputs consistent, so you just have to pop it into your model to take advantage of it!\n",
    "\n",
    "An interesting feature of this scaling is that it actually takes the opposite approach of what is described in the dropout academic paper. Instead of scaling the values of a neuron down _after_ training, it scales them up _during_ training. Let's take a look at the scaling so we can see what's going on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones(10)\n",
    "dropper = tf.nn.dropout(a, keep_prob=0.5) # named arg for clarity\n",
    "tf.Session().run(dropper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a `keep_prob` of 0.5, the expected value of the inputs _without_ dropout is `2*a`. That's why we see the input values of 1 multiplied by 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating layers with custom layer functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that we end up copy-pasting a lot of code when creating neural networks. As with anything in programming, if you find yourself copying and pasting pieces of boilerplate code, it's often best to create a function that can fix the problem. Let's take a look at these three example layers:\n",
    "\n",
    "```python\n",
    "with tf.name_scope('hidden1'):\n",
    "    w = tf.Variable(tf.truncated_normal([100, 300]), name='W')\n",
    "    b = tf.Variable(tf.zeros([300]), name='b')\n",
    "    z = tf.matmul(x, w) + b\n",
    "    a = tf.nn.sigmoid(z)\n",
    " \n",
    "with tf.name_scope('hidden2'):\n",
    "    w = tf.Variable(tf.truncated_normal([300, 300]), name='W')\n",
    "    b = tf.Variable(tf.zeros([300]), name='b')\n",
    "    z = tf.matmul(x, w) + b\n",
    "    a = tf.nn.sigmoid(z)\n",
    "    \n",
    "with tf.name_scope('output'):\n",
    "    w = tf.Variable(tf.truncated_normal([300, 50]), name='W')\n",
    "    b = tf.Variable(tf.zeros([50]), name='b')\n",
    "    z = tf.matmul(a, w) + b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Looking at the above, we can start breaking down the patterns we see:\n",
    "\n",
    "* We always have weight and bias Variables, `w`, and `b`\n",
    "* We the first dimension of the weight variable should be the previous layer's depth\n",
    "* The depth of the layer can be any scalar\n",
    "* The output layer doesn't have an activation function\n",
    "* We'd like to group everything with a name scope.\n",
    "\n",
    "Additionally, there are some other features that we'd like to look out for:\n",
    "\n",
    "* Optionally use dropout\n",
    "* Want to be able to use any activation function we want\n",
    "* Want to be able to set the `w` standard deviation and `b` starting number\n",
    "\n",
    "Here's an example layer that achieves this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_layer(incoming_layer, num_nodes, \n",
    "                          activation_fn=tf.nn.sigmoid, w_stddev=0.5, \n",
    "                          b_val=0.0, \n",
    "                          keep_prob=None, name=None):\n",
    "    incoming_layer = tf.convert_to_tensor(incoming_layer)\n",
    "    prev_num_nodes = incoming_layer.shape.dims[-1].value\n",
    "    \n",
    "    with tf.name_scope(name, 'fully_connected'):\n",
    "        tn = tf.truncated_normal([prev_num_nodes, num_nodes], \n",
    "                                 stddev=w_stddev)\n",
    "        W = tf.Variable(tn, name='W')\n",
    "        const = tf.constant(b_val, shape=[num_nodes])\n",
    "        b = tf.Variable(const, name='bias')\n",
    "        \n",
    "        z = tf.matmul(incoming_layer, W) + b\n",
    "        \n",
    "        # using Python's if/else expression\n",
    "        # usually expect to have an activation - fallback to identity\n",
    "        # we'll expect keep_prob to be None, and replace if needed\n",
    "        a = activation_fn(z) if activation_fn is not None else z \n",
    "        final_a = a if keep_prob is None else tf.nn.dropout(a, keep_prob)\n",
    "        \n",
    "        return final_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None,20], name=\"x\")\n",
    "\n",
    "h1 = fully_connected_layer(x, 40, name='h1')\n",
    "h2 = fully_connected_layer(h1, 20, keep_prob=.8, name='h2')\n",
    "\n",
    "# fully connected from h2 to me (out)\n",
    "out = fully_connected_layer(h2, 10, name='out')\n",
    "\n",
    "print(h1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tensor`s have a [`TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) attribute that stores their size.  We use `incoming_layer.shape.dims[-1].value` to get the ouput size of the prior layer as a Python scalar.\n",
    "\n",
    "The way we use this is pretty straightforward. Here's how we would replicate the previous code that had a distinct cut-and-paste feel:\n",
    "\n",
    "    fc = fully_connected_layer(x, 300,  name='hidden1')\n",
    "    fc = fully_connected_layer(fc, 300, name='hidden2')\n",
    "    z  = fully_connected_layer(fc, 50,  \n",
    "                               activation_fn=None, name='output')\n",
    "\n",
    "Much more concise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The below code is another commonly used type of layer. It \"flattens\" a tensor (excluding the first dimension, which is the number of batches). For example, a matrix of ten 28x28 RGB images normally has a shape like this: `[10, 28, 28, 3]`. However, our model expects inputs to only be dimension 2. By flattening it, we \"string out\" the input pixels into a vector of length `28*28*3=2352`, so our final output shape is `[10, 2352]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(incoming, name=None):\n",
    "    flat_shape = [-1, np.prod(incoming.shape[1:]).value]\n",
    "    return tf.reshape(incoming, flat_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.uint8, shape=[None, 28, 28, 3], name=\"x\")\n",
    "flatten(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download and Open MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've defined some helper functions to pull down the MNIST data.  It's sort of a pain (and this isn't the prettiest code), but it works and it lets us focus on TensorFlow.  Here goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this reads to ndarrays\n",
    "(train_data, train_labels, \n",
    " test_data, test_labels) = helpers_04.get_mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape,\n",
    "      train_data[0].shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick look at what our primitive data looks like (literally):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img):\n",
    "    imgplot = plt.imshow(np.squeeze(img), cmap='Greys_r')\n",
    "\n",
    "# which image?  we could select at random \n",
    "# pick_me = np.random.randint(len(test_data))\n",
    "pick_me = 0\n",
    "show_image(test_data[pick_me])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get down to business and create a model that can recognize these images.  We're going to make heavy use of `batches`, `flatten`, and `fully_connected_layer` from above.  You might want to scroll back up and remind yourself what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Model(object):\n",
    "    def __init__(s):  # non-standard, for abbreviation\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            with tf.name_scope('inputs'):\n",
    "                s.x = tf.placeholder(tf.uint8, shape=[None, 28, 28, 1], name=\"x\")\n",
    "                s.y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "            with tf.name_scope('hyperparams'):\n",
    "                s.learning_rate = tf.placeholder(tf.float32, [], 'learning_rate')\n",
    "                s.momentum      = tf.placeholder(tf.float32, [], 'momentum')\n",
    "   \n",
    "                s.x_keep_prob = tf.placeholder(tf.float32, [], 'x_keep_prob')\n",
    "                s.h_keep_prob = tf.placeholder(tf.float32, [], 'h_keep_prob')\n",
    "            \n",
    "            with tf.name_scope('preprocess'):\n",
    "                x_flat  = flatten(s.x)\n",
    "                x_float = tf.cast(x_flat, tf.float32) \n",
    "                s.x_dropped = tf.nn.dropout(x_float, s.x_keep_prob)\n",
    "                s.one_hot_labels = tf.one_hot(s.y, 10)\n",
    "\n",
    "            with tf.name_scope('model'):\n",
    "                make_fc = fully_connected_layer # abbreviation\n",
    "                \n",
    "                s.h1  = make_fc(s.x_dropped, 1200, keep_prob=s.h_keep_prob, name='h1')                \n",
    "                s.h2  = make_fc(s.h1,        1200, keep_prob=s.h_keep_prob, name='h2')\n",
    "                s.h3  = make_fc(s.h2,        1200, keep_prob=s.h_keep_prob, name='h3')\n",
    "\n",
    "                s.out = make_fc(s.h3, 10, activation_fn=None, name='out') \n",
    "            \n",
    "            with tf.name_scope('loss'):\n",
    "                smce = tf.nn.softmax_cross_entropy_with_logits\n",
    "                s.loss = tf.reduce_mean(smce(logits=s.out, labels=s.one_hot_labels))\n",
    "\n",
    "            with tf.name_scope('train'):\n",
    "                # we're using a momentum optimizer to (hopefully)\n",
    "                # make faster progress in regions where we'd prefer to take bigger\n",
    "                # steps\n",
    "                opt = tf.train.MomentumOptimizer(s.learning_rate, s.momentum)\n",
    "                s.train = opt.minimize(s.loss)\n",
    "\n",
    "            with tf.name_scope('global_step'):\n",
    "                global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "                s.inc_step = tf.assign_add(global_step, 1, name='inc_step')\n",
    "            \n",
    "            with tf.name_scope('prediction'):\n",
    "                s.softmax = tf.nn.softmax(s.out, name=\"softmax\")\n",
    "                s.prediction = tf.cast(tf.arg_max(s.softmax, 1), tf.int32)\n",
    "                s.pred_correct = tf.equal(s.y, s.prediction)\n",
    "                s.pred_accuracy = tf.reduce_mean(tf.cast(s.pred_correct, tf.float32))    \n",
    "            s.init = tf.global_variables_initializer()\n",
    "            \n",
    "        s.session = tf.Session(graph=graph)\n",
    "        s.session.run(s.init)\n",
    "    \n",
    "    def fit(s, train_dict):\n",
    "        tr_loss, step, tr_acc, _ = s.session.run([s.loss, s.inc_step, s.pred_accuracy, \n",
    "                                                  s.train], \n",
    "                                              feed_dict=train_dict)\n",
    "        return tr_loss, step, tr_acc\n",
    "    \n",
    "    def predict(s, test_dict):\n",
    "        ct_correct, preds = s.session.run([s.pred_correct, s.prediction], \n",
    "                                          feed_dict=test_dict)\n",
    "        return ct_correct, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above\n",
    "(train_data, train_labels, \n",
    " test_data, test_labels) = helpers_04.get_mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MNIST_Model()\n",
    "for epoch in range(15):\n",
    "    for batch_data, batch_labels in batches(train_data, train_labels, 100):\n",
    "        m = min(.5 + epoch * 0.001, 0.99) # safeguard for very large # epochs\n",
    "        train_dict = {mm.x : batch_data, \n",
    "                      mm.y : batch_labels, \n",
    "\n",
    "                      mm.learning_rate : 0.1,\n",
    "                      mm.momentum      : m,\n",
    "\n",
    "                      mm.x_keep_prob : 0.8, \n",
    "                      mm.h_keep_prob : 0.5}\n",
    "        tr_loss, step, tr_acc = mm.fit(train_dict)\n",
    "    info_update = \"Epoch: {:2d} Step: {:5d} Loss: {:8.2f} Acc: {:5.2f}\"\n",
    "    print(info_update.format(epoch, step, tr_loss, tr_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test accuracy\n",
    "batch_correct_cts = []\n",
    "for batch_data, batch_labels in batches(test_data, test_labels, 200):\n",
    "    test_dict = {mm.x : batch_data,   mm.x_keep_prob : 1.0,\n",
    "                 mm.y : batch_labels, mm.h_keep_prob : 1.0}\n",
    "    correctness, curr_preds = mm.predict(test_dict)\n",
    "    batch_correct_cts.append(correctness.sum())\n",
    "\n",
    "print(sum(batch_correct_cts) / len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_show_images(data, labels):\n",
    "    test_dict = {mm.x : data,   mm.x_keep_prob : 1.0,\n",
    "                 mm.y : labels, mm.h_keep_prob : 1.0}\n",
    "    \n",
    "    correctness, curr_preds = mm.predict(test_dict)\n",
    "    \n",
    "    # setup axes\n",
    "    fig, axes = plt.subplots(nrows=(len(data) // 3) + 1, ncols=3, figsize=(9,len(data)*1.5))\n",
    "    axes = axes.flat\n",
    "    for d, lbl, c, p, ax in zip(data, labels, correctness, curr_preds, axes):\n",
    "        ax.imshow(np.squeeze(d), cmap='Greys_r') # FIXME: is cmap needed?\n",
    "        ax.set_title(\"Predicted: {}\\nCorrect: {}\".format(p, bool(c)))\n",
    "        ax.axis('off')\n",
    "    for ax in axes: ax.set_visible(False)\n",
    "\n",
    "indices = [25, 125]\n",
    "test_and_show_images(test_data[indices], test_labels[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get correct and incorrect images\n",
    "correctness = []\n",
    "for batch_data, batch_labels in batches(test_data, test_labels, 200):\n",
    "    test_dict = {mm.x : batch_data,   mm.x_keep_prob : 1.0,\n",
    "                 mm.y : batch_labels, mm.h_keep_prob : 1.0}\n",
    "    curr_correctness, curr_preds = mm.predict(test_dict)\n",
    "    correctness.extend(curr_correctness)\n",
    "\n",
    "incorrect = np.where(np.logical_not(np.array(correctness, dtype=np.bool)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the incorrect images is very interesting!\n",
    "indices = incorrect[:12]\n",
    "test_and_show_images(test_data[indices], test_labels[indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model above, there are actually several buried \"magic\" constants.  Neural networks can be very sensitive to these values.  As an example, here are some magic values we used above:\n",
    "\n",
    "|activation|w_stddev|b_val|learning_rate|momentum|train acc|epochs|test acc|\n",
    "|-|-|-|-|-|\n",
    "|sigmoid|.5|0.0|0.1|`min()` expr|mid 40s|11| 65%|\n",
    "\n",
    "Note, that in a developmental accident, using that scenario and applying a higher keep_prob to the first hidden layer (`s.h1`) resulted in a slightly improved, and more stable (making progress), rate of convergence.  Many of these \"minor\" tweaks aren't that minor!\n",
    "\n",
    "Now, try this:  Instead of using `sigmoid`s for your activation function of the fully connected layers, try using `tf.nn.relu` activations.  I'll give you two hints.  In addition, set `w_stddev=0.001` and  `bval=0.1`.  See how your network performs.\n",
    "\n",
    "Now, there may have been \"issues\".  Try adjusting your learning rate to something smaller.  See how high a testing accuracy you can get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the above, by replacing a few arguments in the `fully_connected_layer` function:\n",
    "\n",
    "    activation_fn=tf.nn.relu, w_stddev=0.001, b_val=0.1,\n",
    "\n",
    "You could also achieve this by passing specific arguments into the function calls (`make_fc`) to create the hidden layers in the `__init__` method of the model.  There is an examle of this in the CIFAR model exercise below (see the assignment to `shared_args`).\n",
    "\n",
    "Your results might differ (because we have random initialization and batching steps), but here are some results from an `relu` based run:\n",
    "\n",
    "|activation|w_stddev|b_val|learning_rate|momentum|train acc|epochs|test acc|\n",
    "|-|-|-|-|-|\n",
    "|relu|.001|0.1|0.01|`min()` expr|high 90s|3|98%|\n",
    "\n",
    "Note, the test accuracy was after doing a full 15 epochs of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CIFAR 10\n",
    "\n",
    "The [CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) datasets consist of images comprising 10 or 100 different classes, respectively.  We can train and test from CIFAR in almost the exact same way as we did with MNIST.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers_04\n",
    "(train_data, train_labels,\n",
    "            test_data, test_labels), readable_labels = helpers_04.load_cifar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll want to look at the shape of the training/testing data.  In particular, it is going to come in pre-flattened.  So, you will have to adjust the shape you use with your models input `x` values (a `placeholder`).  Other than that, you can adjust the number of layers and the size of the layers, if you'd like.  Once you get a working model, you can experiment any number of different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
