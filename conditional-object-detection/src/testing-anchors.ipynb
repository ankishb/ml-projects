{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/ankish1/anaconda3/envs/tensor/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/ankish1/anaconda3/envs/tensor/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/ankish1/anaconda3/envs/tensor/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\nImportError: dlopen: cannot load any more object with static TLS\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-89783fb96834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_annotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ankish1/ankish_save/yolo/retinaNet/keras-yolo2/model-ipynb-file/preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimgaug\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimgaug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maugmenters\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0miaa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mElementTree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBoundBox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_iou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ankish1/anaconda3/envs/tensor/lib/python2.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ankish1/anaconda3/envs/tensor/lib/python2.7/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Globally-importable utils.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ankish1/anaconda3/envs/tensor/lib/python2.7/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ankish1/anaconda3/envs/tensor/lib/python2.7/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Try and load external backend.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ankish1/anaconda3/envs/tensor/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ankish1/anaconda3/envs/tensor/lib/python2.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ankish1/anaconda3/envs/tensor/lib/python2.7/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomponent_api_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ankish1/anaconda3/envs/tensor/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/home/ankish1/anaconda3/envs/tensor/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/ankish1/anaconda3/envs/tensor/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/ankish1/anaconda3/envs/tensor/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\nImportError: dlopen: cannot load any more object with static TLS\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from preprocessing import parse_annotation\n",
    "import json\n",
    "\n",
    "argparser = argparse.ArgumentParser()\n",
    "\n",
    "argparser.add_argument(\n",
    "    '-c',\n",
    "    '--conf',\n",
    "    default='config.json',\n",
    "    help='path to configuration file')\n",
    "\n",
    "argparser.add_argument(\n",
    "    '-a',\n",
    "    '--anchors',\n",
    "    default=5,\n",
    "    help='number of anchors to use')\n",
    "\n",
    "def IOU(ann, centroids):\n",
    "    w, h = ann\n",
    "    similarities = []\n",
    "\n",
    "    for centroid in centroids:\n",
    "        c_w, c_h = centroid\n",
    "\n",
    "        if c_w >= w and c_h >= h:\n",
    "            similarity = w*h/(c_w*c_h)\n",
    "        elif c_w >= w and c_h <= h:\n",
    "            similarity = w*c_h/(w*h + (c_w-w)*c_h)\n",
    "        elif c_w <= w and c_h >= h:\n",
    "            similarity = c_w*h/(w*h + c_w*(c_h-h))\n",
    "        else: #means both w,h are bigger than c_w and c_h respectively\n",
    "            similarity = (c_w*c_h)/(w*h)\n",
    "        similarities.append(similarity) # will become (k,) shape\n",
    "\n",
    "    return np.array(similarities)\n",
    "\n",
    "def avg_IOU(anns, centroids):\n",
    "    n,d = anns.shape\n",
    "    sum = 0.\n",
    "\n",
    "    for i in range(anns.shape[0]):\n",
    "        sum+= max(IOU(anns[i], centroids))\n",
    "\n",
    "    return sum/n\n",
    "\n",
    "def print_anchors(centroids):\n",
    "    anchors = centroids.copy()\n",
    "\n",
    "    widths = anchors[:, 0]\n",
    "    sorted_indices = np.argsort(widths)\n",
    "\n",
    "    r = \"anchors: [\"\n",
    "    for i in sorted_indices[:-1]:\n",
    "        r += '%0.2f,%0.2f, ' % (anchors[i,0], anchors[i,1])\n",
    "\n",
    "    #there should not be comma after last anchor, that's why\n",
    "    r += '%0.2f,%0.2f' % (anchors[sorted_indices[-1:],0], anchors[sorted_indices[-1:],1])\n",
    "    r += \"]\"\n",
    "\n",
    "    print(r)\n",
    "    return r\n",
    "\n",
    "def run_kmeans(ann_dims, anchor_num):\n",
    "    ann_num = ann_dims.shape[0]\n",
    "    iterations = 0\n",
    "    prev_assignments = np.ones(ann_num)*(-1)\n",
    "    iteration = 0\n",
    "    old_distances = np.zeros((ann_num, anchor_num))\n",
    "\n",
    "    indices = [random.randrange(ann_dims.shape[0]) for i in range(anchor_num)]\n",
    "    centroids = ann_dims[indices]\n",
    "    anchor_dim = ann_dims.shape[1]\n",
    "\n",
    "    while True:\n",
    "        distances = []\n",
    "        iteration += 1\n",
    "        for i in range(ann_num):\n",
    "            d = 1 - IOU(ann_dims[i], centroids)\n",
    "            distances.append(d)\n",
    "        distances = np.array(distances) # distances.shape = (ann_num, anchor_num)\n",
    "\n",
    "        print(\"iteration {}: dists = {}\".format(iteration, np.sum(np.abs(old_distances-distances))))\n",
    "\n",
    "        #assign samples to centroids\n",
    "        assignments = np.argmin(distances,axis=1)\n",
    "\n",
    "        if (assignments == prev_assignments).all() :\n",
    "            return centroids\n",
    "\n",
    "        #calculate new centroids\n",
    "        centroid_sums=np.zeros((anchor_num, anchor_dim), np.float)\n",
    "        for i in range(ann_num):\n",
    "            centroid_sums[assignments[i]]+=ann_dims[i]\n",
    "        for j in range(anchor_num):\n",
    "            centroids[j] = centroid_sums[j]/(np.sum(assignments==j) + 1e-6)\n",
    "\n",
    "        prev_assignments = assignments.copy()\n",
    "        old_distances = distances.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    config_path = \"/home/ankish/kaggle_data_science/yolo3/retinaNet/keras-yolo2/config.json\"#args.conf\n",
    "    num_anchors = 10#args.anchors\n",
    "\n",
    "    with open(config_path) as config_buffer:\n",
    "        config = json.loads(config_buffer.read())\n",
    "\n",
    "    train_imgs, train_labels = parse_annotation(config['train']['train_annot_folder'],\n",
    "                                                config['train']['train_image_folder'],\n",
    "                                                config['model']['labels'])\n",
    "\n",
    "    grid_w = config['model']['input_size']/32\n",
    "    grid_h = config['model']['input_size']/32\n",
    "\n",
    "    # run k_mean to find the anchors\n",
    "    annotation_dims = []\n",
    "    for image in train_imgs:\n",
    "        cell_w = image['width']/grid_w\n",
    "        cell_h = image['height']/grid_h\n",
    "\n",
    "        for obj in image['object']:\n",
    "            relative_w = (float(obj['xmax']) - float(obj['xmin']))/cell_w\n",
    "            relatice_h = (float(obj[\"ymax\"]) - float(obj['ymin']))/cell_h\n",
    "            annotation_dims.append(tuple(map(float, (relative_w,relatice_h))))\n",
    "\n",
    "    annotation_dims = np.array(annotation_dims)\n",
    "    print(\"annotation_dims: \", annotation_dims)\n",
    "    centroids = run_kmeans(annotation_dims, num_anchors)\n",
    "\n",
    "    # write anchors to file\n",
    "    print('\\naverage IOU for', num_anchors, 'anchors:', '%0.2f' % avg_IOU(annotation_dims, centroids))\n",
    "    print_anchors(centroids)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     args = argparser.parse_args()\n",
    "#     main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object': [{'name': 'raccoon', 'xmin': 100, 'ymin': 124, 'xmax': 266, 'ymax': 324}, {'name': 'raccoon', 'xmin': 342, 'ymin': 101, 'xmax': 570, 'ymax': 297}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-117.jpg', 'width': 640, 'height': 448}\n",
      "2\n",
      "=====\n",
      "{'object': [{'name': 'raccoon', 'xmin': 16, 'ymin': 62, 'xmax': 362, 'ymax': 353}, {'name': 'raccoon', 'xmin': 211, 'ymin': 359, 'xmax': 277, 'ymax': 402}, {'name': 'raccoon', 'xmin': 198, 'ymin': 392, 'xmax': 280, 'ymax': 473}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-119.jpg', 'width': 400, 'height': 533}\n",
      "3\n",
      "=====\n",
      "{'object': [{'name': 'raccoon', 'xmin': 28, 'ymin': 21, 'xmax': 126, 'ymax': 181}, {'name': 'raccoon', 'xmin': 85, 'ymin': 33, 'xmax': 235, 'ymax': 193}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-12.jpg', 'width': 259, 'height': 194}\n",
      "2\n",
      "=====\n",
      "{'object': [{'name': 'raccoon', 'xmin': 223, 'ymin': 62, 'xmax': 497, 'ymax': 307}, {'name': 'raccoon', 'xmin': 453, 'ymin': 41, 'xmax': 640, 'ymax': 423}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-130.jpg', 'width': 640, 'height': 426}\n",
      "2\n",
      "=====\n",
      "{'object': [{'name': 'raccoon', 'xmin': 3, 'ymin': 36, 'xmax': 345, 'ymax': 450}, {'name': 'raccoon', 'xmin': 260, 'ymin': 41, 'xmax': 569, 'ymax': 449}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-145.jpg', 'width': 600, 'height': 450}\n",
      "2\n",
      "=====\n",
      "{'object': [{'name': 'raccoon', 'xmin': 32, 'ymin': 177, 'xmax': 174, 'ymax': 316}, {'name': 'raccoon', 'xmin': 309, 'ymin': 172, 'xmax': 428, 'ymax': 315}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-148.jpg', 'width': 500, 'height': 375}\n",
      "2\n",
      "=====\n",
      "{'object': [{'name': 'raccoon', 'xmin': 98, 'ymin': 88, 'xmax': 374, 'ymax': 303}, {'name': 'raccoon', 'xmin': 173, 'ymin': 1, 'xmax': 471, 'ymax': 309}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-168.jpg', 'width': 628, 'height': 314}\n",
      "2\n",
      "=====\n",
      "{'object': [{'name': 'raccoon', 'xmin': 308, 'ymin': 90, 'xmax': 611, 'ymax': 426}, {'name': 'raccoon', 'xmin': 103, 'ymin': 1, 'xmax': 314, 'ymax': 189}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-176.jpg', 'width': 800, 'height': 533}\n",
      "2\n",
      "=====\n",
      "{'object': [{'name': 'raccoon', 'xmin': 8, 'ymin': 18, 'xmax': 157, 'ymax': 178}, {'name': 'raccoon', 'xmin': 146, 'ymin': 13, 'xmax': 263, 'ymax': 146}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-177.jpg', 'width': 276, 'height': 183}\n",
      "2\n",
      "=====\n",
      "{'object': [{'name': 'raccoon', 'xmin': 57, 'ymin': 21, 'xmax': 158, 'ymax': 184}, {'name': 'raccoon', 'xmin': 112, 'ymin': 32, 'xmax': 199, 'ymax': 158}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-198.jpg', 'width': 259, 'height': 194}\n",
      "2\n",
      "=====\n",
      "{'object': [{'name': 'raccoon', 'xmin': 77, 'ymin': 48, 'xmax': 179, 'ymax': 156}, {'name': 'raccoon', 'xmin': 139, 'ymin': 77, 'xmax': 202, 'ymax': 145}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-24.jpg', 'width': 268, 'height': 188}\n",
      "2\n",
      "=====\n",
      "{'object': [{'name': 'raccoon', 'xmin': 82, 'ymin': 21, 'xmax': 187, 'ymax': 197}, {'name': 'raccoon', 'xmin': 11, 'ymin': 55, 'xmax': 80, 'ymax': 145}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-31.jpg', 'width': 236, 'height': 214}\n",
      "2\n",
      "=====\n",
      "{'object': [{'name': 'raccoon', 'xmin': 6, 'ymin': 49, 'xmax': 250, 'ymax': 320}, {'name': 'raccoon', 'xmin': 274, 'ymin': 27, 'xmax': 563, 'ymax': 410}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-55.jpg', 'width': 634, 'height': 417}\n",
      "2\n",
      "=====\n",
      "{'object': [{'name': 'raccoon', 'xmin': 94, 'ymin': 63, 'xmax': 195, 'ymax': 148}, {'name': 'raccoon', 'xmin': 142, 'ymin': 39, 'xmax': 213, 'ymax': 108}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-61.jpg', 'width': 274, 'height': 184}\n",
      "2\n",
      "=====\n",
      "{'object': [{'name': 'raccoon', 'xmin': 74, 'ymin': 107, 'xmax': 280, 'ymax': 290}, {'name': 'raccoon', 'xmin': 227, 'ymin': 93, 'xmax': 403, 'ymax': 298}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-63.jpg', 'width': 600, 'height': 400}\n",
      "2\n",
      "=====\n",
      "{'object': [{'name': 'raccoon', 'xmin': 219, 'ymin': 195, 'xmax': 446, 'ymax': 375}, {'name': 'raccoon', 'xmin': 98, 'ymin': 34, 'xmax': 284, 'ymax': 336}], 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-72.jpg', 'width': 560, 'height': 420}\n",
      "2\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "config_path = \"/home/ankish/kaggle_data_science/yolo3/retinaNet/keras-yolo2/config.json\"#args.conf\n",
    "num_anchors = 10#args.anchors\n",
    "\n",
    "with open(config_path) as config_buffer:\n",
    "    config = json.loads(config_buffer.read())\n",
    "\n",
    "# train_imgs, train_labels = parse_annotation(config['train']['train_annot_folder'],\n",
    "#                                             config['train']['train_image_folder'],\n",
    "#                                             config['model']['labels'])\n",
    "img_dir = '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/'\n",
    "ann_dir = '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/annotations/'\n",
    "train_imgs, train_labels = parse_annotation(ann_dir, img_dir)\n",
    "\n",
    "grid_w = config['model']['input_size']/32\n",
    "grid_h = config['model']['input_size']/32\n",
    "\n",
    "# run k_mean to find the anchors\n",
    "annotation_dims = []\n",
    "for image in train_imgs:\n",
    "    cell_w = image['width']/grid_w\n",
    "    cell_h = image['height']/grid_h\n",
    "    \n",
    "    count = 0\n",
    "    for obj in image['object']:\n",
    "        relative_w = (float(obj['xmax']) - float(obj['xmin']))/cell_w\n",
    "        relatice_h = (float(obj[\"ymax\"]) - float(obj['ymin']))/cell_h\n",
    "#         print(map(float, (relative_w,relatice_h)))\n",
    "#         print(tuple(map(float, (relative_w,relatice_h))))\n",
    "        annotation_dims.append(tuple(map(float, (relative_w,relatice_h))))\n",
    "#         break\n",
    "        count += 1\n",
    "    if count > 1:\n",
    "        print(image)\n",
    "        print(count)\n",
    "        print(\"=====\")\n",
    "#     break\n",
    "annotation_dims = np.array(annotation_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1: dists = 1094.1701425230578\n",
      "iteration 2: dists = 95.89416794976658\n",
      "iteration 3: dists = 32.07709807236874\n",
      "iteration 4: dists = 20.83417006580313\n",
      "iteration 5: dists = 24.243086241055863\n",
      "iteration 6: dists = 32.33465601254913\n",
      "iteration 7: dists = 17.51010394986301\n",
      "iteration 8: dists = 7.707568520988861\n",
      "iteration 9: dists = 6.945878415743149\n",
      "iteration 10: dists = 7.349986317484446\n",
      "iteration 11: dists = 4.101095988896969\n",
      "iteration 12: dists = 7.470390345297306\n",
      "iteration 13: dists = 4.446576130959513\n",
      "iteration 14: dists = 5.122952249932992\n",
      "iteration 15: dists = 3.39908093372861\n",
      "iteration 16: dists = 0.9217926706395791\n",
      "iteration 17: dists = 0.9309140894985454\n",
      "\n",
      "average IOU for 10 anchors: 0.87\n",
      "anchors: [2.40,1.51, 3.54,5.08, 4.42,8.37, 4.76,5.34, 5.90,10.70, 6.85,7.26, 7.48,11.40, 9.23,11.86, 10.27,8.97, 11.60,12.02]\n"
     ]
    }
   ],
   "source": [
    "# print(\"annotation_dims: \", annotation_dims)\n",
    "centroids = run_kmeans(annotation_dims, num_anchors)\n",
    "\n",
    "# write anchors to file\n",
    "print('\\naverage IOU for', num_anchors, 'anchors:', '%0.2f' % avg_IOU(annotation_dims, centroids))\n",
    "print_anchors(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchors: [2.40,1.51, 3.54,5.08, 4.42,8.37, 4.76,5.34, 5.90,10.70, 6.85,7.26, 7.48,11.40, 9.23,11.86, 10.27,8.97, 11.60,12.02]\n"
     ]
    }
   ],
   "source": [
    "anchors = print_anchors(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12.02,\n",
       " 11.86,\n",
       " 11.6,\n",
       " 11.4,\n",
       " 10.7,\n",
       " 10.27,\n",
       " 9.23,\n",
       " 8.97,\n",
       " 8.37,\n",
       " 7.48,\n",
       " 7.26,\n",
       " 6.85,\n",
       " 5.9,\n",
       " 5.34,\n",
       " 5.08,\n",
       " 4.76,\n",
       " 4.42,\n",
       " 3.54,\n",
       " 2.4,\n",
       " 1.51]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anc = [2.40,1.51, 3.54,5.08, 4.42,8.37, 4.76,5.34, 5.90,10.70, 6.85,7.26, 7.48,11.40, 9.23,11.86, 10.27,8.97, 11.60,12.02]\n",
    "sorted(anc, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((217, 2), 13.0, 13.0, 13.0)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_dims.shape, grid_h, grid_w, 416/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/annotations/'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['train']['train_annot_folder']#['train_image_folder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ann in sorted(os.listdir(ann_dir)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element 'annotation' at 0x7ff3f806f778>\n",
      "<Element 'folder' at 0x7ff3f806f7c8>\n",
      "<Element 'filename' at 0x7ff3f806f818>\n",
      "<Element 'path' at 0x7ff3f806f868>\n",
      "<Element 'source' at 0x7ff3f806f8b8>\n",
      "<Element 'database' at 0x7ff3f806f908>\n",
      "<Element 'size' at 0x7ff3f806f958>\n",
      "<Element 'width' at 0x7ff3f806f9a8>\n",
      "<Element 'height' at 0x7ff3f806f9f8>\n",
      "<Element 'depth' at 0x7ff3f806fa48>\n",
      "<Element 'segmented' at 0x7ff3f806fa98>\n",
      "<Element 'object' at 0x7ff3f806fae8>\n",
      "<Element 'name' at 0x7ff3f806fb38>\n",
      "<Element 'pose' at 0x7ff3f806fb88>\n",
      "<Element 'truncated' at 0x7ff3f806fbd8>\n",
      "<Element 'difficult' at 0x7ff3f806fc28>\n",
      "<Element 'bndbox' at 0x7ff3f806fc78>\n",
      "<Element 'xmin' at 0x7ff3f806fcc8>\n",
      "<Element 'ymin' at 0x7ff3f806fd18>\n",
      "<Element 'xmax' at 0x7ff3f806fd68>\n",
      "<Element 'ymax' at 0x7ff3f806fdb8>\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/annotations/raccoon-85.xml')\n",
    "for elem in tree.iter():\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotation(ann_dir, img_dir, labels=[]):\n",
    "    all_imgs = []\n",
    "    seen_labels = {}\n",
    "    \n",
    "    for ann in sorted(os.listdir(ann_dir)):\n",
    "        img = {'object':[]}\n",
    "\n",
    "        tree = ET.parse(ann_dir + ann)\n",
    "        \n",
    "        for elem in tree.iter():\n",
    "            if 'filename' in elem.tag:\n",
    "                img['filename'] = img_dir + elem.text\n",
    "            if 'width' in elem.tag:\n",
    "                img['width'] = int(elem.text)\n",
    "            if 'height' in elem.tag:\n",
    "                img['height'] = int(elem.text)\n",
    "            if 'object' in elem.tag or 'part' in elem.tag:\n",
    "                obj = {}\n",
    "                \n",
    "                for attr in list(elem):\n",
    "                    if 'name' in attr.tag:\n",
    "                        obj['name'] = attr.text\n",
    "\n",
    "                        if obj['name'] in seen_labels:\n",
    "                            seen_labels[obj['name']] += 1\n",
    "                        else:\n",
    "                            seen_labels[obj['name']] = 1\n",
    "                        \n",
    "                        if len(labels) > 0 and obj['name'] not in labels:\n",
    "                            break\n",
    "                        else:\n",
    "                            img['object'] += [obj]\n",
    "                            \n",
    "                    if 'bndbox' in attr.tag:\n",
    "                        for dim in list(attr):\n",
    "                            if 'xmin' in dim.tag:\n",
    "                                obj['xmin'] = int(round(float(dim.text)))\n",
    "                            if 'ymin' in dim.tag:\n",
    "                                obj['ymin'] = int(round(float(dim.text)))\n",
    "                            if 'xmax' in dim.tag:\n",
    "                                obj['xmax'] = int(round(float(dim.text)))\n",
    "                            if 'ymax' in dim.tag:\n",
    "                                obj['ymax'] = int(round(float(dim.text)))\n",
    "\n",
    "        if len(img['object']) > 0:\n",
    "            all_imgs += [img]\n",
    "                        \n",
    "    return all_imgs, seen_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/imagesraccoon-85.jpg\n",
      "620\n",
      "465\n"
     ]
    }
   ],
   "source": [
    "img_dir = '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images'\n",
    "tree = ET.parse('/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/annotations/raccoon-85.xml')\n",
    "# for elem in tree.iter():\n",
    "#     print(elem)\n",
    "#     if 'filename' in elem.tag:\n",
    "#         print(elem.text)\n",
    "#         img['filename'] = img_dir + elem.text\n",
    "    \n",
    "img = {'object':[]}\n",
    "\n",
    "for elem in tree.iter():\n",
    "    if 'filename' in elem.tag:\n",
    "        print(img_dir + elem.text)\n",
    "        img['filename'] = img_dir + elem.text\n",
    "    if 'width' in elem.tag:\n",
    "        print(int(elem.text))\n",
    "        img['width'] = int(elem.text)\n",
    "    if 'height' in elem.tag:\n",
    "        print(int(elem.text))\n",
    "        img['height'] = int(elem.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/'\n",
    "ann_dir = '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/annotations/'\n",
    "imgs, labels = parse_annotation(ann_dir, img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': [{'name': 'raccoon',\n",
       "   'xmin': 81,\n",
       "   'ymin': 88,\n",
       "   'xmax': 522,\n",
       "   'ymax': 408}],\n",
       " 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-1.jpg',\n",
       " 'width': 650,\n",
       " 'height': 417}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': [{'name': 'raccoon',\n",
       "   'xmin': 130,\n",
       "   'ymin': 2,\n",
       "   'xmax': 446,\n",
       "   'ymax': 488}],\n",
       " 'filename': '/home/ankish/kaggle_data_science/yolo3/retinaNet/raccoon_dataset/images/raccoon-10.jpg',\n",
       " 'width': 450,\n",
       " 'height': 495}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dumps = json.dumps(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>height</th>\n",
       "      <th>object</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/ankish/kaggle_data_science/yolo3/retinaN...</td>\n",
       "      <td>417</td>\n",
       "      <td>[{'name': 'raccoon', 'xmin': 81, 'ymin': 88, '...</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/ankish/kaggle_data_science/yolo3/retinaN...</td>\n",
       "      <td>495</td>\n",
       "      <td>[{'name': 'raccoon', 'xmin': 130, 'ymin': 2, '...</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/ankish/kaggle_data_science/yolo3/retinaN...</td>\n",
       "      <td>576</td>\n",
       "      <td>[{'name': 'raccoon', 'xmin': 548, 'ymin': 10, ...</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/ankish/kaggle_data_science/yolo3/retinaN...</td>\n",
       "      <td>426</td>\n",
       "      <td>[{'name': 'raccoon', 'xmin': 86, 'ymin': 53, '...</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/ankish/kaggle_data_science/yolo3/retinaN...</td>\n",
       "      <td>194</td>\n",
       "      <td>[{'name': 'raccoon', 'xmin': 1, 'ymin': 1, 'xm...</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  height  \\\n",
       "0  /home/ankish/kaggle_data_science/yolo3/retinaN...     417   \n",
       "1  /home/ankish/kaggle_data_science/yolo3/retinaN...     495   \n",
       "2  /home/ankish/kaggle_data_science/yolo3/retinaN...     576   \n",
       "3  /home/ankish/kaggle_data_science/yolo3/retinaN...     426   \n",
       "4  /home/ankish/kaggle_data_science/yolo3/retinaN...     194   \n",
       "\n",
       "                                              object  width  \n",
       "0  [{'name': 'raccoon', 'xmin': 81, 'ymin': 88, '...    650  \n",
       "1  [{'name': 'raccoon', 'xmin': 130, 'ymin': 2, '...    450  \n",
       "2  [{'name': 'raccoon', 'xmin': 548, 'ymin': 10, ...    960  \n",
       "3  [{'name': 'raccoon', 'xmin': 86, 'ymin': 53, '...    640  \n",
       "4  [{'name': 'raccoon', 'xmin': 1, 'ymin': 1, 'xm...    259  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(json_dumps)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'raccoon', 'xmin': 81, 'ymin': 88, 'xmax': 522, 'ymax': 408}]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['object'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmins, ymins, xmaxs, ymaxs, names = [], [], [], [], []\n",
    "for neww in df['object']:\n",
    "    xmins.append(neww[0]['xmin'])\n",
    "    ymins.append(neww[0]['ymin'])\n",
    "    xmaxs.append(neww[0]['xmax'])\n",
    "    ymaxs.append(neww[0]['ymax'])\n",
    "    names.append(neww[0]['name'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>xmin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymin</th>\n",
       "      <th>ymax</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/ankish/kaggle_data_science/yolo3/retinaN...</td>\n",
       "      <td>417</td>\n",
       "      <td>650</td>\n",
       "      <td>81</td>\n",
       "      <td>522</td>\n",
       "      <td>88</td>\n",
       "      <td>408</td>\n",
       "      <td>raccoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/ankish/kaggle_data_science/yolo3/retinaN...</td>\n",
       "      <td>495</td>\n",
       "      <td>450</td>\n",
       "      <td>130</td>\n",
       "      <td>446</td>\n",
       "      <td>2</td>\n",
       "      <td>488</td>\n",
       "      <td>raccoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/ankish/kaggle_data_science/yolo3/retinaN...</td>\n",
       "      <td>576</td>\n",
       "      <td>960</td>\n",
       "      <td>548</td>\n",
       "      <td>954</td>\n",
       "      <td>10</td>\n",
       "      <td>520</td>\n",
       "      <td>raccoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/ankish/kaggle_data_science/yolo3/retinaN...</td>\n",
       "      <td>426</td>\n",
       "      <td>640</td>\n",
       "      <td>86</td>\n",
       "      <td>400</td>\n",
       "      <td>53</td>\n",
       "      <td>356</td>\n",
       "      <td>raccoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/ankish/kaggle_data_science/yolo3/retinaN...</td>\n",
       "      <td>194</td>\n",
       "      <td>259</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>raccoon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  height  width  xmin  \\\n",
       "0  /home/ankish/kaggle_data_science/yolo3/retinaN...     417    650    81   \n",
       "1  /home/ankish/kaggle_data_science/yolo3/retinaN...     495    450   130   \n",
       "2  /home/ankish/kaggle_data_science/yolo3/retinaN...     576    960   548   \n",
       "3  /home/ankish/kaggle_data_science/yolo3/retinaN...     426    640    86   \n",
       "4  /home/ankish/kaggle_data_science/yolo3/retinaN...     194    259     1   \n",
       "\n",
       "   xmax  ymin  ymax     name  \n",
       "0   522    88   408  raccoon  \n",
       "1   446     2   488  raccoon  \n",
       "2   954    10   520  raccoon  \n",
       "3   400    53   356  raccoon  \n",
       "4   118     1   152  raccoon  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df,pd.Series(data=xmins,name='xmin')],axis=1)\n",
    "df = pd.concat([df,pd.Series(data=xmaxs,name='xmax')],axis=1)\n",
    "df = pd.concat([df,pd.Series(data=ymins,name='ymin')],axis=1)\n",
    "df = pd.concat([df,pd.Series(data=ymaxs,name='ymax')],axis=1)\n",
    "df = pd.concat([df,pd.Series(data=names,name='name')],axis=1)\n",
    "\n",
    "df.drop(['object'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "FULL_YOLO_BACKEND_PATH  = \"full_yolo_backend.h5\"   # should be hosted on a server\n",
    "TINY_YOLO_BACKEND_PATH  = \"tiny_yolo_backend.h5\"   # should be hosted on a server\n",
    "SQUEEZENET_BACKEND_PATH = \"squeezenet_backend.h5\"  # should be hosted on a server\n",
    "MOBILENET_BACKEND_PATH  = \"mobilenet_backend.h5\"   # should be hosted on a server\n",
    "INCEPTION3_BACKEND_PATH = \"inception_backend.h5\"   # should be hosted on a server\n",
    "VGG16_BACKEND_PATH      = \"vgg16_backend.h5\"       # should be hosted on a server\n",
    "RESNET50_BACKEND_PATH   = \"resnet50_backend.h5\"    # should be hosted on a server\n",
    "\n",
    "class BaseFeatureExtractor(object):\n",
    "    \"\"\"docstring for ClassName\"\"\"\n",
    "\n",
    "    # to be defined in each subclass\n",
    "    def __init__(self, input_size):\n",
    "        raise NotImplementedError(\"error message\")\n",
    "\n",
    "    # to be defined in each subclass\n",
    "    def normalize(self, image):\n",
    "        raise NotImplementedError(\"error message\")       \n",
    "\n",
    "    def get_output_shape(self):\n",
    "        return self.feature_extractor.get_output_shape_at(-1)[1:3]\n",
    "\n",
    "    def extract(self, input_image):\n",
    "        return self.feature_extractor(input_image)\n",
    "\n",
    "    \n",
    "class MobileNetFeature(BaseFeatureExtractor):\n",
    "    \"\"\"docstring for ClassName\"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        input_image = Input(shape=(input_size, input_size, 3))\n",
    "\n",
    "        mobilenet = MobileNet(input_shape=(224,224,3), include_top=False, weights=None)\n",
    "        mobilenet.load_weights('mobile-net-yolo.h5')\n",
    "\n",
    "        x = mobilenet(input_image)\n",
    "\n",
    "        self.feature_extractor = Model(input_image, x)  \n",
    "\n",
    "    def normalize(self, image):\n",
    "        image = image / 255.\n",
    "        image = image - 0.5\n",
    "        image = image * 2.\n",
    "\n",
    "        return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 13)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 416, 416, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 13, 13, 1024) 3228864     input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "DetectionLayer (Conv2D)         (None, 13, 13, 75)   76875       model_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 13, 13, 5, 15 0           DetectionLayer[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 1, 1, 1, 10,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 13, 13, 5, 15 0           reshape_1[0][0]                  \n",
      "                                                                 input_14[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,305,739\n",
      "Trainable params: 3,283,851\n",
      "Non-trainable params: 21,888\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from utils import decode_netout, compute_overlap, compute_ap\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from preprocessing import BatchGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "# from backend import MobileNetFeature \n",
    "\n",
    "input_size = 416\n",
    "max_box_per_image = 10\n",
    "nb_box = 5\n",
    "nb_class = 10\n",
    "\n",
    "\n",
    "input_image     = Input(shape=(input_size, input_size, 3))\n",
    "true_boxes = Input(shape=(1, 1, 1, max_box_per_image , 4))  \n",
    "\n",
    "feature_extractor = MobileNetFeature(input_size)\n",
    "\n",
    "print(feature_extractor.get_output_shape())    \n",
    "grid_h, grid_w = feature_extractor.get_output_shape()        \n",
    "features = feature_extractor.extract(input_image)            \n",
    "\n",
    "# make the object detection layer\n",
    "output = Conv2D(nb_box * (4 + 1 + nb_class), \n",
    "                (1,1), strides=(1,1), \n",
    "                padding='same', \n",
    "                name='DetectionLayer', \n",
    "                kernel_initializer='lecun_normal')(features)\n",
    "output = Reshape((grid_h, grid_w, nb_box, 4 + 1 + nb_class))(output)\n",
    "output = Lambda(lambda args: args[0])([output, true_boxes])\n",
    "\n",
    "model = Model([input_image, true_boxes], output)\n",
    "\n",
    "\n",
    "# initialize the weights of the detection layer\n",
    "layer = model.layers[-4]\n",
    "weights = layer.get_weights()\n",
    "\n",
    "new_kernel = np.random.normal(size=weights[0].shape)/(grid_h*grid_w)\n",
    "new_bias   = np.random.normal(size=weights[1].shape)/(grid_h*grid_w)\n",
    "\n",
    "layer.set_weights([new_kernel, new_bias])\n",
    "\n",
    "# print a summary of the whole model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional.Conv2D at 0x7ff3bd40bdd8>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 416, 416, 3)       0         \n",
      "_________________________________________________________________\n",
      "mobilenet_1.00_224 (Model)   multiple                  3228864   \n",
      "=================================================================\n",
      "Total params: 3,228,864\n",
      "Trainable params: 3,206,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_image     = Input(shape=(input_size, input_size, 3))\n",
    "true_boxes = Input(shape=(1, 1, 1, max_box_per_image , 4))  \n",
    "\n",
    "feature_extractor = MobileNetFeature(input_size)\n",
    "\n",
    "feature_extractor.feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tfe = tf.contrib.eager\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.random.randn(1,5,5,2,5) # (batch, grid_x, grid_y, n_box, n_dims)\n",
    "y_true = np.where(y_pred>0.1,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant(y_true)\n",
    "y_pred = tf.constant(y_pred)\n",
    "\n",
    "\n",
    "mask_shape = tf.shape(y_true)[:4]\n",
    "\n",
    "cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(5), [5]), (1, 5, 5, 1, 1)))\n",
    "cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
    "\n",
    "cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [1, 1, 1, 2, 1])\n",
    "\n",
    "coord_mask = tf.zeros(mask_shape)\n",
    "conf_mask  = tf.zeros(mask_shape)\n",
    "class_mask = tf.zeros(mask_shape)\n",
    "\n",
    "seen = tfe.Variable(0.)\n",
    "total_recall = tfe.Variable(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=173, shape=(5, 5), dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  2.,  3.,  4.],\n",
       "       [ 0.,  1.,  2.,  3.,  4.],\n",
       "       [ 0.,  1.,  2.,  3.,  4.],\n",
       "       [ 0.,  1.,  2.,  3.,  4.],\n",
       "       [ 0.,  1.,  2.,  3.,  4.]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.concat([cell_x,cell_y], -1),(5,5,2))[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=164, shape=(5, 5), dtype=float32, numpy=\n",
       "array([[ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.],\n",
       "       [ 2.,  2.,  2.,  2.,  2.],\n",
       "       [ 3.,  3.,  3.,  3.,  3.],\n",
       "       [ 4.,  4.,  4.,  4.,  4.]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.concat([cell_x,cell_y], -1),(5,5,2))[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adjust prediction\n",
    "\"\"\"\n",
    "### adjust x and y      \n",
    "pred_box_xy = tf.cast(tf.sigmoid(y_pred[..., :2]),'float32') + cell_grid\n",
    "\n",
    "### adjust w and h\n",
    "pred_box_wh = tf.exp(y_pred[..., 2:4])# * np.reshape(anchors, [1,1,1,2,2])\n",
    "\n",
    "### adjust confidence\n",
    "pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
    "\n",
    "### adjust class probabilities\n",
    "pred_box_class = y_pred[..., 5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(1), Dimension(5), Dimension(5), Dimension(2), Dimension(0)]),\n",
       " TensorShape([Dimension(1), Dimension(5), Dimension(5), Dimension(2)]),\n",
       " TensorShape([Dimension(1), Dimension(5), Dimension(5), Dimension(2), Dimension(2)]),\n",
       " TensorShape([Dimension(1), Dimension(5), Dimension(5), Dimension(2), Dimension(2)]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_box_class.get_shape(), pred_box_conf.get_shape(), pred_box_wh.get_shape(), pred_box_xy.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adjust ground truth\n",
    "\"\"\"\n",
    "### adjust x and y\n",
    "true_box_xy = y_true[..., 0:2] # relative position to the containing cell\n",
    "\n",
    "### adjust w and h\n",
    "true_box_wh = y_true[..., 2:4] # number of cells accross, horizontally and vertically\n",
    "\n",
    "### adjust confidence\n",
    "true_wh_half = true_box_wh / 2.\n",
    "true_mins    = tf.cast(true_box_xy,'float64') - true_wh_half\n",
    "true_maxes   = tf.cast(true_box_xy,'float64') + true_wh_half\n",
    "\n",
    "pred_wh_half = pred_box_wh / 2.\n",
    "pred_mins    = tf.cast(pred_box_xy,'float64') - pred_wh_half\n",
    "pred_maxes   = tf.cast(pred_box_xy,'float64') + pred_wh_half       \n",
    "\n",
    "intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "\n",
    "true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
    "pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
    "\n",
    "union_areas = pred_areas + tf.cast(true_areas,'float64') - intersect_areas\n",
    "iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "\n",
    "true_box_conf = iou_scores * tf.cast(y_true[..., 4],'float64')\n",
    "\n",
    "### adjust class probabilities\n",
    "# true_box_class = tf.argmax(tf.cast(y_true[..., 5:],'float64'), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(1), Dimension(5), Dimension(5), Dimension(2)]),\n",
       " TensorShape([Dimension(1), Dimension(5), Dimension(5), Dimension(2)]),\n",
       " TensorShape([Dimension(1), Dimension(5), Dimension(5), Dimension(2)]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersect_areas.get_shape(), union_areas.get_shape(), iou_scores.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_loss(self, y_true, y_pred):\n",
    "    mask_shape = tf.shape(y_true)[:4]\n",
    "\n",
    "    cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(self.grid_w), [self.grid_h]), (1, self.grid_h, self.grid_w, 1, 1)))\n",
    "    cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
    "\n",
    "    cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [self.batch_size, 1, 1, self.nb_box, 1])\n",
    "\n",
    "    coord_mask = tf.zeros(mask_shape)\n",
    "    conf_mask  = tf.zeros(mask_shape)\n",
    "    class_mask = tf.zeros(mask_shape)\n",
    "\n",
    "    seen = tf.Variable(0.)\n",
    "    total_recall = tf.Variable(0.)\n",
    "\n",
    "    \"\"\"\n",
    "    Adjust prediction\n",
    "    \"\"\"\n",
    "    ### adjust x and y      \n",
    "    pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid\n",
    "\n",
    "    ### adjust w and h\n",
    "    pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(self.anchors, [1,1,1,self.nb_box,2])\n",
    "\n",
    "    ### adjust confidence\n",
    "    pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
    "\n",
    "    ### adjust class probabilities\n",
    "    pred_box_class = y_pred[..., 5:]\n",
    "\n",
    "    \"\"\"\n",
    "    Adjust ground truth\n",
    "    \"\"\"\n",
    "    ### adjust x and y\n",
    "    true_box_xy = y_true[..., 0:2] # relative position to the containing cell\n",
    "\n",
    "    ### adjust w and h\n",
    "    true_box_wh = y_true[..., 2:4] # number of cells accross, horizontally and vertically\n",
    "\n",
    "    ### adjust confidence\n",
    "    true_wh_half = true_box_wh / 2.\n",
    "    true_mins    = true_box_xy - true_wh_half\n",
    "    true_maxes   = true_box_xy + true_wh_half\n",
    "\n",
    "    pred_wh_half = pred_box_wh / 2.\n",
    "    pred_mins    = pred_box_xy - pred_wh_half\n",
    "    pred_maxes   = pred_box_xy + pred_wh_half       \n",
    "\n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "\n",
    "    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
    "    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "\n",
    "    true_box_conf = iou_scores * y_true[..., 4]\n",
    "\n",
    "    ### adjust class probabilities\n",
    "    true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
    "\n",
    "    \"\"\"\n",
    "    Determine the masks\n",
    "    \"\"\"\n",
    "    ### coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
    "    coord_mask = tf.expand_dims(y_true[..., 4], axis=-1) * self.coord_scale\n",
    "\n",
    "    ### confidence mask: penelize predictors + penalize boxes with low IOU\n",
    "    # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
    "    true_xy = self.true_boxes[..., 0:2]\n",
    "    true_wh = self.true_boxes[..., 2:4]\n",
    "\n",
    "    true_wh_half = true_wh / 2.\n",
    "    true_mins    = true_xy - true_wh_half\n",
    "    true_maxes   = true_xy + true_wh_half\n",
    "\n",
    "    pred_xy = tf.expand_dims(pred_box_xy, 4)\n",
    "    pred_wh = tf.expand_dims(pred_box_wh, 4)\n",
    "\n",
    "    pred_wh_half = pred_wh / 2.\n",
    "    pred_mins    = pred_xy - pred_wh_half\n",
    "    pred_maxes   = pred_xy + pred_wh_half    \n",
    "\n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "\n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "\n",
    "    best_ious = tf.reduce_max(iou_scores, axis=4)\n",
    "    conf_mask = conf_mask + tf.to_float(best_ious < 0.6) * (1 - y_true[..., 4]) * self.no_object_scale\n",
    "\n",
    "    # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
    "    conf_mask = conf_mask + y_true[..., 4] * self.object_scale\n",
    "\n",
    "    ### class mask: simply the position of the ground truth boxes (the predictors)\n",
    "    class_mask = y_true[..., 4] * tf.gather(self.class_wt, true_box_class) * self.class_scale       \n",
    "\n",
    "    \"\"\"\n",
    "    Warm-up training\n",
    "    \"\"\"\n",
    "    no_boxes_mask = tf.to_float(coord_mask < self.coord_scale/2.)\n",
    "    seen = tf.assign_add(seen, 1.)\n",
    "\n",
    "    true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, self.warmup_batches+1), \n",
    "                          lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask, \n",
    "                                   true_box_wh + tf.ones_like(true_box_wh) * \\\n",
    "                                   np.reshape(self.anchors, [1,1,1,self.nb_box,2]) * \\\n",
    "                                   no_boxes_mask, \n",
    "                                   tf.ones_like(coord_mask)],\n",
    "                          lambda: [true_box_xy, \n",
    "                                   true_box_wh,\n",
    "                                   coord_mask])\n",
    "\n",
    "    \"\"\"\n",
    "    Finalize the loss\n",
    "    \"\"\"\n",
    "    nb_coord_box = tf.reduce_sum(tf.to_float(coord_mask > 0.0))\n",
    "    nb_conf_box  = tf.reduce_sum(tf.to_float(conf_mask  > 0.0))\n",
    "    nb_class_box = tf.reduce_sum(tf.to_float(class_mask > 0.0))\n",
    "\n",
    "    loss_xy    = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_wh    = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_conf  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask)  / (nb_conf_box  + 1e-6) / 2.\n",
    "    loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
    "    loss_class = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)\n",
    "\n",
    "    loss = tf.cond(tf.less(seen, self.warmup_batches+1), \n",
    "                  lambda: loss_xy + loss_wh + loss_conf + loss_class + 10,\n",
    "                  lambda: loss_xy + loss_wh + loss_conf + loss_class)\n",
    "\n",
    "    if self.debug:\n",
    "        nb_true_box = tf.reduce_sum(y_true[..., 4])\n",
    "        nb_pred_box = tf.reduce_sum(tf.to_float(true_box_conf > 0.5) * tf.to_float(pred_box_conf > 0.3))\n",
    "\n",
    "        current_recall = nb_pred_box/(nb_true_box + 1e-6)\n",
    "        total_recall = tf.assign_add(total_recall, current_recall) \n",
    "\n",
    "        loss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
    "        loss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
    "        loss = tf.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
    "        loss = tf.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
    "        loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
    "        loss = tf.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
    "        loss = tf.Print(loss, [total_recall/seen], message='Average Recall \\t', summarize=1000)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.save('mobile-net-yolo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_extractor.get_output_shape())    \n",
    "grid_h, grid_w = feature_extractor.get_output_shape()        \n",
    "features = feature_extractor.extract(input_image)            \n",
    "\n",
    "# make the object detection layer\n",
    "output = Conv2D(nb_box * (4 + 1 + nb_class), \n",
    "                (1,1), strides=(1,1), \n",
    "                padding='same', \n",
    "                name='DetectionLayer', \n",
    "                kernel_initializer='lecun_normal')(features)\n",
    "output = Reshape((grid_h, grid_w, nb_box, 4 + 1 + nb_class))(output)\n",
    "output = Lambda(lambda args: args[0])([output, true_boxes])\n",
    "\n",
    "model = Model([input_image, true_boxes], output)\n",
    "\n",
    "\n",
    "# initialize the weights of the detection layer\n",
    "layer = model.layers[-4]\n",
    "weights = layer.get_weights()\n",
    "\n",
    "new_kernel = np.random.normal(size=weights[0].shape)/(grid_h*grid_w)\n",
    "new_bias   = np.random.normal(size=weights[1].shape)/(grid_h*grid_w)\n",
    "\n",
    "layer.set_weights([new_kernel, new_bias])\n",
    "\n",
    "# print a summary of the whole model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 5)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anchors)//2, 10//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO(object):\n",
    "    def __init__(self, backend,\n",
    "                       input_size, \n",
    "                       labels, \n",
    "                       max_box_per_image,\n",
    "                       anchors):\n",
    "\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.labels   = list(labels)\n",
    "        self.nb_class = len(self.labels)\n",
    "        self.nb_box   = len(anchors)//2\n",
    "        self.class_wt = np.ones(self.nb_class, dtype='float32')\n",
    "        self.anchors  = anchors\n",
    "\n",
    "        self.max_box_per_image = max_box_per_image\n",
    "\n",
    "        ##########################\n",
    "        # Make the model\n",
    "        ##########################\n",
    "\n",
    "        # make the feature extractor layers\n",
    "        input_image     = Input(shape=(self.input_size, self.input_size, 3))\n",
    "        self.true_boxes = Input(shape=(1, 1, 1, max_box_per_image , 4))  \n",
    "\n",
    "        if backend == 'Inception3':\n",
    "            self.feature_extractor = Inception3Feature(self.input_size)  \n",
    "        elif backend == 'SqueezeNet':\n",
    "            self.feature_extractor = SqueezeNetFeature(self.input_size)        \n",
    "        elif backend == 'MobileNet':\n",
    "            self.feature_extractor = MobileNetFeature(self.input_size)\n",
    "        elif backend == 'Full Yolo':\n",
    "            self.feature_extractor = FullYoloFeature(self.input_size)\n",
    "        elif backend == 'Tiny Yolo':\n",
    "            self.feature_extractor = TinyYoloFeature(self.input_size)\n",
    "        elif backend == 'VGG16':\n",
    "            self.feature_extractor = VGG16Feature(self.input_size)\n",
    "        elif backend == 'ResNet50':\n",
    "            self.feature_extractor = ResNet50Feature(self.input_size)\n",
    "        else:\n",
    "            raise Exception('Architecture not supported! Only support Full Yolo, Tiny Yolo, MobileNet, SqueezeNet, VGG16, ResNet50, and Inception3 at the moment!')\n",
    "\n",
    "        print(self.feature_extractor.get_output_shape())    \n",
    "        self.grid_h, self.grid_w = self.feature_extractor.get_output_shape()        \n",
    "        features = self.feature_extractor.extract(input_image)            \n",
    "\n",
    "        # make the object detection layer\n",
    "        output = Conv2D(self.nb_box * (4 + 1 + self.nb_class), \n",
    "                        (1,1), strides=(1,1), \n",
    "                        padding='same', \n",
    "                        name='DetectionLayer', \n",
    "                        kernel_initializer='lecun_normal')(features)\n",
    "        output = Reshape((self.grid_h, self.grid_w, self.nb_box, 4 + 1 + self.nb_class))(output)\n",
    "        output = Lambda(lambda args: args[0])([output, self.true_boxes])\n",
    "\n",
    "        self.model = Model([input_image, self.true_boxes], output)\n",
    "\n",
    "        \n",
    "        # initialize the weights of the detection layer\n",
    "        layer = self.model.layers[-4]\n",
    "        weights = layer.get_weights()\n",
    "\n",
    "        new_kernel = np.random.normal(size=weights[0].shape)/(self.grid_h*self.grid_w)\n",
    "        new_bias   = np.random.normal(size=weights[1].shape)/(self.grid_h*self.grid_w)\n",
    "\n",
    "        layer.set_weights([new_kernel, new_bias])\n",
    "\n",
    "        # print a summary of the whole model\n",
    "        self.model.summary()\n",
    "\n",
    "    def custom_loss(self, y_true, y_pred):\n",
    "        mask_shape = tf.shape(y_true)[:4]\n",
    "        \n",
    "        cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(self.grid_w), [self.grid_h]), (1, self.grid_h, self.grid_w, 1, 1)))\n",
    "        cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
    "\n",
    "        cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [self.batch_size, 1, 1, self.nb_box, 1])\n",
    "        \n",
    "        coord_mask = tf.zeros(mask_shape)\n",
    "        conf_mask  = tf.zeros(mask_shape)\n",
    "        class_mask = tf.zeros(mask_shape)\n",
    "        \n",
    "        seen = tf.Variable(0.)\n",
    "        total_recall = tf.Variable(0.)\n",
    "        \n",
    "        \"\"\"\n",
    "        Adjust prediction\n",
    "        \"\"\"\n",
    "        ### adjust x and y      \n",
    "        pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid\n",
    "        \n",
    "        ### adjust w and h\n",
    "        pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(self.anchors, [1,1,1,self.nb_box,2])\n",
    "        \n",
    "        ### adjust confidence\n",
    "        pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
    "        \n",
    "        ### adjust class probabilities\n",
    "        pred_box_class = y_pred[..., 5:]\n",
    "        \n",
    "        \"\"\"\n",
    "        Adjust ground truth\n",
    "        \"\"\"\n",
    "        ### adjust x and y\n",
    "        true_box_xy = y_true[..., 0:2] # relative position to the containing cell\n",
    "        \n",
    "        ### adjust w and h\n",
    "        true_box_wh = y_true[..., 2:4] # number of cells accross, horizontally and vertically\n",
    "        \n",
    "        ### adjust confidence\n",
    "        true_wh_half = true_box_wh / 2.\n",
    "        true_mins    = true_box_xy - true_wh_half\n",
    "        true_maxes   = true_box_xy + true_wh_half\n",
    "        \n",
    "        pred_wh_half = pred_box_wh / 2.\n",
    "        pred_mins    = pred_box_xy - pred_wh_half\n",
    "        pred_maxes   = pred_box_xy + pred_wh_half       \n",
    "        \n",
    "        intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "        intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "        intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "        intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "        \n",
    "        true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
    "        pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
    "\n",
    "        union_areas = pred_areas + true_areas - intersect_areas\n",
    "        iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "        \n",
    "        true_box_conf = iou_scores * y_true[..., 4]\n",
    "        \n",
    "        ### adjust class probabilities\n",
    "        true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
    "        \n",
    "        \"\"\"\n",
    "        Determine the masks\n",
    "        \"\"\"\n",
    "        ### coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
    "        coord_mask = tf.expand_dims(y_true[..., 4], axis=-1) * self.coord_scale\n",
    "        \n",
    "        ### confidence mask: penelize predictors + penalize boxes with low IOU\n",
    "        # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
    "        true_xy = self.true_boxes[..., 0:2]\n",
    "        true_wh = self.true_boxes[..., 2:4]\n",
    "        \n",
    "        true_wh_half = true_wh / 2.\n",
    "        true_mins    = true_xy - true_wh_half\n",
    "        true_maxes   = true_xy + true_wh_half\n",
    "        \n",
    "        pred_xy = tf.expand_dims(pred_box_xy, 4)\n",
    "        pred_wh = tf.expand_dims(pred_box_wh, 4)\n",
    "        \n",
    "        pred_wh_half = pred_wh / 2.\n",
    "        pred_mins    = pred_xy - pred_wh_half\n",
    "        pred_maxes   = pred_xy + pred_wh_half    \n",
    "        \n",
    "        intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "        intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "        intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "        intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "        \n",
    "        true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "        pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "\n",
    "        union_areas = pred_areas + true_areas - intersect_areas\n",
    "        iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "\n",
    "        best_ious = tf.reduce_max(iou_scores, axis=4)\n",
    "        conf_mask = conf_mask + tf.to_float(best_ious < 0.6) * (1 - y_true[..., 4]) * self.no_object_scale\n",
    "        \n",
    "        # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
    "        conf_mask = conf_mask + y_true[..., 4] * self.object_scale\n",
    "        \n",
    "        ### class mask: simply the position of the ground truth boxes (the predictors)\n",
    "        class_mask = y_true[..., 4] * tf.gather(self.class_wt, true_box_class) * self.class_scale       \n",
    "        \n",
    "        \"\"\"\n",
    "        Warm-up training\n",
    "        \"\"\"\n",
    "        no_boxes_mask = tf.to_float(coord_mask < self.coord_scale/2.)\n",
    "        seen = tf.assign_add(seen, 1.)\n",
    "        \n",
    "        true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, self.warmup_batches+1), \n",
    "                              lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask, \n",
    "                                       true_box_wh + tf.ones_like(true_box_wh) * \\\n",
    "                                       np.reshape(self.anchors, [1,1,1,self.nb_box,2]) * \\\n",
    "                                       no_boxes_mask, \n",
    "                                       tf.ones_like(coord_mask)],\n",
    "                              lambda: [true_box_xy, \n",
    "                                       true_box_wh,\n",
    "                                       coord_mask])\n",
    "        \n",
    "        \"\"\"\n",
    "        Finalize the loss\n",
    "        \"\"\"\n",
    "        nb_coord_box = tf.reduce_sum(tf.to_float(coord_mask > 0.0))\n",
    "        nb_conf_box  = tf.reduce_sum(tf.to_float(conf_mask  > 0.0))\n",
    "        nb_class_box = tf.reduce_sum(tf.to_float(class_mask > 0.0))\n",
    "        \n",
    "        loss_xy    = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "        loss_wh    = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "        loss_conf  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask)  / (nb_conf_box  + 1e-6) / 2.\n",
    "        loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
    "        loss_class = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)\n",
    "        \n",
    "        loss = tf.cond(tf.less(seen, self.warmup_batches+1), \n",
    "                      lambda: loss_xy + loss_wh + loss_conf + loss_class + 10,\n",
    "                      lambda: loss_xy + loss_wh + loss_conf + loss_class)\n",
    "        \n",
    "        if self.debug:\n",
    "            nb_true_box = tf.reduce_sum(y_true[..., 4])\n",
    "            nb_pred_box = tf.reduce_sum(tf.to_float(true_box_conf > 0.5) * tf.to_float(pred_box_conf > 0.3))\n",
    "            \n",
    "            current_recall = nb_pred_box/(nb_true_box + 1e-6)\n",
    "            total_recall = tf.assign_add(total_recall, current_recall) \n",
    "\n",
    "            loss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
    "            loss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
    "            loss = tf.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
    "            loss = tf.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
    "            loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
    "            loss = tf.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
    "            loss = tf.Print(loss, [total_recall/seen], message='Average Recall \\t', summarize=1000)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def load_weights(self, weight_path):\n",
    "        self.model.load_weights(weight_path)\n",
    "\n",
    "    def train(self, train_imgs,     # the list of images to train the model\n",
    "                    valid_imgs,     # the list of images used to validate the model\n",
    "                    train_times,    # the number of time to repeat the training set, often used for small datasets\n",
    "                    valid_times,    # the number of times to repeat the validation set, often used for small datasets\n",
    "                    nb_epochs,      # number of epoches\n",
    "                    learning_rate,  # the learning rate\n",
    "                    batch_size,     # the size of the batch\n",
    "                    warmup_epochs,  # number of initial batches to let the model familiarize with the new dataset\n",
    "                    object_scale,\n",
    "                    no_object_scale,\n",
    "                    coord_scale,\n",
    "                    class_scale,\n",
    "                    saved_weights_name='best_weights.h5',\n",
    "                    debug=False):     \n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.object_scale    = object_scale\n",
    "        self.no_object_scale = no_object_scale\n",
    "        self.coord_scale     = coord_scale\n",
    "        self.class_scale     = class_scale\n",
    "\n",
    "        self.debug = debug\n",
    "\n",
    "        ############################################\n",
    "        # Make train and validation generators\n",
    "        ############################################\n",
    "\n",
    "        generator_config = {\n",
    "            'IMAGE_H'         : self.input_size, \n",
    "            'IMAGE_W'         : self.input_size,\n",
    "            'GRID_H'          : self.grid_h,  \n",
    "            'GRID_W'          : self.grid_w,\n",
    "            'BOX'             : self.nb_box,\n",
    "            'LABELS'          : self.labels,\n",
    "            'CLASS'           : len(self.labels),\n",
    "            'ANCHORS'         : self.anchors,\n",
    "            'BATCH_SIZE'      : self.batch_size,\n",
    "            'TRUE_BOX_BUFFER' : self.max_box_per_image,\n",
    "        }    \n",
    "\n",
    "        train_generator = BatchGenerator(train_imgs, \n",
    "                                     generator_config, \n",
    "                                     norm=self.feature_extractor.normalize)\n",
    "        valid_generator = BatchGenerator(valid_imgs, \n",
    "                                     generator_config, \n",
    "                                     norm=self.feature_extractor.normalize,\n",
    "                                     jitter=False)   \n",
    "                                     \n",
    "        self.warmup_batches  = warmup_epochs * (train_times*len(train_generator) + valid_times*len(valid_generator))   \n",
    "\n",
    "        ############################################\n",
    "        # Compile the model\n",
    "        ############################################\n",
    "\n",
    "        optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "        self.model.compile(loss=self.custom_loss, optimizer=optimizer)\n",
    "\n",
    "        ############################################\n",
    "        # Make a few callbacks\n",
    "        ############################################\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           min_delta=0.001, \n",
    "                           patience=3, \n",
    "                           mode='min', \n",
    "                           verbose=1)\n",
    "        checkpoint = ModelCheckpoint(saved_weights_name, \n",
    "                                     monitor='val_loss', \n",
    "                                     verbose=1, \n",
    "                                     save_best_only=True, \n",
    "                                     mode='min', \n",
    "                                     period=1)\n",
    "        tensorboard = TensorBoard(log_dir=os.path.expanduser('~/logs/'), \n",
    "                                  histogram_freq=0, \n",
    "                                  #write_batch_performance=True,\n",
    "                                  write_graph=True, \n",
    "                                  write_images=False)\n",
    "\n",
    "        ############################################\n",
    "        # Start the training process\n",
    "        ############################################        \n",
    "\n",
    "        self.model.fit_generator(generator        = train_generator, \n",
    "                                 steps_per_epoch  = len(train_generator) * train_times, \n",
    "                                 epochs           = warmup_epochs + nb_epochs, \n",
    "                                 verbose          = 2 if debug else 1,\n",
    "                                 validation_data  = valid_generator,\n",
    "                                 validation_steps = len(valid_generator) * valid_times,\n",
    "                                 callbacks        = [early_stop, checkpoint, tensorboard], \n",
    "                                 workers          = 3,\n",
    "                                 max_queue_size   = 8)      \n",
    "\n",
    "        ############################################\n",
    "        # Compute mAP on the validation set\n",
    "        ############################################\n",
    "        average_precisions = self.evaluate(valid_generator)     \n",
    "\n",
    "        # print evaluation\n",
    "        for label, average_precision in average_precisions.items():\n",
    "            print(self.labels[label], '{:.4f}'.format(average_precision))\n",
    "        print('mAP: {:.4f}'.format(sum(average_precisions.values()) / len(average_precisions)))         \n",
    "\n",
    "    def evaluate(self, \n",
    "                 generator, \n",
    "                 iou_threshold=0.3,\n",
    "                 score_threshold=0.3,\n",
    "                 max_detections=100,\n",
    "                 save_path=None):\n",
    "        \"\"\" Evaluate a given dataset using a given model.\n",
    "        code originally from https://github.com/fizyr/keras-retinanet\n",
    "\n",
    "        # Arguments\n",
    "            generator       : The generator that represents the dataset to evaluate.\n",
    "            model           : The model to evaluate.\n",
    "            iou_threshold   : The threshold used to consider when a detection is positive or negative.\n",
    "            score_threshold : The score confidence threshold to use for detections.\n",
    "            max_detections  : The maximum number of detections to use per image.\n",
    "            save_path       : The path to save images with visualized detections to.\n",
    "        # Returns\n",
    "            A dict mapping class names to mAP scores.\n",
    "        \"\"\"    \n",
    "        # gather all detections and annotations\n",
    "        all_detections     = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
    "        all_annotations    = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
    "\n",
    "        for i in range(generator.size()):\n",
    "            raw_image = generator.load_image(i)\n",
    "            raw_height, raw_width, raw_channels = raw_image.shape\n",
    "\n",
    "            # make the boxes and the labels\n",
    "            pred_boxes  = self.predict(raw_image)\n",
    "\n",
    "            \n",
    "            score = np.array([box.score for box in pred_boxes])\n",
    "            pred_labels = np.array([box.label for box in pred_boxes])        \n",
    "            \n",
    "            if len(pred_boxes) > 0:\n",
    "                pred_boxes = np.array([[box.xmin*raw_width, box.ymin*raw_height, box.xmax*raw_width, box.ymax*raw_height, box.score] for box in pred_boxes])\n",
    "            else:\n",
    "                pred_boxes = np.array([[]])  \n",
    "            \n",
    "            # sort the boxes and the labels according to scores\n",
    "            score_sort = np.argsort(-score)\n",
    "            pred_labels = pred_labels[score_sort]\n",
    "            pred_boxes  = pred_boxes[score_sort]\n",
    "            \n",
    "            # copy detections to all_detections\n",
    "            for label in range(generator.num_classes()):\n",
    "                all_detections[i][label] = pred_boxes[pred_labels == label, :]\n",
    "                \n",
    "            annotations = generator.load_annotation(i)\n",
    "            \n",
    "            # copy detections to all_annotations\n",
    "            for label in range(generator.num_classes()):\n",
    "                all_annotations[i][label] = annotations[annotations[:, 4] == label, :4].copy()\n",
    "                \n",
    "        # compute mAP by comparing all detections and all annotations\n",
    "        average_precisions = {}\n",
    "        \n",
    "        for label in range(generator.num_classes()):\n",
    "            false_positives = np.zeros((0,))\n",
    "            true_positives  = np.zeros((0,))\n",
    "            scores          = np.zeros((0,))\n",
    "            num_annotations = 0.0\n",
    "\n",
    "            for i in range(generator.size()):\n",
    "                detections           = all_detections[i][label]\n",
    "                annotations          = all_annotations[i][label]\n",
    "                num_annotations     += annotations.shape[0]\n",
    "                detected_annotations = []\n",
    "\n",
    "                for d in detections:\n",
    "                    scores = np.append(scores, d[4])\n",
    "\n",
    "                    if annotations.shape[0] == 0:\n",
    "                        false_positives = np.append(false_positives, 1)\n",
    "                        true_positives  = np.append(true_positives, 0)\n",
    "                        continue\n",
    "\n",
    "                    overlaps            = compute_overlap(np.expand_dims(d, axis=0), annotations)\n",
    "                    assigned_annotation = np.argmax(overlaps, axis=1)\n",
    "                    max_overlap         = overlaps[0, assigned_annotation]\n",
    "\n",
    "                    if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n",
    "                        false_positives = np.append(false_positives, 0)\n",
    "                        true_positives  = np.append(true_positives, 1)\n",
    "                        detected_annotations.append(assigned_annotation)\n",
    "                    else:\n",
    "                        false_positives = np.append(false_positives, 1)\n",
    "                        true_positives  = np.append(true_positives, 0)\n",
    "\n",
    "            # no annotations -> AP for this class is 0 (is this correct?)\n",
    "            if num_annotations == 0:\n",
    "                average_precisions[label] = 0\n",
    "                continue\n",
    "\n",
    "            # sort by score\n",
    "            indices         = np.argsort(-scores)\n",
    "            false_positives = false_positives[indices]\n",
    "            true_positives  = true_positives[indices]\n",
    "\n",
    "            # compute false positives and true positives\n",
    "            false_positives = np.cumsum(false_positives)\n",
    "            true_positives  = np.cumsum(true_positives)\n",
    "\n",
    "            # compute recall and precision\n",
    "            recall    = true_positives / num_annotations\n",
    "            precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "\n",
    "            # compute average precision\n",
    "            average_precision  = compute_ap(recall, precision)  \n",
    "            average_precisions[label] = average_precision\n",
    "\n",
    "        return average_precisions    \n",
    "\n",
    "    def predict(self, image):\n",
    "        image_h, image_w, _ = image.shape\n",
    "        image = cv2.resize(image, (self.input_size, self.input_size))\n",
    "        image = self.feature_extractor.normalize(image)\n",
    "\n",
    "        input_image = image[:,:,::-1]\n",
    "        input_image = np.expand_dims(input_image, 0)\n",
    "        dummy_array = np.zeros((1,1,1,1,self.max_box_per_image,4))\n",
    "\n",
    "        netout = self.model.predict([input_image, dummy_array])[0]\n",
    "        boxes  = decode_netout(netout, self.anchors, self.nb_class)\n",
    "\n",
    "        return boxes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
