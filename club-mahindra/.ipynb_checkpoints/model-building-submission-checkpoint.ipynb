{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, gc\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "sns.set(context='notebook', style='whitegrid', palette='deep', font='sans-serif', \n",
    "        font_scale=2, color_codes=True, rc=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(488189, 62)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test = pd.read_csv('data/train_test.csv')\n",
    "train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(341424, 61) (146765, 61) (341424,)\n",
      "Shape:  (341424, 61)  ==>  (341424, 58)\n"
     ]
    }
   ],
   "source": [
    "train_len = 341424\n",
    "\n",
    "train_df = train_test.iloc[:train_len]\n",
    "test_df  = train_test.iloc[train_len:]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "target = train_df['amount_spent_per_room_night_scaled']\n",
    "\n",
    "train_df.drop('amount_spent_per_room_night_scaled', axis=1, inplace=True)\n",
    "test_df.drop('amount_spent_per_room_night_scaled', axis=1, inplace=True)\n",
    "\n",
    "print(train_df.shape, test_df.shape, target.shape)\n",
    "\n",
    "print(\"Shape: \", train_df.shape, \" ==> \", end=\" \")\n",
    "\n",
    "train_df.drop(['booking_date','checkin_date','checkout_date'], axis=1, inplace=True)\n",
    "test_df.drop(['booking_date','checkin_date','checkout_date'], axis=1, inplace=True)\n",
    "\n",
    "print(train_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101327, 43496, 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_df.memberid)), len(set(test_df.memberid)), len(set(train_df.memberid).intersection(set(test_df.memberid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_df.columns:\n",
    "    train_df[col] = train_df[col].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupKFold(n_splits=4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((256068, 58), (85356, 58), (256068,), (85356,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "\n",
    "group_kfold = GroupKFold(n_splits=4)\n",
    "\n",
    "print(group_kfold)\n",
    "\n",
    "for train_index, test_index in group_kfold.split(train_df.drop('memberid', axis=1), target, train_df['memberid']):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_, valid_ = train_df.iloc[train_index], train_df.iloc[test_index]\n",
    "    y_tr, y_val    = target[train_index], target[test_index]\n",
    "    break\n",
    "\n",
    "train_.shape, valid_.shape, y_tr.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_.drop('memberid', axis=1, inplace=True)\n",
    "valid_.drop('memberid', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_df, test_df, train_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def bayesian_opt_lgb(X_train, y_train, X_valid, y_valid, features):\n",
    "        \n",
    "    def train_lgb_model(f_frac, b_frac, \n",
    "                        l1, l2, split_gain,\n",
    "                        leaves, data_in_leaf, hessian):\n",
    "    \n",
    "        param = {}\n",
    "\n",
    "        param['feature_fraction'] = max(min(f_frac, 1), 0)\n",
    "        param['bagging_fraction'] = max(min(b_frac, 1), 0)\n",
    "\n",
    "        param['lambda_l1'] = max(l1, 0)\n",
    "        param['lambda_l2'] = max(l2, 0)\n",
    "        param['min_split_gain'] = split_gain\n",
    "#     #     params['min_child_weight'] = min_child_weight\n",
    "\n",
    "        param['num_leaves'] = int(leaves)\n",
    "        param['min_data_in_leaf'] = int(data_in_leaf)\n",
    "        param['min_sum_hessian_in_leaf'] = max(hessian, 0)\n",
    "\n",
    "        param_const = {\n",
    "            'max_bins'               : 63,\n",
    "            'learning_rate'          : 0.01,\n",
    "            'num_threads'            : 4,\n",
    "            'metric'                 : 'rmse',\n",
    "            'boost'                  : 'gbdt',\n",
    "            'tree_learner'           : 'serial',\n",
    "            'objective'              : 'root_mean_squared_error',\n",
    "            'verbosity'              : 0,\n",
    "        }\n",
    "\n",
    "        for key, item in param_const.items():\n",
    "            param[key] = item\n",
    "    \n",
    "#         print(param)\n",
    "\n",
    "        _train = lgb.Dataset(X_train[features], label=y_train, feature_name=list(features))\n",
    "        _valid = lgb.Dataset(X_valid[features], label=y_valid,feature_name=list(features))\n",
    "\n",
    "        clf = lgb.train(param, _train, 10000, \n",
    "                        valid_sets = [_train, _valid], \n",
    "                        verbose_eval=0, \n",
    "                        early_stopping_rounds = 25)                  \n",
    "\n",
    "        oof = clf.predict(X_valid[features], num_iteration=clf.best_iteration)\n",
    "        score = mean_squared_error(y_valid, oof)\n",
    "\n",
    "        return -score\n",
    "\n",
    "\n",
    "    _bo = BayesianOptimization(train_lgb_model, {\n",
    "        # speed\n",
    "#         'bagging_freq'           : 5, #int\n",
    "        'b_frac'       : (0.2,0.7),\n",
    "        'f_frac'       : (0.2,0.8),\n",
    "\n",
    "#         # accuracy\n",
    "# #         'max_bins'               : 127,\n",
    "#         'learning_rate'          : 0.01,\n",
    "        'leaves'             : (20,90), # int\n",
    "    \n",
    "        # regularization\n",
    "        'split_gain'      : (0, 10),\n",
    "        'l1'              : (0, 5),\n",
    "        'l2'              : (0, 5),\n",
    "        \n",
    "#         # deal with overfitting\n",
    "        'data_in_leaf'       : (20, 500), # int\n",
    "        'hessian': (0, 100),\n",
    "\n",
    "\n",
    "    }, random_state=23456)\n",
    "    _bo.maximize(init_points=25, n_iter=12, acq='ei')\n",
    "    \n",
    "    return _bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  b_frac   | data_i... |  f_frac   |  hessian  |    l1     |    l2     |  leaves   | split_... |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.9448  \u001b[0m | \u001b[0m 0.3609  \u001b[0m | \u001b[0m 177.1   \u001b[0m | \u001b[0m 0.7565  \u001b[0m | \u001b[0m 31.17   \u001b[0m | \u001b[0m 0.8098  \u001b[0m | \u001b[0m 1.82    \u001b[0m | \u001b[0m 57.05   \u001b[0m | \u001b[0m 7.891   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.9448  \u001b[0m | \u001b[0m 0.6377  \u001b[0m | \u001b[0m 325.6   \u001b[0m | \u001b[0m 0.7938  \u001b[0m | \u001b[0m 81.59   \u001b[0m | \u001b[0m 0.2066  \u001b[0m | \u001b[0m 1.015   \u001b[0m | \u001b[0m 30.67   \u001b[0m | \u001b[0m 7.531   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-0.9437  \u001b[0m | \u001b[95m 0.4276  \u001b[0m | \u001b[95m 478.2   \u001b[0m | \u001b[95m 0.7624  \u001b[0m | \u001b[95m 13.54   \u001b[0m | \u001b[95m 1.18    \u001b[0m | \u001b[95m 1.574   \u001b[0m | \u001b[95m 67.09   \u001b[0m | \u001b[95m 6.206   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.9471  \u001b[0m | \u001b[0m 0.5111  \u001b[0m | \u001b[0m 422.2   \u001b[0m | \u001b[0m 0.5785  \u001b[0m | \u001b[0m 56.09   \u001b[0m | \u001b[0m 4.934   \u001b[0m | \u001b[0m 3.416   \u001b[0m | \u001b[0m 27.09   \u001b[0m | \u001b[0m 9.401   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.947   \u001b[0m | \u001b[0m 0.5257  \u001b[0m | \u001b[0m 129.2   \u001b[0m | \u001b[0m 0.4591  \u001b[0m | \u001b[0m 39.01   \u001b[0m | \u001b[0m 3.779   \u001b[0m | \u001b[0m 0.7132  \u001b[0m | \u001b[0m 20.69   \u001b[0m | \u001b[0m 8.379   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.9447  \u001b[0m | \u001b[0m 0.2544  \u001b[0m | \u001b[0m 139.6   \u001b[0m | \u001b[0m 0.4855  \u001b[0m | \u001b[0m 88.39   \u001b[0m | \u001b[0m 0.8874  \u001b[0m | \u001b[0m 3.93    \u001b[0m | \u001b[0m 22.45   \u001b[0m | \u001b[0m 6.086   \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m-0.9392  \u001b[0m | \u001b[95m 0.3542  \u001b[0m | \u001b[95m 389.5   \u001b[0m | \u001b[95m 0.558   \u001b[0m | \u001b[95m 9.443   \u001b[0m | \u001b[95m 2.217   \u001b[0m | \u001b[95m 4.023   \u001b[0m | \u001b[95m 66.26   \u001b[0m | \u001b[95m 2.169   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.941   \u001b[0m | \u001b[0m 0.6566  \u001b[0m | \u001b[0m 303.2   \u001b[0m | \u001b[0m 0.7013  \u001b[0m | \u001b[0m 59.13   \u001b[0m | \u001b[0m 1.415   \u001b[0m | \u001b[0m 0.2832  \u001b[0m | \u001b[0m 40.42   \u001b[0m | \u001b[0m 3.318   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.9456  \u001b[0m | \u001b[0m 0.6414  \u001b[0m | \u001b[0m 416.1   \u001b[0m | \u001b[0m 0.6168  \u001b[0m | \u001b[0m 58.78   \u001b[0m | \u001b[0m 1.218   \u001b[0m | \u001b[0m 2.475   \u001b[0m | \u001b[0m 77.52   \u001b[0m | \u001b[0m 9.213   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.9452  \u001b[0m | \u001b[0m 0.4674  \u001b[0m | \u001b[0m 290.2   \u001b[0m | \u001b[0m 0.2127  \u001b[0m | \u001b[0m 31.66   \u001b[0m | \u001b[0m 0.03533 \u001b[0m | \u001b[0m 2.889   \u001b[0m | \u001b[0m 60.49   \u001b[0m | \u001b[0m 6.575   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.9409  \u001b[0m | \u001b[0m 0.2728  \u001b[0m | \u001b[0m 259.5   \u001b[0m | \u001b[0m 0.4937  \u001b[0m | \u001b[0m 12.4    \u001b[0m | \u001b[0m 2.283   \u001b[0m | \u001b[0m 0.2311  \u001b[0m | \u001b[0m 65.28   \u001b[0m | \u001b[0m 3.88    \u001b[0m |\n",
      "| \u001b[95m 12      \u001b[0m | \u001b[95m-0.9389  \u001b[0m | \u001b[95m 0.4834  \u001b[0m | \u001b[95m 201.5   \u001b[0m | \u001b[95m 0.702   \u001b[0m | \u001b[95m 56.52   \u001b[0m | \u001b[95m 4.932   \u001b[0m | \u001b[95m 3.971   \u001b[0m | \u001b[95m 54.07   \u001b[0m | \u001b[95m 1.856   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.9409  \u001b[0m | \u001b[0m 0.6426  \u001b[0m | \u001b[0m 84.42   \u001b[0m | \u001b[0m 0.7285  \u001b[0m | \u001b[0m 94.59   \u001b[0m | \u001b[0m 1.265   \u001b[0m | \u001b[0m 4.616   \u001b[0m | \u001b[0m 73.62   \u001b[0m | \u001b[0m 3.702   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.9439  \u001b[0m | \u001b[0m 0.6423  \u001b[0m | \u001b[0m 401.4   \u001b[0m | \u001b[0m 0.3068  \u001b[0m | \u001b[0m 78.15   \u001b[0m | \u001b[0m 3.354   \u001b[0m | \u001b[0m 0.8281  \u001b[0m | \u001b[0m 55.67   \u001b[0m | \u001b[0m 5.9     \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.946   \u001b[0m | \u001b[0m 0.6373  \u001b[0m | \u001b[0m 375.5   \u001b[0m | \u001b[0m 0.6142  \u001b[0m | \u001b[0m 32.58   \u001b[0m | \u001b[0m 2.428   \u001b[0m | \u001b[0m 2.413   \u001b[0m | \u001b[0m 88.99   \u001b[0m | \u001b[0m 9.712   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.9462  \u001b[0m | \u001b[0m 0.5821  \u001b[0m | \u001b[0m 107.9   \u001b[0m | \u001b[0m 0.304   \u001b[0m | \u001b[0m 4.032   \u001b[0m | \u001b[0m 3.471   \u001b[0m | \u001b[0m 0.5296  \u001b[0m | \u001b[0m 56.04   \u001b[0m | \u001b[0m 8.852   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.9467  \u001b[0m | \u001b[0m 0.6801  \u001b[0m | \u001b[0m 442.5   \u001b[0m | \u001b[0m 0.5876  \u001b[0m | \u001b[0m 72.75   \u001b[0m | \u001b[0m 4.384   \u001b[0m | \u001b[0m 1.846   \u001b[0m | \u001b[0m 37.53   \u001b[0m | \u001b[0m 9.611   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.9436  \u001b[0m | \u001b[0m 0.4092  \u001b[0m | \u001b[0m 334.8   \u001b[0m | \u001b[0m 0.326   \u001b[0m | \u001b[0m 34.59   \u001b[0m | \u001b[0m 0.7599  \u001b[0m | \u001b[0m 0.7827  \u001b[0m | \u001b[0m 74.48   \u001b[0m | \u001b[0m 6.527   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.9438  \u001b[0m | \u001b[0m 0.4788  \u001b[0m | \u001b[0m 221.0   \u001b[0m | \u001b[0m 0.2436  \u001b[0m | \u001b[0m 35.97   \u001b[0m | \u001b[0m 0.7153  \u001b[0m | \u001b[0m 4.802   \u001b[0m | \u001b[0m 47.4    \u001b[0m | \u001b[0m 5.367   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.9408  \u001b[0m | \u001b[0m 0.6123  \u001b[0m | \u001b[0m 188.4   \u001b[0m | \u001b[0m 0.5071  \u001b[0m | \u001b[0m 28.26   \u001b[0m | \u001b[0m 4.161   \u001b[0m | \u001b[0m 4.136   \u001b[0m | \u001b[0m 37.87   \u001b[0m | \u001b[0m 3.052   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.9457  \u001b[0m | \u001b[0m 0.4495  \u001b[0m | \u001b[0m 429.2   \u001b[0m | \u001b[0m 0.4899  \u001b[0m | \u001b[0m 54.85   \u001b[0m | \u001b[0m 4.81    \u001b[0m | \u001b[0m 3.204   \u001b[0m | \u001b[0m 23.96   \u001b[0m | \u001b[0m 7.019   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.9433  \u001b[0m | \u001b[0m 0.6741  \u001b[0m | \u001b[0m 300.9   \u001b[0m | \u001b[0m 0.7675  \u001b[0m | \u001b[0m 8.746   \u001b[0m | \u001b[0m 4.636   \u001b[0m | \u001b[0m 2.953   \u001b[0m | \u001b[0m 37.32   \u001b[0m | \u001b[0m 5.491   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.9464  \u001b[0m | \u001b[0m 0.607   \u001b[0m | \u001b[0m 219.1   \u001b[0m | \u001b[0m 0.7156  \u001b[0m | \u001b[0m 44.78   \u001b[0m | \u001b[0m 0.1332  \u001b[0m | \u001b[0m 2.02    \u001b[0m | \u001b[0m 30.85   \u001b[0m | \u001b[0m 9.811   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.9439  \u001b[0m | \u001b[0m 0.4758  \u001b[0m | \u001b[0m 110.2   \u001b[0m | \u001b[0m 0.2146  \u001b[0m | \u001b[0m 67.55   \u001b[0m | \u001b[0m 4.2     \u001b[0m | \u001b[0m 1.713   \u001b[0m | \u001b[0m 81.41   \u001b[0m | \u001b[0m 5.533   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.9469  \u001b[0m | \u001b[0m 0.4988  \u001b[0m | \u001b[0m 373.3   \u001b[0m | \u001b[0m 0.2627  \u001b[0m | \u001b[0m 96.45   \u001b[0m | \u001b[0m 3.22    \u001b[0m | \u001b[0m 4.234   \u001b[0m | \u001b[0m 72.99   \u001b[0m | \u001b[0m 9.081   \u001b[0m |\n",
      "| \u001b[95m 26      \u001b[0m | \u001b[95m-0.9377  \u001b[0m | \u001b[95m 0.4679  \u001b[0m | \u001b[95m 342.9   \u001b[0m | \u001b[95m 0.7846  \u001b[0m | \u001b[95m 4.181   \u001b[0m | \u001b[95m 2.173   \u001b[0m | \u001b[95m 1.602   \u001b[0m | \u001b[95m 86.47   \u001b[0m | \u001b[95m 0.02274 \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.938   \u001b[0m | \u001b[0m 0.3108  \u001b[0m | \u001b[0m 208.6   \u001b[0m | \u001b[0m 0.7231  \u001b[0m | \u001b[0m 92.84   \u001b[0m | \u001b[0m 4.443   \u001b[0m | \u001b[0m 0.05969 \u001b[0m | \u001b[0m 86.38   \u001b[0m | \u001b[0m 0.007699\u001b[0m |\n",
      "| \u001b[95m 28      \u001b[0m | \u001b[95m-0.937   \u001b[0m | \u001b[95m 0.2914  \u001b[0m | \u001b[95m 331.0   \u001b[0m | \u001b[95m 0.6414  \u001b[0m | \u001b[95m 2.579   \u001b[0m | \u001b[95m 3.538   \u001b[0m | \u001b[95m 4.831   \u001b[0m | \u001b[95m 87.05   \u001b[0m | \u001b[95m 0.04384 \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.9383  \u001b[0m | \u001b[0m 0.4651  \u001b[0m | \u001b[0m 21.14   \u001b[0m | \u001b[0m 0.5977  \u001b[0m | \u001b[0m 9.749   \u001b[0m | \u001b[0m 0.1685  \u001b[0m | \u001b[0m 1.142   \u001b[0m | \u001b[0m 89.23   \u001b[0m | \u001b[0m 0.1293  \u001b[0m |\n",
      "| \u001b[95m 30      \u001b[0m | \u001b[95m-0.9363  \u001b[0m | \u001b[95m 0.265   \u001b[0m | \u001b[95m 495.4   \u001b[0m | \u001b[95m 0.5353  \u001b[0m | \u001b[95m 4.985   \u001b[0m | \u001b[95m 3.543   \u001b[0m | \u001b[95m 3.802   \u001b[0m | \u001b[95m 87.88   \u001b[0m | \u001b[95m 0.05235 \u001b[0m |\n",
      "| \u001b[95m 31      \u001b[0m | \u001b[95m-0.9362  \u001b[0m | \u001b[95m 0.3678  \u001b[0m | \u001b[95m 495.4   \u001b[0m | \u001b[95m 0.5422  \u001b[0m | \u001b[95m 5.039   \u001b[0m | \u001b[95m 1.064   \u001b[0m | \u001b[95m 2.565   \u001b[0m | \u001b[95m 89.63   \u001b[0m | \u001b[95m 0.1654  \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.9367  \u001b[0m | \u001b[0m 0.6061  \u001b[0m | \u001b[0m 497.8   \u001b[0m | \u001b[0m 0.2382  \u001b[0m | \u001b[0m 87.92   \u001b[0m | \u001b[0m 0.04201 \u001b[0m | \u001b[0m 0.1018  \u001b[0m | \u001b[0m 81.56   \u001b[0m | \u001b[0m 0.08633 \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.9365  \u001b[0m | \u001b[0m 0.6852  \u001b[0m | \u001b[0m 496.5   \u001b[0m | \u001b[0m 0.2887  \u001b[0m | \u001b[0m 99.8    \u001b[0m | \u001b[0m 0.263   \u001b[0m | \u001b[0m 4.784   \u001b[0m | \u001b[0m 86.44   \u001b[0m | \u001b[0m 0.2472  \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.9372  \u001b[0m | \u001b[0m 0.2935  \u001b[0m | \u001b[0m 490.3   \u001b[0m | \u001b[0m 0.7544  \u001b[0m | \u001b[0m 45.17   \u001b[0m | \u001b[0m 1.688   \u001b[0m | \u001b[0m 2.056   \u001b[0m | \u001b[0m 89.48   \u001b[0m | \u001b[0m 0.06235 \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.9373  \u001b[0m | \u001b[0m 0.3762  \u001b[0m | \u001b[0m 487.2   \u001b[0m | \u001b[0m 0.7702  \u001b[0m | \u001b[0m 6.928   \u001b[0m | \u001b[0m 0.2973  \u001b[0m | \u001b[0m 0.8589  \u001b[0m | \u001b[0m 89.36   \u001b[0m | \u001b[0m 0.07048 \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.9365  \u001b[0m | \u001b[0m 0.2601  \u001b[0m | \u001b[0m 492.0   \u001b[0m | \u001b[0m 0.5311  \u001b[0m | \u001b[0m 96.29   \u001b[0m | \u001b[0m 1.109   \u001b[0m | \u001b[0m 4.223   \u001b[0m | \u001b[0m 85.61   \u001b[0m | \u001b[0m 0.02338 \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.9364  \u001b[0m | \u001b[0m 0.6813  \u001b[0m | \u001b[0m 485.0   \u001b[0m | \u001b[0m 0.4583  \u001b[0m | \u001b[0m 91.66   \u001b[0m | \u001b[0m 4.287   \u001b[0m | \u001b[0m 4.279   \u001b[0m | \u001b[0m 86.16   \u001b[0m | \u001b[0m 0.0152  \u001b[0m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "bo_tuning_lgb = bayesian_opt_lgb(train_, y_tr, valid_, y_val, list(train_.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def bayesian_opt_cat(X_train, y_train, X_valid, y_valid, features):\n",
    "        \n",
    "    def train_cat_model(r_str, b_temp, l2, depth):\n",
    "    \n",
    "        params = {}\n",
    "\n",
    "        params['random_strength']     = max(min(r_str, 1), 0)\n",
    "        params['bagging_temperature'] = max(b_temp, 0)\n",
    "        \n",
    "        params['l2_leaf_reg'] = max(l2, 0)\n",
    "#         params['min_data_in_leaf'] = max(data_leaf,1)\n",
    "        \n",
    "#         params['subsample'] = subsample\n",
    "        params['depth']     = int(depth)\n",
    "\n",
    "        param_const = {\n",
    "            'border_count'          : 63,\n",
    "            'early_stopping_rounds' : 50,\n",
    "            'random_seed'           : 1337,\n",
    "            'task_type'             : 'CPU', \n",
    "            'loss_function'         : \"RMSE\", \n",
    "    #         'subsample'             = 0.7, \n",
    "            'iterations'            : 10000, \n",
    "            'learning_rate'         : 0.01,\n",
    "            'thread_count'          : 4,\n",
    "#             'bootstrap_type'        : 'No'\n",
    "        }\n",
    "\n",
    "        for key, item in param_const.items():\n",
    "            params[key] = item\n",
    "    \n",
    "        \n",
    "\n",
    "        _train = Pool(X_train[features], label=y_train)#, cat_features=cate_features_index)\n",
    "        _valid = Pool(X_valid[features], label=y_valid)#, cat_features=cate_features_index)\n",
    "\n",
    "        watchlist = [_train, _valid]\n",
    "        clf = CatBoostRegressor(**params)\n",
    "        clf.fit(_train, \n",
    "                eval_set=watchlist, \n",
    "                verbose=0,\n",
    "                use_best_model=True)\n",
    "\n",
    "        oof  = clf.predict(X_valid[features])\n",
    "\n",
    "        score = mean_squared_error(y_valid, oof)\n",
    "\n",
    "        return -score\n",
    "\n",
    "    _bo = BayesianOptimization(train_cat_model, {\n",
    "\n",
    "        'r_str'      : (1, 5),\n",
    "        'b_temp'     : (0.01, 100),\n",
    "        'depth'      : (3,8), # int\n",
    "#         'subsample'  : (0.3, 0.8),\n",
    "#         'data_leaf'  : (2,7),\n",
    "        'l2'         : (0, 5),\n",
    "\n",
    "    }, random_state=23456)\n",
    "    _bo.maximize(init_points=25, n_iter=12, acq='ei')\n",
    "    \n",
    "    return _bo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  b_temp   |   depth   |    l2     |   r_str   |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.9559  \u001b[0m | \u001b[0m 32.19   \u001b[0m | \u001b[0m 4.637   \u001b[0m | \u001b[0m 4.637   \u001b[0m | \u001b[0m 2.247   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.9538  \u001b[0m | \u001b[95m 16.2    \u001b[0m | \u001b[95m 4.82    \u001b[0m | \u001b[95m 2.646   \u001b[0m | \u001b[95m 4.156   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-1.183   \u001b[0m | \u001b[0m 87.54   \u001b[0m | \u001b[0m 6.183   \u001b[0m | \u001b[0m 4.949   \u001b[0m | \u001b[0m 4.264   \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-0.9494  \u001b[0m | \u001b[95m 4.141   \u001b[0m | \u001b[95m 4.015   \u001b[0m | \u001b[95m 0.762   \u001b[0m | \u001b[95m 4.012   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-1.183   \u001b[0m | \u001b[0m 45.52   \u001b[0m | \u001b[0m 7.773   \u001b[0m | \u001b[0m 4.687   \u001b[0m | \u001b[0m 1.542   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.9549  \u001b[0m | \u001b[0m 23.6    \u001b[0m | \u001b[0m 4.574   \u001b[0m | \u001b[0m 3.363   \u001b[0m | \u001b[0m 3.482   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-1.183   \u001b[0m | \u001b[0m 62.22   \u001b[0m | \u001b[0m 7.189   \u001b[0m | \u001b[0m 3.154   \u001b[0m | \u001b[0m 3.244   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-1.183   \u001b[0m | \u001b[0m 98.69   \u001b[0m | \u001b[0m 6.416   \u001b[0m | \u001b[0m 0.5066  \u001b[0m | \u001b[0m 4.76    \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-1.183   \u001b[0m | \u001b[0m 65.14   \u001b[0m | \u001b[0m 4.137   \u001b[0m | \u001b[0m 2.159   \u001b[0m | \u001b[0m 2.56    \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-1.183   \u001b[0m | \u001b[0m 75.58   \u001b[0m | \u001b[0m 3.713   \u001b[0m | \u001b[0m 0.04931 \u001b[0m | \u001b[0m 4.352   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.9532  \u001b[0m | \u001b[0m 10.89   \u001b[0m | \u001b[0m 4.246   \u001b[0m | \u001b[0m 2.379   \u001b[0m | \u001b[0m 4.536   \u001b[0m |\n",
      "| \u001b[95m 12      \u001b[0m | \u001b[95m-0.9463  \u001b[0m | \u001b[95m 17.76   \u001b[0m | \u001b[95m 6.93    \u001b[0m | \u001b[95m 0.1753  \u001b[0m | \u001b[95m 3.435   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.9478  \u001b[0m | \u001b[0m 30.85   \u001b[0m | \u001b[0m 6.849   \u001b[0m | \u001b[0m 2.983   \u001b[0m | \u001b[0m 1.378   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-1.183   \u001b[0m | \u001b[0m 44.35   \u001b[0m | \u001b[0m 7.023   \u001b[0m | \u001b[0m 3.304   \u001b[0m | \u001b[0m 1.868   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-1.183   \u001b[0m | \u001b[0m 91.32   \u001b[0m | \u001b[0m 5.95    \u001b[0m | \u001b[0m 4.178   \u001b[0m | \u001b[0m 3.365   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.9624  \u001b[0m | \u001b[0m 28.3    \u001b[0m | \u001b[0m 3.283   \u001b[0m | \u001b[0m 1.458   \u001b[0m | \u001b[0m 2.327   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-1.183   \u001b[0m | \u001b[0m 88.28   \u001b[0m | \u001b[0m 7.126   \u001b[0m | \u001b[0m 3.473   \u001b[0m | \u001b[0m 3.351   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.95    \u001b[0m | \u001b[0m 24.37   \u001b[0m | \u001b[0m 5.475   \u001b[0m | \u001b[0m 4.108   \u001b[0m | \u001b[0m 4.685   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-1.183   \u001b[0m | \u001b[0m 53.48   \u001b[0m | \u001b[0m 5.814   \u001b[0m | \u001b[0m 0.1062  \u001b[0m | \u001b[0m 2.266   \u001b[0m |\n",
      "| \u001b[95m 20      \u001b[0m | \u001b[95m-0.9375  \u001b[0m | \u001b[95m 0.7165  \u001b[0m | \u001b[95m 5.889   \u001b[0m | \u001b[95m 2.892   \u001b[0m | \u001b[95m 3.63    \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.9489  \u001b[0m | \u001b[0m 14.57   \u001b[0m | \u001b[0m 5.495   \u001b[0m | \u001b[0m 2.448   \u001b[0m | \u001b[0m 1.496   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-1.183   \u001b[0m | \u001b[0m 45.67   \u001b[0m | \u001b[0m 3.231   \u001b[0m | \u001b[0m 3.235   \u001b[0m | \u001b[0m 2.552   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-1.183   \u001b[0m | \u001b[0m 56.69   \u001b[0m | \u001b[0m 4.891   \u001b[0m | \u001b[0m 4.183   \u001b[0m | \u001b[0m 3.261   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-1.183   \u001b[0m | \u001b[0m 98.64   \u001b[0m | \u001b[0m 6.971   \u001b[0m | \u001b[0m 2.433   \u001b[0m | \u001b[0m 1.742   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-1.183   \u001b[0m | \u001b[0m 88.52   \u001b[0m | \u001b[0m 3.671   \u001b[0m | \u001b[0m 4.405   \u001b[0m | \u001b[0m 4.784   \u001b[0m |\n",
      "| \u001b[95m 26      \u001b[0m | \u001b[95m-0.9371  \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 8.0     \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 5.0     \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.9483  \u001b[0m | \u001b[0m 25.12   \u001b[0m | \u001b[0m 8.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.9375  \u001b[0m | \u001b[0m 1.128   \u001b[0m | \u001b[0m 7.895   \u001b[0m | \u001b[0m 0.3371  \u001b[0m | \u001b[0m 1.196   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.9406  \u001b[0m | \u001b[0m 0.2123  \u001b[0m | \u001b[0m 3.574   \u001b[0m | \u001b[0m 4.415   \u001b[0m | \u001b[0m 1.082   \u001b[0m |\n",
      "| \u001b[95m 30      \u001b[0m | \u001b[95m-0.9367  \u001b[0m | \u001b[95m 0.5559  \u001b[0m | \u001b[95m 7.881   \u001b[0m | \u001b[95m 4.868   \u001b[0m | \u001b[95m 4.535   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.9406  \u001b[0m | \u001b[0m 0.3137  \u001b[0m | \u001b[0m 3.147   \u001b[0m | \u001b[0m 4.697   \u001b[0m | \u001b[0m 4.84    \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.9436  \u001b[0m | \u001b[0m 9.286   \u001b[0m | \u001b[0m 7.901   \u001b[0m | \u001b[0m 0.04259 \u001b[0m | \u001b[0m 1.419   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.9385  \u001b[0m | \u001b[0m 0.05085 \u001b[0m | \u001b[0m 5.774   \u001b[0m | \u001b[0m 0.0209  \u001b[0m | \u001b[0m 1.274   \u001b[0m |\n",
      "| \u001b[95m 34      \u001b[0m | \u001b[95m-0.9367  \u001b[0m | \u001b[95m 0.08307 \u001b[0m | \u001b[95m 7.596   \u001b[0m | \u001b[95m 3.979   \u001b[0m | \u001b[95m 1.121   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.9467  \u001b[0m | \u001b[0m 28.66   \u001b[0m | \u001b[0m 7.494   \u001b[0m | \u001b[0m 4.892   \u001b[0m | \u001b[0m 4.489   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.95    \u001b[0m | \u001b[0m 28.69   \u001b[0m | \u001b[0m 5.772   \u001b[0m | \u001b[0m 4.667   \u001b[0m | \u001b[0m 1.001   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.9436  \u001b[0m | \u001b[0m 9.854   \u001b[0m | \u001b[0m 7.995   \u001b[0m | \u001b[0m 4.804   \u001b[0m | \u001b[0m 1.663   \u001b[0m |\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "bo_tuning_cat = bayesian_opt_cat(train_, y_tr, valid_, y_val, list(train_.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': -0.9362267064104927,\n",
       " 'params': {'b_frac': 0.3677516358370858,\n",
       "  'data_in_leaf': 495.44417416221626,\n",
       "  'f_frac': 0.5422060360159515,\n",
       "  'hessian': 5.039378213231793,\n",
       "  'l1': 1.0642598045225304,\n",
       "  'l2': 2.564544963055539,\n",
       "  'leaves': 89.62655396835916,\n",
       "  'split_gain': 0.16542750189034394}}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo_tuning_lgb.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': -0.9366723276780314,\n",
       " 'params': {'b_temp': 0.08307474720468191,\n",
       "  'depth': 7.596402546589758,\n",
       "  'l2': 3.9791105400066655,\n",
       "  'r_str': 1.1206250787323229}}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo_tuning_cat.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256068, 57), (85356, 57))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_1 = train_.copy()\n",
    "valid_1 = valid_.copy()\n",
    "\n",
    "train_1.shape, valid_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def get_normalization(train_df, test_df, which_norm=None, int_cols=None, cat_cols=None):\n",
    "    \"\"\" normalization of data features\n",
    "    Args:\n",
    "        train_df, test_df\n",
    "        which_norm: normalization for numerical columns, ['stdc', 'min-max']\n",
    "        int_cols: list of numerical_columns\n",
    "        cat_cols: list of categorical columns\n",
    "    Return:\n",
    "        standarization scaled version for numeric data\n",
    "        min-max scaled version for categorical data\n",
    "    example:\n",
    "\n",
    "        from sklearn import datasets\n",
    "        iris = datasets.load_iris()\n",
    "        X = iris.data; y = iris.target\n",
    "        X = pd.DataFrame(data=X, columns=iris['feature_names'])\n",
    "        get_normalization(X.iloc[:10,:], X.iloc[10:15,:], 'min-max',int_cols=X.columns)\n",
    "        get_normalization(X.iloc[:10,:], X.iloc[10:15,:], int_cols='sepal width (cm)')\n",
    "        get_normalization(X.iloc[:10,:], X.iloc[10:15,:], int_cols=X.columns)\n",
    "    \"\"\"\n",
    "    complete_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "    stdc = StandardScaler()\n",
    "    min_max = MinMaxScaler()\n",
    "\n",
    "    if int_cols is not None:\n",
    "        if which_norm == 'stdc': norm = StandardScaler()\n",
    "        elif which_norm == 'min-max': norm = MinMaxScaler()\n",
    "        else: \n",
    "            norm = StandardScaler()\n",
    "            print(\"by default: stdc norm is running\")\n",
    "        try: \n",
    "            complete_df[int_cols] = norm.fit_transform(complete_df[int_cols])\n",
    "        except: \n",
    "            # expecting a 2d array, but passed i-d array\n",
    "            complete_df[[int_cols]] = norm.fit_transform(complete_df[[int_cols]])\n",
    "        print(\"done with stdc normalization on numerical columns\")\n",
    "        \n",
    "    if cat_cols is not None:\n",
    "        norm = MinMaxScaler()\n",
    "        try: \n",
    "            complete_df[cat_cols] = norm.fit_transform(complete_df[cat_cols])\n",
    "        except:\n",
    "            complete_df[[cat_cols]] = norm.fit_transform(complete_df[[cat_cols]])\n",
    "        print(\"done with min-max normalization on categorical columns\")\n",
    "\n",
    "    train_df_new = complete_df.iloc[:train_df.shape[0],:]\n",
    "    test_df_new  = complete_df.iloc[train_df.shape[0]:,:]\n",
    "#     print(train_df_new.shape, test_df_new.shape)\n",
    "    \n",
    "    test_df_new = test_df_new.reset_index(drop=True)\n",
    "    \n",
    "    del complete_df\n",
    "    gc.collect()\n",
    "    return train_df_new, test_df_new\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int8, int16 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with min-max normalization on categorical columns\n"
     ]
    }
   ],
   "source": [
    "train_1, valid_1 = get_normalization(train_1, valid_1, cat_cols=list(train_1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def regression_model(reg_name):\n",
    "\n",
    "    if clf_name == 'bayesian_ridge':\n",
    "        params = {'alpha1' : 0.1, \n",
    "                  'alpha2' : 0.1,\n",
    "                  'lambda1': 0.1,\n",
    "                  'lambda2': 0.1}   \n",
    "    else:\n",
    "        params = {'alpha': 0.1}   \n",
    "\n",
    "\n",
    "    if reg_name == 'bayesian_ridge':\n",
    "        reg = BayesianRidge(\n",
    "            n_iter=300, tol=0.001, compute_score=False,\n",
    "            alpha_1=params['alpha1'], alpha_2=params['alpha2'], \n",
    "            lambda_1=params['lambda1'], lambda_2=params['lambda2'],  \n",
    "            fit_intercept=True, normalize=True, verbose=False)\n",
    "\n",
    "    elif reg_name is 'linear':\n",
    "        reg = LinearRegression(fit_intercept=True, normalize=True, n_jobs=-1)\n",
    "\n",
    "    elif reg_name is 'lasso':\n",
    "        reg = Lasso(\n",
    "            alpha=params['alpha'], fit_intercept=True, normalize=True, \n",
    "            positive=False, random_state=1234, selection='cyclic')\n",
    "\n",
    "    \n",
    "    elif reg_name is 'elastic_net':\n",
    "        reg = ElasticNet(\n",
    "            alpha=params['alpha'], l1_ratio=0.5, fit_intercept=True, \n",
    "            normalize=True, random_state=1234, selection='cyclic')\n",
    "    \n",
    "    elif reg_name is 'ridge':\n",
    "        reg = Ridge(\n",
    "            alpha=params['alpha'], fit_intercept=True, normalize=True, \n",
    "            max_iter=500, random_state=1234)\n",
    "\n",
    "    elif reg_name is 'kernel_ridge':\n",
    "        reg = KernelRidge(\n",
    "            alpha=params['alpha'], kernel=kernel_func, gamma=None, \n",
    "            degree=degree, coef0=1, kernel_params=None)\n",
    "    \n",
    "    elif reg_name is 'svm':\n",
    "        reg = SVR(\n",
    "            kernel=kernel_func, degree=degree, coef0=0.0, tol=0.001, \n",
    "            C=params['alpha'], epsilon=0.1, shrinking=True, cache_size=200, \n",
    "            verbose=0, max_iter=500)\n",
    "    \n",
    "    else:\n",
    "        raise Exception('only [bayesian_ridge, lasso, elastic_net, ridge, kernel_ridge, svm] are supported')\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_hyperopt_reg(X_train_, X_valid_, y_train_, y_valid_, max_evals, clf_name, std_norm=True, kernel_func='linear', degree=3):\n",
    "    \"\"\" Return best hyperparameter (mainly regularization parameters)\n",
    "    Args:\n",
    "      train_df, target\n",
    "      max_evals: Total number of iteration to perform for bayesian optimization\n",
    "      clf_name: name of regression model. [bayesian_ridge, lasso, elastic_net, ridge, kernel_ridge, svm]\n",
    "      kernel_func: [linear, rbf, poly]\n",
    "      degree: degree of polynomial kernel\n",
    "    example:\n",
    "      best, tpe_trials = run_hyperopt(train_df, target, 20, 'kernel_ridge', kernel_func='poly')\n",
    "    \"\"\"\n",
    "    def bayesian_opt(params):\n",
    "        random_seed = 1234\n",
    "#         n_splits = 3\n",
    "\n",
    "#         folds = KFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "\n",
    "        score_cv = []\n",
    "\n",
    "        # for fold_, (train_index, valid_index) in enumerate(folds.split(train_df, target)):\n",
    "\n",
    "        #     y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n",
    "        #     X_train, X_valid = train_df.iloc[train_index,:], train_df.iloc[valid_index,:]\n",
    "        X_train, X_valid, y_train, y_valid = X_train_, X_valid_, y_train_, y_valid_\n",
    "\n",
    "        if clf_name == 'bayesian_ridge':\n",
    "        \n",
    "            reg = BayesianRidge(\n",
    "                n_iter=300, tol=0.001, compute_score=False,\n",
    "                alpha_1=params['alpha1'], alpha_2=params['alpha2'], \n",
    "                lambda_1=params['lambda1'], lambda_2=params['lambda2'],  \n",
    "                fit_intercept=True, normalize=std_norm, verbose=False)\n",
    "        \n",
    "        elif clf_name is 'lasso':\n",
    "            reg = Lasso(\n",
    "                alpha=params['alpha'], fit_intercept=True, normalize=std_norm, \n",
    "                positive=False, random_state=1234, selection='cyclic')\n",
    "        \n",
    "        elif clf_name is 'elastic_net':\n",
    "            reg = ElasticNet(\n",
    "                alpha=params['alpha'], l1_ratio=0.5, fit_intercept=True, \n",
    "                normalize=std_norm, random_state=1234, selection='cyclic')\n",
    "        \n",
    "        elif clf_name is 'ridge':\n",
    "            reg = Ridge(\n",
    "                alpha=params['alpha'], fit_intercept=True, normalize=std_norm, \n",
    "                max_iter=500, random_state=1234)\n",
    "\n",
    "        elif clf_name is 'kernel_ridge':\n",
    "            reg = KernelRidge(\n",
    "                alpha=params['alpha'], kernel=kernel_func, gamma=None, \n",
    "                degree=degree, coef0=1, kernel_params=None)\n",
    "        \n",
    "        elif clf_name is 'svm':\n",
    "            reg = SVR(\n",
    "                kernel=kernel_func, degree=degree, coef0=0.0, tol=0.001, \n",
    "                C=params['alpha'], epsilon=0.1, shrinking=True, cache_size=200, \n",
    "                verbose=0, max_iter=500)\n",
    "        \n",
    "        else:\n",
    "            raise Exception('only [bayesian_ridge, lasso, elastic_net, ridge, kernel_ridge, svm] are supported')\n",
    "        reg.fit(X_train, y_train)\n",
    "#         score = reg.score(X_valid, y_valid)\n",
    "        \n",
    "        pred = reg.predict(X_valid)\n",
    "        score = mean_squared_error(y_val, pred)\n",
    "#         score = roc_auc_score(y_valid, oof)\n",
    "        score_cv.append(score)\n",
    "\n",
    "        return np.mean(score_cv)\n",
    "\n",
    "    if clf_name == 'bayesian_ridge':\n",
    "        bayesian_params = {'alpha1': hp.uniform('alpha1', 0.0001, 5),\n",
    "                           'alpha2': hp.uniform('alpha2', 0.0001, 5),\n",
    "                           'lambda1': hp.uniform('lambda1', 0.0001, 5),\n",
    "                           'lambda2': hp.uniform('lambda2', 0.0001, 5),}   \n",
    "    else:\n",
    "        bayesian_params = {'alpha': hp.uniform('alpha', 0.01, 1000),}   \n",
    "\n",
    "    \n",
    "    trials = Trials()\n",
    "    results = fmin(bayesian_opt, bayesian_params, algo=tpe.suggest, \n",
    "                   trials=trials, max_evals=max_evals)\n",
    "        \n",
    "    return results, trials \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.56it/s, best loss: 0.9905023965973607]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, rand, Trials, tpe\n",
    "\n",
    "res, _ = run_hyperopt_reg(train_, valid_, y_tr, y_val, 20, 'ridge', std_norm=False, kernel_func='linear', degree=3)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm ==================================================\n",
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/20 [00:03<01:05,  3.46s/it, best loss: 5698.8877601734175]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:06<00:59,  3.30s/it, best loss: 5698.8877601734175]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:09<00:54,  3.21s/it, best loss: 5698.8877601734175]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:12<00:52,  3.26s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:15<00:47,  3.14s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:18<00:42,  3.06s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:21<00:38,  3.00s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:24<00:36,  3.03s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:27<00:33,  3.01s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:30<00:30,  3.00s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:33<00:27,  3.11s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:36<00:24,  3.03s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:39<00:20,  2.97s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:42<00:18,  3.01s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:45<00:15,  3.03s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [00:48<00:12,  3.01s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [00:51<00:08,  3.00s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [00:54<00:05,  3.00s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [00:57<00:02,  2.99s/it, best loss: 5698.8870165776225]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:00<00:00,  3.00s/it, best loss: 5698.8870165776225]\n",
      "{'alpha': 579.5817266387144}\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"bayesian ridge\",\"=\"*50)\n",
    "# best, _ = run_hyperopt_reg(train_, valid_, y_tr, y_val, 20, 'bayesian_ridge', std_norm=False, kernel_func='linear', degree=3)\n",
    "# print(best)\n",
    "\n",
    "# print(\"lasso\",\"=\"*50)\n",
    "# best, _ = run_hyperopt_reg(train_, valid_, y_tr, y_val, 20, 'lasso', std_norm=False, kernel_func='linear', degree=3)\n",
    "# print(best)\n",
    "\n",
    "# print(\"elastic_net\", \"=\"*50)\n",
    "# best, _ = run_hyperopt_reg(train_, valid_, y_tr, y_val, 20, 'elastic_net', std_norm=False, kernel_func='linear', degree=3)\n",
    "# print(best)\n",
    "\n",
    "# print(\"ridge\", \"=\"*50)\n",
    "# best, _ = run_hyperopt_reg(train_, valid_, y_tr, y_val, 20, 'ridge', std_norm=False, kernel_func='linear', degree=3)\n",
    "# print(best)\n",
    "\n",
    "# print(\"kernel_ridge\", \"=\"*50)\n",
    "# best, _ = run_hyperopt_reg(train_, valid_, y_tr, y_val, 20, 'kernel_ridge', std_norm=False, kernel_func='linear', degree=3)\n",
    "# print(best)\n",
    "\n",
    "# print(\"kernel_ridge\", \"=\"*50)\n",
    "# best, _ = run_hyperopt_reg(train_, valid_, y_tr, y_val, 20, 'kernel_ridge', std_norm=False, kernel_func='rbf', degree=3)\n",
    "# print(best)\n",
    "\n",
    "# print(\"kernel_ridge\", \"=\"*50)\n",
    "# best, _ = run_hyperopt_reg(train_, valid_, y_tr, y_val, 20, 'kernel_ridge', std_norm=False, kernel_func='poly', degree=3)\n",
    "# print(best)\n",
    "\n",
    "print(\"svm\", \"=\"*50)\n",
    "best, _ = run_hyperopt_reg(train_, valid_, y_tr, y_val, 20, 'svm', std_norm=False, kernel_func='poly', degree=3)\n",
    "print(best)\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian ridge ==================================================\n",
    "100%|██████████| 20/20 [00:36<00:00,  1.99s/it, best loss: 0.9904955340098365]\n",
    "{'alpha1': 4.14833136265648, 'alpha2': 7.0581688690887034, 'lambda1': 2.002244407198414, 'lambda2': 9.057908464475108}\n",
    "lasso ==================================================\n",
    "100%|██████████| 20/20 [00:04<00:00,  5.00it/s, best loss: 1.1826605810647528]\n",
    "{'alpha': 353.8346501346175}\n",
    "elastic_net ==================================================\n",
    "100%|██████████| 20/20 [00:04<00:00,  4.76it/s, best loss: 1.1826605810647528]\n",
    "{'alpha': 770.499599294759}\n",
    "ridge ==================================================\n",
    "100%|██████████| 20/20 [00:04<00:00,  4.43it/s, best loss: 0.9904963496185275]\n",
    "{'alpha': 12.63341144211758}\n",
    "svm ==================================================\n",
    "100%|██████████| 20/20 [01:00<00:00,  3.00s/it, best loss: 5698.8870165776225]\n",
    "{'alpha': 579.5817266387144}\n",
    "==================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['booking_type_code', 'channel_code', 'cluster_code',\n",
       "       'main_product_code', 'member_age_buckets', 'numberofadults',\n",
       "       'numberofchildren', 'persontravellingid', 'reservationstatusid_code',\n",
       "       'resort_id', 'resort_region_code', 'resort_type_code',\n",
       "       'room_type_booked_code', 'roomnights', 'season_holidayed_code',\n",
       "       'state_code_residence', 'state_code_resort', 'total_pax', 'tr_flag',\n",
       "       'days_diff', 'booking_week', 'booking_month', 'booking_year',\n",
       "       'booking_dow', 'checkin_week', 'checkin_month', 'checkin_year',\n",
       "       'checkin_dow', 'mem_resortRegion_median', 'mem_resortRegion_count',\n",
       "       'mem_resortRegion_max', 'mem_resortType_median', 'mem_resortType_count',\n",
       "       'mem_resortType_max', 'mem_staResidence_median',\n",
       "       'mem_staResidence_count', 'mem_staResidence_max', 'mem_stResort_median',\n",
       "       'mem_stResort_count', 'mem_stResort_max', 'res_staResidence_median',\n",
       "       'res_staResidence_count', 'res_staResidence_max', 'res_stResort_median',\n",
       "       'res_stResort_count', 'res_stResort_max', 'res_cluster_median',\n",
       "       'res_cluster_count', 'res_cluster_max', 'res_resortType_median',\n",
       "       'res_resortType_count', 'res_resortType_max',\n",
       "       'dayDiff_resortRegion_count', 'dayDiff_resortType_count',\n",
       "       'dayDiff_staResidence_count', 'dayDiff_stResort_count',\n",
       "       'dayDiff_cluster_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>booking_type_code</th>\n",
       "      <th>channel_code</th>\n",
       "      <th>cluster_code</th>\n",
       "      <th>main_product_code</th>\n",
       "      <th>member_age_buckets</th>\n",
       "      <th>numberofadults</th>\n",
       "      <th>numberofchildren</th>\n",
       "      <th>persontravellingid</th>\n",
       "      <th>reservationstatusid_code</th>\n",
       "      <th>resort_id</th>\n",
       "      <th>resort_region_code</th>\n",
       "      <th>resort_type_code</th>\n",
       "      <th>room_type_booked_code</th>\n",
       "      <th>roomnights</th>\n",
       "      <th>season_holidayed_code</th>\n",
       "      <th>state_code_residence</th>\n",
       "      <th>state_code_resort</th>\n",
       "      <th>total_pax</th>\n",
       "      <th>tr_flag</th>\n",
       "      <th>days_diff</th>\n",
       "      <th>booking_week</th>\n",
       "      <th>booking_month</th>\n",
       "      <th>booking_year</th>\n",
       "      <th>booking_dow</th>\n",
       "      <th>checkin_week</th>\n",
       "      <th>checkin_month</th>\n",
       "      <th>checkin_year</th>\n",
       "      <th>checkin_dow</th>\n",
       "      <th>mem_resortRegion_median</th>\n",
       "      <th>mem_resortRegion_count</th>\n",
       "      <th>mem_resortRegion_max</th>\n",
       "      <th>mem_resortType_median</th>\n",
       "      <th>mem_resortType_count</th>\n",
       "      <th>mem_resortType_max</th>\n",
       "      <th>mem_staResidence_median</th>\n",
       "      <th>mem_staResidence_count</th>\n",
       "      <th>mem_staResidence_max</th>\n",
       "      <th>mem_stResort_median</th>\n",
       "      <th>mem_stResort_count</th>\n",
       "      <th>mem_stResort_max</th>\n",
       "      <th>res_staResidence_median</th>\n",
       "      <th>res_staResidence_count</th>\n",
       "      <th>res_staResidence_max</th>\n",
       "      <th>res_stResort_median</th>\n",
       "      <th>res_stResort_count</th>\n",
       "      <th>res_stResort_max</th>\n",
       "      <th>res_cluster_median</th>\n",
       "      <th>res_cluster_count</th>\n",
       "      <th>res_cluster_max</th>\n",
       "      <th>res_resortType_median</th>\n",
       "      <th>res_resortType_count</th>\n",
       "      <th>res_resortType_max</th>\n",
       "      <th>dayDiff_resortRegion_count</th>\n",
       "      <th>dayDiff_resortType_count</th>\n",
       "      <th>dayDiff_staResidence_count</th>\n",
       "      <th>dayDiff_stResort_count</th>\n",
       "      <th>dayDiff_cluster_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>booking_type_code_0</td>\n",
       "      <td>channel_code_2</td>\n",
       "      <td>cluster_code_5</td>\n",
       "      <td>main_product_code_0</td>\n",
       "      <td>member_age_buckets_5</td>\n",
       "      <td>numberofadults_1</td>\n",
       "      <td>numberofchildren_0</td>\n",
       "      <td>persontravellingid_1</td>\n",
       "      <td>reservationstatusid_code_2</td>\n",
       "      <td>resort_id_0</td>\n",
       "      <td>resort_region_code_2</td>\n",
       "      <td>resort_type_code_3</td>\n",
       "      <td>room_type_booked_code_2</td>\n",
       "      <td>roomnights_2</td>\n",
       "      <td>season_holidayed_code_1</td>\n",
       "      <td>state_code_residence_6</td>\n",
       "      <td>state_code_resort_2</td>\n",
       "      <td>total_pax_2</td>\n",
       "      <td>tr_flag_0</td>\n",
       "      <td>days_diff_0</td>\n",
       "      <td>booking_week_13</td>\n",
       "      <td>booking_month_3</td>\n",
       "      <td>booking_year_4</td>\n",
       "      <td>booking_dow_3</td>\n",
       "      <td>checkin_week_13</td>\n",
       "      <td>checkin_month_3</td>\n",
       "      <td>checkin_year_4</td>\n",
       "      <td>checkin_dow_3</td>\n",
       "      <td>mem_resortRegion_median_3</td>\n",
       "      <td>mem_resortRegion_count_3</td>\n",
       "      <td>mem_resortRegion_max_4</td>\n",
       "      <td>mem_resortType_median_0</td>\n",
       "      <td>mem_resortType_count_2</td>\n",
       "      <td>mem_resortType_max_4</td>\n",
       "      <td>mem_staResidence_median_5</td>\n",
       "      <td>mem_staResidence_count_13</td>\n",
       "      <td>mem_staResidence_max_5</td>\n",
       "      <td>mem_stResort_median_1</td>\n",
       "      <td>mem_stResort_count_1</td>\n",
       "      <td>mem_stResort_max_1</td>\n",
       "      <td>res_staResidence_median_4</td>\n",
       "      <td>res_staResidence_count_221</td>\n",
       "      <td>res_staResidence_max_9</td>\n",
       "      <td>res_stResort_median_1</td>\n",
       "      <td>res_stResort_count_4</td>\n",
       "      <td>res_stResort_max_1</td>\n",
       "      <td>res_cluster_median_1</td>\n",
       "      <td>res_cluster_count_4</td>\n",
       "      <td>res_cluster_max_1</td>\n",
       "      <td>res_resortType_median_1</td>\n",
       "      <td>res_resortType_count_4</td>\n",
       "      <td>res_resortType_max_1</td>\n",
       "      <td>dayDiff_resortRegion_count_15</td>\n",
       "      <td>dayDiff_resortType_count_44</td>\n",
       "      <td>dayDiff_staResidence_count_163</td>\n",
       "      <td>dayDiff_stResort_count_68</td>\n",
       "      <td>dayDiff_cluster_count_44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>booking_type_code_0</td>\n",
       "      <td>channel_code_0</td>\n",
       "      <td>cluster_code_5</td>\n",
       "      <td>main_product_code_0</td>\n",
       "      <td>member_age_buckets_5</td>\n",
       "      <td>numberofadults_1</td>\n",
       "      <td>numberofchildren_0</td>\n",
       "      <td>persontravellingid_1</td>\n",
       "      <td>reservationstatusid_code_0</td>\n",
       "      <td>resort_id_1</td>\n",
       "      <td>resort_region_code_2</td>\n",
       "      <td>resort_type_code_3</td>\n",
       "      <td>room_type_booked_code_3</td>\n",
       "      <td>roomnights_6</td>\n",
       "      <td>season_holidayed_code_1</td>\n",
       "      <td>state_code_residence_6</td>\n",
       "      <td>state_code_resort_4</td>\n",
       "      <td>total_pax_1</td>\n",
       "      <td>tr_flag_0</td>\n",
       "      <td>days_diff_4</td>\n",
       "      <td>booking_week_3</td>\n",
       "      <td>booking_month_0</td>\n",
       "      <td>booking_year_1</td>\n",
       "      <td>booking_dow_4</td>\n",
       "      <td>checkin_week_14</td>\n",
       "      <td>checkin_month_3</td>\n",
       "      <td>checkin_year_1</td>\n",
       "      <td>checkin_dow_5</td>\n",
       "      <td>mem_resortRegion_median_3</td>\n",
       "      <td>mem_resortRegion_count_3</td>\n",
       "      <td>mem_resortRegion_max_4</td>\n",
       "      <td>mem_resortType_median_0</td>\n",
       "      <td>mem_resortType_count_2</td>\n",
       "      <td>mem_resortType_max_4</td>\n",
       "      <td>mem_staResidence_median_5</td>\n",
       "      <td>mem_staResidence_count_13</td>\n",
       "      <td>mem_staResidence_max_5</td>\n",
       "      <td>mem_stResort_median_7</td>\n",
       "      <td>mem_stResort_count_1</td>\n",
       "      <td>mem_stResort_max_5</td>\n",
       "      <td>res_staResidence_median_6</td>\n",
       "      <td>res_staResidence_count_266</td>\n",
       "      <td>res_staResidence_max_19</td>\n",
       "      <td>res_stResort_median_2</td>\n",
       "      <td>res_stResort_count_9</td>\n",
       "      <td>res_stResort_max_13</td>\n",
       "      <td>res_cluster_median_2</td>\n",
       "      <td>res_cluster_count_9</td>\n",
       "      <td>res_cluster_max_13</td>\n",
       "      <td>res_resortType_median_2</td>\n",
       "      <td>res_resortType_count_9</td>\n",
       "      <td>res_resortType_max_13</td>\n",
       "      <td>dayDiff_resortRegion_count_9</td>\n",
       "      <td>dayDiff_resortType_count_29</td>\n",
       "      <td>dayDiff_staResidence_count_120</td>\n",
       "      <td>dayDiff_stResort_count_41</td>\n",
       "      <td>dayDiff_cluster_count_26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>booking_type_code_0</td>\n",
       "      <td>channel_code_0</td>\n",
       "      <td>cluster_code_4</td>\n",
       "      <td>main_product_code_0</td>\n",
       "      <td>member_age_buckets_5</td>\n",
       "      <td>numberofadults_1</td>\n",
       "      <td>numberofchildren_0</td>\n",
       "      <td>persontravellingid_2</td>\n",
       "      <td>reservationstatusid_code_0</td>\n",
       "      <td>resort_id_2</td>\n",
       "      <td>resort_region_code_0</td>\n",
       "      <td>resort_type_code_5</td>\n",
       "      <td>room_type_booked_code_3</td>\n",
       "      <td>roomnights_5</td>\n",
       "      <td>season_holidayed_code_1</td>\n",
       "      <td>state_code_residence_6</td>\n",
       "      <td>state_code_resort_0</td>\n",
       "      <td>total_pax_1</td>\n",
       "      <td>tr_flag_0</td>\n",
       "      <td>days_diff_3</td>\n",
       "      <td>booking_week_4</td>\n",
       "      <td>booking_month_0</td>\n",
       "      <td>booking_year_1</td>\n",
       "      <td>booking_dow_2</td>\n",
       "      <td>checkin_week_4</td>\n",
       "      <td>checkin_month_1</td>\n",
       "      <td>checkin_year_1</td>\n",
       "      <td>checkin_dow_6</td>\n",
       "      <td>mem_resortRegion_median_7</td>\n",
       "      <td>mem_resortRegion_count_3</td>\n",
       "      <td>mem_resortRegion_max_4</td>\n",
       "      <td>mem_resortType_median_6</td>\n",
       "      <td>mem_resortType_count_1</td>\n",
       "      <td>mem_resortType_max_3</td>\n",
       "      <td>mem_staResidence_median_5</td>\n",
       "      <td>mem_staResidence_count_13</td>\n",
       "      <td>mem_staResidence_max_5</td>\n",
       "      <td>mem_stResort_median_7</td>\n",
       "      <td>mem_stResort_count_1</td>\n",
       "      <td>mem_stResort_max_4</td>\n",
       "      <td>res_staResidence_median_4</td>\n",
       "      <td>res_staResidence_count_324</td>\n",
       "      <td>res_staResidence_max_15</td>\n",
       "      <td>res_stResort_median_1</td>\n",
       "      <td>res_stResort_count_18</td>\n",
       "      <td>res_stResort_max_5</td>\n",
       "      <td>res_cluster_median_1</td>\n",
       "      <td>res_cluster_count_18</td>\n",
       "      <td>res_cluster_max_5</td>\n",
       "      <td>res_resortType_median_1</td>\n",
       "      <td>res_resortType_count_18</td>\n",
       "      <td>res_resortType_max_5</td>\n",
       "      <td>dayDiff_resortRegion_count_13</td>\n",
       "      <td>dayDiff_resortType_count_32</td>\n",
       "      <td>dayDiff_staResidence_count_148</td>\n",
       "      <td>dayDiff_stResort_count_61</td>\n",
       "      <td>dayDiff_cluster_count_27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>booking_type_code_0</td>\n",
       "      <td>channel_code_0</td>\n",
       "      <td>cluster_code_3</td>\n",
       "      <td>main_product_code_0</td>\n",
       "      <td>member_age_buckets_5</td>\n",
       "      <td>numberofadults_1</td>\n",
       "      <td>numberofchildren_2</td>\n",
       "      <td>persontravellingid_1</td>\n",
       "      <td>reservationstatusid_code_0</td>\n",
       "      <td>resort_id_3</td>\n",
       "      <td>resort_region_code_1</td>\n",
       "      <td>resort_type_code_2</td>\n",
       "      <td>room_type_booked_code_2</td>\n",
       "      <td>roomnights_6</td>\n",
       "      <td>season_holidayed_code_1</td>\n",
       "      <td>state_code_residence_6</td>\n",
       "      <td>state_code_resort_1</td>\n",
       "      <td>total_pax_1</td>\n",
       "      <td>tr_flag_0</td>\n",
       "      <td>days_diff_4</td>\n",
       "      <td>booking_week_17</td>\n",
       "      <td>booking_month_4</td>\n",
       "      <td>booking_year_1</td>\n",
       "      <td>booking_dow_5</td>\n",
       "      <td>checkin_week_23</td>\n",
       "      <td>checkin_month_5</td>\n",
       "      <td>checkin_year_1</td>\n",
       "      <td>checkin_dow_3</td>\n",
       "      <td>mem_resortRegion_median_7</td>\n",
       "      <td>mem_resortRegion_count_5</td>\n",
       "      <td>mem_resortRegion_max_5</td>\n",
       "      <td>mem_resortType_median_8</td>\n",
       "      <td>mem_resortType_count_2</td>\n",
       "      <td>mem_resortType_max_4</td>\n",
       "      <td>mem_staResidence_median_5</td>\n",
       "      <td>mem_staResidence_count_13</td>\n",
       "      <td>mem_staResidence_max_5</td>\n",
       "      <td>mem_stResort_median_9</td>\n",
       "      <td>mem_stResort_count_2</td>\n",
       "      <td>mem_stResort_max_5</td>\n",
       "      <td>res_staResidence_median_4</td>\n",
       "      <td>res_staResidence_count_398</td>\n",
       "      <td>res_staResidence_max_43</td>\n",
       "      <td>res_stResort_median_1</td>\n",
       "      <td>res_stResort_count_31</td>\n",
       "      <td>res_stResort_max_17</td>\n",
       "      <td>res_cluster_median_1</td>\n",
       "      <td>res_cluster_count_31</td>\n",
       "      <td>res_cluster_max_17</td>\n",
       "      <td>res_resortType_median_1</td>\n",
       "      <td>res_resortType_count_31</td>\n",
       "      <td>res_resortType_max_17</td>\n",
       "      <td>dayDiff_resortRegion_count_11</td>\n",
       "      <td>dayDiff_resortType_count_35</td>\n",
       "      <td>dayDiff_staResidence_count_120</td>\n",
       "      <td>dayDiff_stResort_count_57</td>\n",
       "      <td>dayDiff_cluster_count_23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>booking_type_code_0</td>\n",
       "      <td>channel_code_0</td>\n",
       "      <td>cluster_code_3</td>\n",
       "      <td>main_product_code_0</td>\n",
       "      <td>member_age_buckets_5</td>\n",
       "      <td>numberofadults_1</td>\n",
       "      <td>numberofchildren_0</td>\n",
       "      <td>persontravellingid_1</td>\n",
       "      <td>reservationstatusid_code_0</td>\n",
       "      <td>resort_id_3</td>\n",
       "      <td>resort_region_code_1</td>\n",
       "      <td>resort_type_code_2</td>\n",
       "      <td>room_type_booked_code_3</td>\n",
       "      <td>roomnights_6</td>\n",
       "      <td>season_holidayed_code_1</td>\n",
       "      <td>state_code_residence_6</td>\n",
       "      <td>state_code_resort_1</td>\n",
       "      <td>total_pax_1</td>\n",
       "      <td>tr_flag_0</td>\n",
       "      <td>days_diff_4</td>\n",
       "      <td>booking_week_35</td>\n",
       "      <td>booking_month_8</td>\n",
       "      <td>booking_year_1</td>\n",
       "      <td>booking_dow_2</td>\n",
       "      <td>checkin_week_50</td>\n",
       "      <td>checkin_month_11</td>\n",
       "      <td>checkin_year_1</td>\n",
       "      <td>checkin_dow_0</td>\n",
       "      <td>mem_resortRegion_median_7</td>\n",
       "      <td>mem_resortRegion_count_5</td>\n",
       "      <td>mem_resortRegion_max_5</td>\n",
       "      <td>mem_resortType_median_8</td>\n",
       "      <td>mem_resortType_count_2</td>\n",
       "      <td>mem_resortType_max_4</td>\n",
       "      <td>mem_staResidence_median_5</td>\n",
       "      <td>mem_staResidence_count_13</td>\n",
       "      <td>mem_staResidence_max_5</td>\n",
       "      <td>mem_stResort_median_9</td>\n",
       "      <td>mem_stResort_count_2</td>\n",
       "      <td>mem_stResort_max_5</td>\n",
       "      <td>res_staResidence_median_4</td>\n",
       "      <td>res_staResidence_count_398</td>\n",
       "      <td>res_staResidence_max_43</td>\n",
       "      <td>res_stResort_median_1</td>\n",
       "      <td>res_stResort_count_31</td>\n",
       "      <td>res_stResort_max_17</td>\n",
       "      <td>res_cluster_median_1</td>\n",
       "      <td>res_cluster_count_31</td>\n",
       "      <td>res_cluster_max_17</td>\n",
       "      <td>res_resortType_median_1</td>\n",
       "      <td>res_resortType_count_31</td>\n",
       "      <td>res_resortType_max_17</td>\n",
       "      <td>dayDiff_resortRegion_count_11</td>\n",
       "      <td>dayDiff_resortType_count_35</td>\n",
       "      <td>dayDiff_staResidence_count_120</td>\n",
       "      <td>dayDiff_stResort_count_57</td>\n",
       "      <td>dayDiff_cluster_count_23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     booking_type_code    channel_code    cluster_code    main_product_code  \\\n",
       "0  booking_type_code_0  channel_code_2  cluster_code_5  main_product_code_0   \n",
       "1  booking_type_code_0  channel_code_0  cluster_code_5  main_product_code_0   \n",
       "2  booking_type_code_0  channel_code_0  cluster_code_4  main_product_code_0   \n",
       "3  booking_type_code_0  channel_code_0  cluster_code_3  main_product_code_0   \n",
       "4  booking_type_code_0  channel_code_0  cluster_code_3  main_product_code_0   \n",
       "\n",
       "     member_age_buckets    numberofadults    numberofchildren  \\\n",
       "0  member_age_buckets_5  numberofadults_1  numberofchildren_0   \n",
       "1  member_age_buckets_5  numberofadults_1  numberofchildren_0   \n",
       "2  member_age_buckets_5  numberofadults_1  numberofchildren_0   \n",
       "3  member_age_buckets_5  numberofadults_1  numberofchildren_2   \n",
       "4  member_age_buckets_5  numberofadults_1  numberofchildren_0   \n",
       "\n",
       "     persontravellingid    reservationstatusid_code    resort_id  \\\n",
       "0  persontravellingid_1  reservationstatusid_code_2  resort_id_0   \n",
       "1  persontravellingid_1  reservationstatusid_code_0  resort_id_1   \n",
       "2  persontravellingid_2  reservationstatusid_code_0  resort_id_2   \n",
       "3  persontravellingid_1  reservationstatusid_code_0  resort_id_3   \n",
       "4  persontravellingid_1  reservationstatusid_code_0  resort_id_3   \n",
       "\n",
       "     resort_region_code    resort_type_code    room_type_booked_code  \\\n",
       "0  resort_region_code_2  resort_type_code_3  room_type_booked_code_2   \n",
       "1  resort_region_code_2  resort_type_code_3  room_type_booked_code_3   \n",
       "2  resort_region_code_0  resort_type_code_5  room_type_booked_code_3   \n",
       "3  resort_region_code_1  resort_type_code_2  room_type_booked_code_2   \n",
       "4  resort_region_code_1  resort_type_code_2  room_type_booked_code_3   \n",
       "\n",
       "     roomnights    season_holidayed_code    state_code_residence  \\\n",
       "0  roomnights_2  season_holidayed_code_1  state_code_residence_6   \n",
       "1  roomnights_6  season_holidayed_code_1  state_code_residence_6   \n",
       "2  roomnights_5  season_holidayed_code_1  state_code_residence_6   \n",
       "3  roomnights_6  season_holidayed_code_1  state_code_residence_6   \n",
       "4  roomnights_6  season_holidayed_code_1  state_code_residence_6   \n",
       "\n",
       "     state_code_resort    total_pax    tr_flag    days_diff     booking_week  \\\n",
       "0  state_code_resort_2  total_pax_2  tr_flag_0  days_diff_0  booking_week_13   \n",
       "1  state_code_resort_4  total_pax_1  tr_flag_0  days_diff_4   booking_week_3   \n",
       "2  state_code_resort_0  total_pax_1  tr_flag_0  days_diff_3   booking_week_4   \n",
       "3  state_code_resort_1  total_pax_1  tr_flag_0  days_diff_4  booking_week_17   \n",
       "4  state_code_resort_1  total_pax_1  tr_flag_0  days_diff_4  booking_week_35   \n",
       "\n",
       "     booking_month    booking_year    booking_dow     checkin_week  \\\n",
       "0  booking_month_3  booking_year_4  booking_dow_3  checkin_week_13   \n",
       "1  booking_month_0  booking_year_1  booking_dow_4  checkin_week_14   \n",
       "2  booking_month_0  booking_year_1  booking_dow_2   checkin_week_4   \n",
       "3  booking_month_4  booking_year_1  booking_dow_5  checkin_week_23   \n",
       "4  booking_month_8  booking_year_1  booking_dow_2  checkin_week_50   \n",
       "\n",
       "      checkin_month    checkin_year    checkin_dow    mem_resortRegion_median  \\\n",
       "0   checkin_month_3  checkin_year_4  checkin_dow_3  mem_resortRegion_median_3   \n",
       "1   checkin_month_3  checkin_year_1  checkin_dow_5  mem_resortRegion_median_3   \n",
       "2   checkin_month_1  checkin_year_1  checkin_dow_6  mem_resortRegion_median_7   \n",
       "3   checkin_month_5  checkin_year_1  checkin_dow_3  mem_resortRegion_median_7   \n",
       "4  checkin_month_11  checkin_year_1  checkin_dow_0  mem_resortRegion_median_7   \n",
       "\n",
       "     mem_resortRegion_count    mem_resortRegion_max    mem_resortType_median  \\\n",
       "0  mem_resortRegion_count_3  mem_resortRegion_max_4  mem_resortType_median_0   \n",
       "1  mem_resortRegion_count_3  mem_resortRegion_max_4  mem_resortType_median_0   \n",
       "2  mem_resortRegion_count_3  mem_resortRegion_max_4  mem_resortType_median_6   \n",
       "3  mem_resortRegion_count_5  mem_resortRegion_max_5  mem_resortType_median_8   \n",
       "4  mem_resortRegion_count_5  mem_resortRegion_max_5  mem_resortType_median_8   \n",
       "\n",
       "     mem_resortType_count    mem_resortType_max    mem_staResidence_median  \\\n",
       "0  mem_resortType_count_2  mem_resortType_max_4  mem_staResidence_median_5   \n",
       "1  mem_resortType_count_2  mem_resortType_max_4  mem_staResidence_median_5   \n",
       "2  mem_resortType_count_1  mem_resortType_max_3  mem_staResidence_median_5   \n",
       "3  mem_resortType_count_2  mem_resortType_max_4  mem_staResidence_median_5   \n",
       "4  mem_resortType_count_2  mem_resortType_max_4  mem_staResidence_median_5   \n",
       "\n",
       "      mem_staResidence_count    mem_staResidence_max    mem_stResort_median  \\\n",
       "0  mem_staResidence_count_13  mem_staResidence_max_5  mem_stResort_median_1   \n",
       "1  mem_staResidence_count_13  mem_staResidence_max_5  mem_stResort_median_7   \n",
       "2  mem_staResidence_count_13  mem_staResidence_max_5  mem_stResort_median_7   \n",
       "3  mem_staResidence_count_13  mem_staResidence_max_5  mem_stResort_median_9   \n",
       "4  mem_staResidence_count_13  mem_staResidence_max_5  mem_stResort_median_9   \n",
       "\n",
       "     mem_stResort_count    mem_stResort_max    res_staResidence_median  \\\n",
       "0  mem_stResort_count_1  mem_stResort_max_1  res_staResidence_median_4   \n",
       "1  mem_stResort_count_1  mem_stResort_max_5  res_staResidence_median_6   \n",
       "2  mem_stResort_count_1  mem_stResort_max_4  res_staResidence_median_4   \n",
       "3  mem_stResort_count_2  mem_stResort_max_5  res_staResidence_median_4   \n",
       "4  mem_stResort_count_2  mem_stResort_max_5  res_staResidence_median_4   \n",
       "\n",
       "       res_staResidence_count     res_staResidence_max    res_stResort_median  \\\n",
       "0  res_staResidence_count_221   res_staResidence_max_9  res_stResort_median_1   \n",
       "1  res_staResidence_count_266  res_staResidence_max_19  res_stResort_median_2   \n",
       "2  res_staResidence_count_324  res_staResidence_max_15  res_stResort_median_1   \n",
       "3  res_staResidence_count_398  res_staResidence_max_43  res_stResort_median_1   \n",
       "4  res_staResidence_count_398  res_staResidence_max_43  res_stResort_median_1   \n",
       "\n",
       "      res_stResort_count     res_stResort_max    res_cluster_median  \\\n",
       "0   res_stResort_count_4   res_stResort_max_1  res_cluster_median_1   \n",
       "1   res_stResort_count_9  res_stResort_max_13  res_cluster_median_2   \n",
       "2  res_stResort_count_18   res_stResort_max_5  res_cluster_median_1   \n",
       "3  res_stResort_count_31  res_stResort_max_17  res_cluster_median_1   \n",
       "4  res_stResort_count_31  res_stResort_max_17  res_cluster_median_1   \n",
       "\n",
       "      res_cluster_count     res_cluster_max    res_resortType_median  \\\n",
       "0   res_cluster_count_4   res_cluster_max_1  res_resortType_median_1   \n",
       "1   res_cluster_count_9  res_cluster_max_13  res_resortType_median_2   \n",
       "2  res_cluster_count_18   res_cluster_max_5  res_resortType_median_1   \n",
       "3  res_cluster_count_31  res_cluster_max_17  res_resortType_median_1   \n",
       "4  res_cluster_count_31  res_cluster_max_17  res_resortType_median_1   \n",
       "\n",
       "      res_resortType_count     res_resortType_max  \\\n",
       "0   res_resortType_count_4   res_resortType_max_1   \n",
       "1   res_resortType_count_9  res_resortType_max_13   \n",
       "2  res_resortType_count_18   res_resortType_max_5   \n",
       "3  res_resortType_count_31  res_resortType_max_17   \n",
       "4  res_resortType_count_31  res_resortType_max_17   \n",
       "\n",
       "      dayDiff_resortRegion_count     dayDiff_resortType_count  \\\n",
       "0  dayDiff_resortRegion_count_15  dayDiff_resortType_count_44   \n",
       "1   dayDiff_resortRegion_count_9  dayDiff_resortType_count_29   \n",
       "2  dayDiff_resortRegion_count_13  dayDiff_resortType_count_32   \n",
       "3  dayDiff_resortRegion_count_11  dayDiff_resortType_count_35   \n",
       "4  dayDiff_resortRegion_count_11  dayDiff_resortType_count_35   \n",
       "\n",
       "       dayDiff_staResidence_count     dayDiff_stResort_count  \\\n",
       "0  dayDiff_staResidence_count_163  dayDiff_stResort_count_68   \n",
       "1  dayDiff_staResidence_count_120  dayDiff_stResort_count_41   \n",
       "2  dayDiff_staResidence_count_148  dayDiff_stResort_count_61   \n",
       "3  dayDiff_staResidence_count_120  dayDiff_stResort_count_57   \n",
       "4  dayDiff_staResidence_count_120  dayDiff_stResort_count_57   \n",
       "\n",
       "      dayDiff_cluster_count  \n",
       "0  dayDiff_cluster_count_44  \n",
       "1  dayDiff_cluster_count_26  \n",
       "2  dayDiff_cluster_count_27  \n",
       "3  dayDiff_cluster_count_23  \n",
       "4  dayDiff_cluster_count_23  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "train_test = pd.concat([train_, valid_], axis=0).reset_index(drop=True)\n",
    "W2V_CONFIG = {\"cols\": list(train_.columns),\n",
    "             \"vector_size\": 120,\n",
    "             \"window_size\":57,\n",
    "             \"epochs\": 30,\n",
    "             \"min_count\": 1,\n",
    "             \"sample\": 1e-1\n",
    "             }\n",
    "\n",
    "train_test = train_test[W2V_CONFIG['cols']]\n",
    "cols = train_test.columns\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "for col in cols:\n",
    "    new_df[col] = train_test[col].apply(lambda x: col+'_'+str(x))\n",
    "    \n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['sentence'] = new_df.apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14833"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = new_df[['sentence']]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence length:  341424\n",
      "Vocabulary corpus:  1954\n",
      "one example: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ -71.906555  ,   11.171295  ,   -0.64614105,  -23.836838  ,\n",
       "         -78.656845  ,  -43.374493  ,   60.05238   ,  -33.367443  ,\n",
       "          -3.1000988 ,   -4.952582  ,   66.52924   ,  -54.010723  ,\n",
       "         -33.317677  ,  -12.4652815 ,   63.45051   ,   90.285515  ,\n",
       "         -56.72413   ,   -8.333235  ,   69.79113   ,  -74.89561   ,\n",
       "          -9.492345  ,  -29.879686  ,   46.997128  ,  -18.557074  ,\n",
       "          46.246693  ,   -9.780729  ,   23.01675   ,    5.7164874 ,\n",
       "         -32.769516  ,    3.9074755 ,  -28.635956  ,  -10.705525  ,\n",
       "         -11.111194  ,   -1.0095211 ,  -26.227207  ,   29.903194  ,\n",
       "          23.721258  ,   -6.1944375 ,  -28.12295   ,   96.92595   ,\n",
       "         -34.835354  ,  -44.273617  ,    4.149005  ,    9.323924  ,\n",
       "          22.161764  ,   22.896936  ,   25.921717  ,   20.946913  ,\n",
       "          -2.6392713 ,  -18.64039   ,    4.074904  , -100.53755   ,\n",
       "          22.336224  ,  -50.22788   ,   -6.6328864 ,  -63.7724    ,\n",
       "         -14.197877  ,   50.100243  ,   54.36016   ,  -18.435293  ,\n",
       "          32.220924  ,   60.077293  ,   29.176502  ,   27.488394  ,\n",
       "         -21.48074   ,   12.093206  ,   31.688147  ,   49.323597  ,\n",
       "         -23.247993  ,  -23.031006  ,   15.436258  ,   72.888664  ,\n",
       "         -21.00773   ,  -24.636698  , -116.264     ,   17.702147  ,\n",
       "          18.827772  ,    4.6661925 ,  103.28116   ,  -76.20997   ,\n",
       "         -46.054703  ,  -57.70621   ,    2.8429375 ,   42.36341   ,\n",
       "          -7.0693755 , -103.43945   ,   13.736759  ,  -14.314608  ,\n",
       "         -21.725477  ,   46.26451   ,  -17.424751  ,   54.82532   ,\n",
       "         -28.689808  ,   17.8223    ,   27.73616   ,    9.514271  ,\n",
       "          -5.0204554 ,   28.817524  ,  -41.075718  ,   77.227875  ,\n",
       "          88.276     ,  -45.43335   ,    3.513176  ,   26.204357  ,\n",
       "         122.27335   ,  -24.876852  ,  -33.085438  ,  -11.048607  ,\n",
       "           9.317074  ,   24.277082  ,  -76.280174  ,   53.765606  ,\n",
       "          21.168161  ,   19.32682   ,   73.53897   ,   44.4651    ,\n",
       "          39.518288  ,   -5.568153  ,  -12.358174  ,  -41.627235  ],\n",
       "       dtype=float32), (120,))"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_sentences = list(new_df['sentence'].str.split(\" \").values)\n",
    "print(\"sentence length: \", len(all_sentences))\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(min_count=W2V_CONFIG[\"min_count\"],\n",
    "                 window=W2V_CONFIG[\"window_size\"],\n",
    "                 size=W2V_CONFIG[\"vector_size\"],\n",
    "                 sample=W2V_CONFIG[\"sample\"],\n",
    "                 workers=4)\n",
    "w2v_model.build_vocab(all_sentences, progress_per=10000)\n",
    "\n",
    "print(\"Vocabulary corpus: \", len(w2v_model.wv.vocab))\n",
    "w2v_model.train(all_sentences, total_examples=w2v_model.corpus_count, \n",
    "                epochs=W2V_CONFIG['epochs'], \n",
    "                report_delay=1)\n",
    "\n",
    "print(\"one example: \")\n",
    "w2v_model.wv.get_vector('cluster_code_5'), w2v_model.wv.get_vector('cluster_code_5').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras tokenizers:   1954\n",
      "Vocabulary corpus:  1954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   6,   19,   22,   80,   98,   14,    9,  154,  314,  688,   67,\n",
       "         105,   16,  196,   20,  195,  147,   30,    1,   69,  499,  197,\n",
       "          51,   96,  503,  157,   47,  116,   64,  118,  265,  328,   87,\n",
       "         284,  244,  890,   50,  313,   29,  364,    7,  979,  717,    2,\n",
       "         689,  690,    3,  691,  692,    4,  693,  694,  321,  399,  614,\n",
       "         490,  242],\n",
       "       [   6,   11,   22,   80,   98,   14,    9,  154,    5,  598,   67,\n",
       "         105,   46,  330,   20,  195,  150,   27,    1,  407,  572,  149,\n",
       "          53,  102,  496,  157,   54,   93,   64,  118,  265,  328,   87,\n",
       "         284,  244,  890,   50,  101,   29,  306,  108,  984,  287,  189,\n",
       "         599,  600,  190,  601,  602,  191,  603,  604,  811,  844, 1030,\n",
       "         971,  723],\n",
       "       [   6,   11,   76,   80,   98,   14,    9,  152,    5,  390,   15,\n",
       "         192,   46,   78,   20,  195,   77,   27,    1,  148,  373,  149,\n",
       "          53,   89,  489,  156,   54,   88,  104,  118,  265,  103,   33,\n",
       "          58,  244,  890,   50,  101,   29,   60,    7,  995,  348,    2,\n",
       "         391,  203,    3,  392,  204,    4,  393,  205,  322,  794,  775,\n",
       "         636,  637],\n",
       "       [   6,   11,   85,   80,   98,   14,  145,  154,    5,  138,   18,\n",
       "          57,   16,  330,   20,  195,   86,   27,    1,  407,  447,  182,\n",
       "          53,  106,  511,  176,   54,  116,  104,  285,   68,  312,   87,\n",
       "         284,  244,  890,   50,  308,  117,  306,    7,  761,  762,    2,\n",
       "         139,  140,    3,  141,  142,    4,  143,  144,  679,  719, 1030,\n",
       "         743,  744],\n",
       "       [   6,   11,   85,   80,   98,   14,    9,  154,    5,  138,   18,\n",
       "          57,   46,  330,   20,  195,   86,   27,    1,  407,  450,  162,\n",
       "          53,   89,  485,  161,   54,  112,  104,  285,   68,  312,   87,\n",
       "         284,  244,  890,   50,  308,  117,  306,    7,  761,  762,    2,\n",
       "         139,  140,    3,  141,  142,    4,  143,  144,  679,  719, 1030,\n",
       "         743,  744]])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "\n",
    "## Tokenize the sentences\n",
    "max_features = 15000\n",
    "tokenizer = Tokenizer(num_words=max_features, filters=\"\", lower=False)\n",
    "tokenizer.fit_on_texts(list(new_df.sentence.values))\n",
    "train_test_X = tokenizer.texts_to_sequences(new_df.sentence)\n",
    "\n",
    "print(\"keras tokenizers:  \", len(tokenizer.word_index))\n",
    "print(\"Vocabulary corpus: \", len(w2v_model.wv.vocab))\n",
    "\n",
    "np.array(train_test_X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  1954\n"
     ]
    }
   ],
   "source": [
    "max_features = min(len(tokenizer.word_counts), max_features)\n",
    "print(\"vocabulary size: \", max_features)\n",
    "\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, 120))\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    embedding_matrix[idx] = w2v_model.wv.get_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalMaxPool1D, GlobalAveragePooling1D, Concatenate, Dropout\n",
    "from keras.regularizers import l2, l1, l1_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 57)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 57, 120)      234600      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 120)          0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 120)          0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 240)          0           global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 240)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 100)          24100       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 100)          0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 32)           3232        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 32)           0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1)            33          dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 261,965\n",
      "Trainable params: 27,365\n",
      "Non-trainable params: 234,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Model\n",
    "embed_size = 120\n",
    "\n",
    "inp = Input(shape=(len(train_test_X[0]),))\n",
    "x = Embedding(max_features+1, embed_size, \n",
    "              weights=[embedding_matrix], trainable=False)(inp)\n",
    "x1 = GlobalMaxPool1D()(x)\n",
    "x2 = GlobalAveragePooling1D()(x)\n",
    "\n",
    "x = Concatenate(axis=-1)([x1,x2])\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(100, activation='relu', activity_regularizer=l2(1e-4))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(32, activation='relu', activity_regularizer=l2(1e-4))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(1)(x)\n",
    "\n",
    "model = Model(inp, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdc = StandardScaler()\n",
    "y_tr1 = stdc.fit_transform(y_tr.values.reshape(-1,1))\n",
    "y_val1 = stdc.transform(y_val.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256068, 57), (85356, 57))"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_X = np.array(train_test_X)\n",
    "train_x_, valid_x_ = train_test_X[:train_.shape[0],:], train_test_X[train_.shape[0]:,:]\n",
    "train_x_.shape, valid_x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle.\n",
    "            \n",
    "        step_size: number of training iterations per half cycle. \n",
    "            Authors suggest setting step_size 2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}. (Default 'triangular')\n",
    "            \"triangular\": A basic triangular cycle w/ no amplitude scaling.\n",
    "            \"triangular2\": A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "            \"exp_range\": A cycle that scales initial amplitude by gamma**(cycle iterations) at each cycle iteration. \n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function: gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0. mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}. \n",
    "        The amplitude of the cycle can be scaled on a per-iteration or per-cycle basis.\n",
    "            Defines whether scale_fn is evaluated on cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    # Example\n",
    "        clr = CyclicLR(base_lr=0.001, max_lr=0.006,step_size=2000., mode='triangular')\n",
    "        model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        \n",
    "    Class also supports custom scaling functions:\n",
    "        # sinusoidal learning rate\n",
    "        clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "        # exp sinusoidal behaviour\n",
    "        clr_fn = lambda x: 1/(5**(x*0.0001))\n",
    "\n",
    "        clr = CyclicLR(base_lr=0.001, max_lr=0.006, step_size=2000., scale_fn=clr_fn, scale_mode='cycle')\n",
    "        model.fit(X_train, Y_train, callbacks=[clr])\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, Y_train, callbacks=[clr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256068 samples, validate on 85356 samples\n",
      "Epoch 1/10\n",
      "256068/256068 [==============================] - 11s 41us/step - loss: 200.7153 - val_loss: 0.9937\n",
      "Epoch 2/10\n",
      "256068/256068 [==============================] - 9s 34us/step - loss: 1.1178 - val_loss: 0.9936\n",
      "Epoch 3/10\n",
      "256068/256068 [==============================] - 9s 34us/step - loss: 1.0334 - val_loss: 0.9937\n",
      "Epoch 4/10\n",
      "256068/256068 [==============================] - 9s 35us/step - loss: 1.0116 - val_loss: 0.9937\n",
      "Epoch 5/10\n",
      "256068/256068 [==============================] - 9s 33us/step - loss: 1.0054 - val_loss: 0.9937\n",
      "Epoch 6/10\n",
      "256068/256068 [==============================] - 8s 33us/step - loss: 1.0045 - val_loss: 0.9938\n",
      "Epoch 7/10\n",
      "256068/256068 [==============================] - 9s 34us/step - loss: 1.0012 - val_loss: 0.9939\n",
      "Epoch 8/10\n",
      "256068/256068 [==============================] - 9s 35us/step - loss: 1.0019 - val_loss: 0.9942\n",
      "Epoch 9/10\n",
      "256068/256068 [==============================] - 9s 35us/step - loss: 1.0002 - val_loss: 0.9937\n",
      "Epoch 10/10\n",
      "256068/256068 [==============================] - 10s 38us/step - loss: 1.0003 - val_loss: 0.9938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbcd54812e8>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "clr = CyclicLR(base_lr=0.001, max_lr=0.006, step_size=2000., scale_fn=clr_fn, scale_mode='cycle')\n",
    "\n",
    "model.compile('adam',loss='mean_squared_error')\n",
    "model.fit(train_x_, y_tr1, epochs=10, batch_size=120, validation_data=(valid_x_, y_val1), callbacks=[clr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbcd6399c50>]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.history.history['loss'],'-p')\n",
    "plt.plot(model.history.history['val_loss'],'-p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01156774, 0.01156774, 0.01156774, ..., 0.01156774, 0.01156774,\n",
       "       0.01156774], dtype=float32)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(train_x_).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_1, valid_1, train_test_X, train_x_, valid_x_, embedding_matrix\n",
    "del new_df, w2v_model, tokenizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_121\"; OpenJDK Runtime Environment (Zulu 8.20.0.5-linux64) (build 1.8.0_121-b15); OpenJDK 64-Bit Server VM (Zulu 8.20.0.5-linux64) (build 25.121-b15, mixed mode)\n",
      "  Starting server from /home/ankish/anaconda3/h2o_jar/h2o.jar\n",
      "  Ice root: /tmp/tmpk4b_xv35\n",
      "  JVM stdout: /tmp/tmpk4b_xv35/h2o_ankish_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpk4b_xv35/h2o_ankish_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n",
      "Warning: Your H2O cluster version is too old (1 year, 1 month and 28 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>03 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Asia/Kolkata</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.18.0.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>1 year, 1 month and 28 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_ankish_bz2ak0</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>2.590 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.8 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         03 secs\n",
       "H2O cluster timezone:       Asia/Kolkata\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.18.0.2\n",
       "H2O cluster version age:    1 year, 1 month and 28 days !!!\n",
       "H2O cluster name:           H2O_from_python_ankish_bz2ak0\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    2.590 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.8 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "h2o.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train1 = train_.astype('object')\n",
    "# valid1 = valid_.astype('object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "(256068, 58) (85356, 58)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = h2o.H2OFrame(pd.concat([train1, y_tr], axis=1))                     \n",
    "test  = h2o.H2OFrame(pd.concat([valid1, y_val], axis=1))                     \n",
    "\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "x = list(train_.columns)\n",
    "y = \"amount_spent_per_room_night_scaled\"\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_params = {\n",
    "#     'nfolds'              :  3,\n",
    "#     'ignored_columns'     : drop_cols,\n",
    "    'ntrees'              : 200,\n",
    "    'max_depth'           : 4,# (default=20)\n",
    "    'min_rows'            : None, # Specify the minimum number of observations for a leaf\n",
    "    'nbins'               :  63, # Specify the number of bins for the histogram to build, then split at the best point.\n",
    "#     'nbins_cats'          : None # (Extensively tuning needed)\n",
    "#     'verbose'             : True,\n",
    "    'seed'                : 1234,\n",
    "    'learn_rate'          : 0.01,\n",
    "#     'learn_rate_annealing': 0.99, # (danger as it reduce the lr_rate rapidly) (for lr:0.01, use lr:0.05, with anealing of 0.99, lead to better converger (fast))\n",
    "    'distribution'        :'gaussian', # Classification: binomial(binary), quasibinomial(binary), multinomial(Categorical) and for numeric: poisson, laplace, tweedie, gaussian, huber, gamma, quantile\n",
    "    'sample_rate'         : 0.7, # default 0.63 (samples without replacement)\n",
    "#     'sample_rate_per_class':0.7, # sample from the full dataset using a per-class-specific sampling rate rather than a global sample factor\n",
    "    'col_sample_rate'     : 0.7, # sampling without replacement\n",
    "#     'col_sample_rate_per_tree':0.7, # sample without replacement.\n",
    "    'histogram_type'      : 'AUTO', # [AUTO, UniformAdaptive, Random ==> (Extremely Randomized Trees), QuantilesGlobal, RoundRobin]\n",
    "#     'fold_column'         : None, # col name for cv fold\n",
    "#     'weights_column'      : col_name, # which should be present in the dataframe as an indiaction to weights of each row.\n",
    "#     'fold_assignment'     : 'Random', # (used only is fold_column is not specified) [Random, Modulo, Stratified]\n",
    "#     'balance_classes'     : True, # only for classification (balance the classes by oversampling),\n",
    "\n",
    "    'min_split_improvement' : 1e-5, # need extensive tuning (the minimum relative improvement in squared error reduction in order for a split to happen. When properly tuned, this option can help reduce overfitting. Optimal values would be in the 1e-10…1e-3 range.)\n",
    "    'categorical_encoding'  : 'eigen', #[AUTO, enum, enum_limited, one_hot_explicit, binary, eigen, label_encoder, sort_by_response (Reorders the levels by the mean response)]\n",
    "#     'keep_cross_validation_predictions':True,\n",
    "    'score_each_iteration'  : True, # scoring at each iteration\n",
    "    'score_tree_interval'   : 5, # score after each 5 tree built\n",
    "    'stopping_rounds'       : 25, # wait for n(25) itrs for early stopping\n",
    "    'stopping_metric'       : 'rmse', # [deviance, logloss, mse, rmse, mae, rmsle, auc, misclassification, mean_per_class_error]\n",
    "    'stopping_tolerance'    : 0.001, # tolerance factor for wait till stopping\n",
    "#     'max_after_balance_size': 1,# (0-inf) for oversampling choose > 1, else < 1.\n",
    "#     'class_sampling_factors': 1, # ration of over/under-sampling rate. By default, these ratios are automatically computed during training to obtain the class balance. Note that this requires balance_classes=true.\n",
    "#     'quantile_alpha'        : 0.01, # when distribution is quantile. (Specify the quantile to be used for Quantile Regression.)\n",
    "#     'huber_alpha'           : 0.001, # Huber/M-regression (the threshold between quadratic and linear loss)\n",
    "#     'max_abs_leafnode_pred' : None, # (only for clf), it reduce overfitting by limiting the maximum absolute value of a leaf node prediction\n",
    "#     'pred_noise_bandwidth'  : 0 # The bandwidth (sigma) of Gaussian multiplicative noise ~N(1,sigma) for tree node predictions. If this parameter is specified with a value greater than 0, then every leaf node prediction is randomly scaled by a number drawn from a Normal distribution centered around 1 with a bandwidth given by this parameter\n",
    "#     'nbins_top_level'       : None # Specify the minimum number of bins at the root level to use to build the histogram. This number will then be decreased by a factor of two per level.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.009754306062416"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = list(train.columns)\n",
    "y = \"amount_spent_per_room_night_scaled\"\n",
    "x.remove(y)\n",
    "\n",
    "gbm_model = H2OGradientBoostingEstimator(**gbm_params)\n",
    "gbm_model.train(x, y, training_frame=train, validation_frame=test)\n",
    "gbm_model.rmse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0097543060623764"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gbm_model.hit_ratio_table(valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drf_model_deep = H2ORandomForestEstimator( \n",
    "nfolds=3, seed=1234,\n",
    "keep_cross_validation_predictions=True,\n",
    "fold_assignment = 'stratified',\n",
    "histogram_type = 'QuantilesGlobal',\n",
    "categorical_encoding = 'eigen',\n",
    "stopping_metric = 'auc',\n",
    "ntrees = 100,\n",
    "balance_classes = True\n",
    ")\n",
    "\n",
    "def get_drf_details():\n",
    "    \"\"\"\n",
    "    'nfolds': 3,\n",
    "    'keep_cross_validation_predictions':True,\n",
    "    'score_each_iteration' : True, # scoring at each iteration\n",
    "    'score_tree_interval'  : 5, # score after each 5 tree built\n",
    "    'fold_assignment'      : 'Random', # (used only is fold_column is not specified) [Random, Modulo, Stratified]\n",
    "    'fold_column'          : None, # col name for cv fold\n",
    "    'ignored_columns'      : drop_cols,\n",
    "    'balance_classes'      : True, # only for classification (balance the classes by oversampling),\n",
    "    'max_after_balance_size':1,# (0-inf) for oversampling choose > 1, else < 1.\n",
    "    'ntrees'               : 200,\n",
    "    'max_depth'            : 10,# (default=20)\n",
    "    'min_rows'             : None, # Specify the minimum number of observations for a leaf\n",
    "    'nbins'                : 63, # Specify the number of bins for the histogram to build, then split at the best point.\n",
    "    'nbins_top_level'      : None # Specify the minimum number of bins at the root level to use to build the histogram. This number will then be decreased by a factor of two per level.\n",
    "    # 'nbins_cats': # (Extensively tuning needed)\n",
    "    'stopping_rounds'      : 25, # wait for n(25) itrs for early stopping\n",
    "    'stopping_metric'      : 'auc', # [deviance, logloss, mse, rmse, mae, rmsle, auc, misclassification, mean_per_class_error]\n",
    "    'stopping_tolerance'   : 0.001, # tolerance factor for wait till stopping\n",
    "    'seed'                 : 1234,\n",
    "    'categorical_encoding' : 'AUTO', #[AUTO, enum, enum_limited, one_hot_explicit, binary, eigen, label_encoder, sort_by_response (Reorders the levels by the mean response)]\n",
    "    'verbose'              : 25,\n",
    "    'histogram_type'       : 'AUTO', # [AUTO, UniformAdaptive, Random ==> (Extremely Randomized Trees), QuantilesGlobal, RoundRobin]\n",
    "    'col_sample_rate_per_tree':0.7, # sample without replacement.\n",
    "    'min_split_improvement': 1e-5, # need extensive tuning (the minimum relative improvement in squared error reduction in order for a split to happen. When properly tuned, this option can help reduce overfitting. Optimal values would be in the 1e-10…1e-3 range.)\n",
    "    'sample_rate'          : 0.7, # default 0.63 (samples without replacement)\n",
    "    'sample_rate_per_class': 0.7, # sample from the full dataset using a per-class-specific sampling rate rather than a global sample factor\n",
    "    'binomial_double_trees': True, # (Binary classification only) Build twice as many trees (one per class). Enabling this option can lead to higher accuracy, while disabling can result in faster model building.\n",
    "    'mtries'               : -1, # Specify the columns to randomly select at each level. If the default value of -1 is used, the number of variables is the square root of the number of columns for classification and p/3 for regression (where p is the number of predictors). The range is -1 to >=1.\n",
    "    'class_sampling_factors':1, # ration of over/under-sampling rate. By default, these ratios are automatically computed during training to obtain the class balance. Note that this requires balance_classes=true.\n",
    "    'weights_column':col_name, # which should be present in the dataframe as an indiaction to weights of each row.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_params = {\n",
    "    'max_depth'           : 4,# (default=20)\n",
    "    'min_rows'            : None, # Specify the minimum number of observations for a leaf\n",
    "    'sample_rate'         : 0.7, # default 0.63 (samples without replacement)\n",
    "    'col_sample_rate'     : 0.7, # sampling without replacement\n",
    "    \n",
    "    'ntrees'              : 200,\n",
    "    'nbins'               :  63, # Specify the number of bins for the histogram to build, then split at the best point.\n",
    "    'seed'                : 1234,\n",
    "    'learn_rate'          : 0.01,\n",
    "    'distribution'        :'gaussian', # Classification: binomial(binary), quasibinomial(binary), multinomial(Categorical) and for numeric: poisson, laplace, tweedie, gaussian, huber, gamma, quantile\n",
    "    'histogram_type'      : 'AUTO', # [AUTO, UniformAdaptive, Random ==> (Extremely Randomized Trees), QuantilesGlobal, RoundRobin]\n",
    "    'min_split_improvement' : 1e-5, # need extensive tuning (the minimum relative improvement in squared error reduction in order for a split to happen. When properly tuned, this option can help reduce overfitting. Optimal values would be in the 1e-10…1e-3 range.)\n",
    "    'categorical_encoding'  : 'AUTO', #[AUTO, enum, enum_limited, one_hot_explicit, binary, eigen, label_encoder, sort_by_response (Reorders the levels by the mean response)]\n",
    "    'score_each_iteration'  : True, # scoring at each iteration\n",
    "    'score_tree_interval'   : 5, # score after each 5 tree built\n",
    "    'stopping_rounds'       : 25, # wait for n(25) itrs for early stopping\n",
    "    'stopping_metric'       : 'rmse', # [deviance, logloss, mse, rmse, mae, rmsle, auc, misclassification, mean_per_class_error]\n",
    "    'stopping_tolerance'    : 0.001, # tolerance factor for wait till stopping\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def bayesian_opt_gbm(X_train, X_valid):\n",
    "        \n",
    "    def train_gbm_model(min_rows, sample, depth, col_sample, min_split):\n",
    "     \n",
    "        params = {}\n",
    "\n",
    "        params['min_rows']        = max(min_rows, 0.3)        \n",
    "        params['max_depth']       = int(depth)\n",
    "        params['sample_rate']     = max(sample, 0.3)\n",
    "        params['col_sample_rate'] = col_sample\n",
    "        params['min_split_improvement'] = min_split\n",
    "\n",
    "        param_const = {\n",
    "            'ntrees'               : 5000,\n",
    "            'nbins'                :  63, # Specify the number of bins for the histogram to build, then split at the best point.\n",
    "            'seed'                 : 1234,\n",
    "            'learn_rate'           : 0.01,\n",
    "            'distribution'         :'gaussian', # Classification: binomial(binary), quasibinomial(binary), multinomial(Categorical) and for numeric: poisson, laplace, tweedie, gaussian, huber, gamma, quantile\n",
    "            'histogram_type'       : 'AUTO', # [AUTO, UniformAdaptive, Random ==> (Extremely Randomized Trees), QuantilesGlobal, RoundRobin]\n",
    "#             'min_split_improvement': 1e-5, # (1e-10…1e-3) need extensive tuning (the minimum relative improvement in squared error reduction in order for a split to happen. When properly tuned, this option can help reduce overfitting. Optimal values would be in the 1e-10…1e-3 range.)\n",
    "            'categorical_encoding' : 'AUTO', #[AUTO, enum, enum_limited, one_hot_explicit, binary, eigen, label_encoder, sort_by_response (Reorders the levels by the mean response)]\n",
    "            'score_each_iteration' : True, # scoring at each iteration\n",
    "            'score_tree_interval'  : 2, # score after each 5 tree built\n",
    "            'stopping_rounds'      : 5, # wait for n(25) itrs for early stopping\n",
    "            'stopping_metric'      : 'rmse', # [deviance, logloss, mse, rmse, mae, rmsle, auc, misclassification, mean_per_class_error]\n",
    "            'stopping_tolerance'   : 0.01, # tolerance factor for wait till stopping\n",
    "        }\n",
    "\n",
    "        for key, item in param_const.items():\n",
    "            params[key] = item\n",
    "    \n",
    "        \n",
    "\n",
    "        gbm_model = H2OGradientBoostingEstimator(**params)\n",
    "        gbm_model.train(x, y, training_frame=X_train, validation_frame=X_valid)\n",
    "        score = gbm_model.rmse()\n",
    "\n",
    "        return -score\n",
    "\n",
    "    _bo = BayesianOptimization(train_gbm_model, {\n",
    "\n",
    "        'min_rows'   : (1,9),\n",
    "        'sample'     : (0.3,0.8),\n",
    "        'depth'      : (3,15), # int\n",
    "        'col_sample' : (0.3,0.8),\n",
    "        'min_split'  : (1e-9,1e-3),\n",
    "\n",
    "    }, random_state=23456)\n",
    "    _bo.maximize(init_points=50, n_iter=40, acq='ei')\n",
    "    \n",
    "    return _bo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.no_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | col_sa... |   depth   | min_rows  | min_split |  sample   |\n",
      "-------------------------------------------------------------------------------------\n",
      "|  1        | -1.078    |  0.4609   |  6.928    |  8.419    |  0.000311 |  0.381    |\n",
      "|  2        | -1.074    |  0.482    |  9.351    |  7.313    |  0.000875 |  0.6183   |\n",
      "|  3        | -1.069    |  0.7949   |  12.79    |  1.33     |  0.000202 |  0.3762   |\n",
      "|  4        | -1.075    |  0.6765   |  8.462    |  8.637    |  0.000937 |  0.3677   |\n",
      "|  5        | -1.078    |  0.418    |  6.778    |  6.382    |  0.000620 |  0.6111   |\n",
      "|  6        | -1.072    |  0.7189   |  10.57    |  5.488    |  0.000986 |  0.6416   |\n",
      "|  7        | -1.067    |  0.3507   |  14.28    |  6.211    |  0.000227 |  0.5159   |\n",
      "|  8        | -1.068    |  0.495    |  12.07    |  2.141    |  9.862e-0 |  0.7189   |\n",
      "|  9        | -1.08     |  0.3544   |  5.991    |  4.807    |  0.000883 |  0.3887   |\n",
      "|  10       | -1.082    |  0.693    |  3.421    |  5.869    |  0.000308 |  0.6849   |\n",
      "|  11       | -1.08     |  0.5983   |  4.133    |  4.548    |  0.000804 |  0.6304   |\n",
      "|  12       | -1.067    |  0.4084   |  13.96    |  5.72     |  0.000835 |  0.5957   |\n",
      "|  13       | -1.083    |  0.4415   |  3.68     |  3.333    |  0.000331 |  0.7414   |\n",
      "|  14       | -1.071    |  0.7126   |  11.34    |  5.703    |  0.000243 |  0.5475   |\n",
      "|  15       | -1.067    |  0.7108   |  14.06    |  5.278    |  0.000562 |  0.3106   |\n",
      "|  16       | -1.083    |  0.4583   |  3.085    |  5.623    |  0.000578 |  0.6288   |\n",
      "|  17       | -1.076    |  0.3728   |  8.987    |  4.917    |  0.000124 |  0.5283   |\n",
      "|  18       | -1.073    |  0.3231   |  10.76    |  4.104    |  0.000566 |  0.4891   |\n",
      "|  19       | -1.074    |  0.7183   |  9.783    |  8.891    |  0.000794 |  0.5433   |\n",
      "|  20       | -1.065    |  0.3928   |  13.62    |  2.074    |  0.000880 |  0.773    |\n",
      "|  21       | -1.065    |  0.4265   |  14.08    |  7.128    |  0.000370 |  0.7423   |\n",
      "|  22       | -1.079    |  0.6973   |  5.137    |  7.252    |  0.000670 |  0.3828   |\n",
      "|  23       | -1.073    |  0.5548   |  10.08    |  7.997    |  0.000740 |  0.6452   |\n",
      "|  24       | -1.076    |  0.4629   |  8.828    |  4.861    |  0.000985 |  0.7856   |\n",
      "|  25       | -1.079    |  0.6821   |  5.198    |  2.387    |  4.032e-0 |  0.6471   |\n",
      "|  26       | -1.075    |  0.353    |  9.178    |  8.082    |  0.000960 |  0.7401   |\n",
      "|  27       | -1.072    |  0.623    |  11.73    |  8.014    |  0.000369 |  0.4252   |\n",
      "|  28       | -1.075    |  0.7805   |  8.022    |  6.247    |  0.000210 |  0.473    |\n",
      "|  29       | -1.081    |  0.376    |  4.878    |  7.226    |  0.000652 |  0.5788   |\n",
      "|  30       | -1.083    |  0.5094   |  3.873    |  3.878    |  0.000143 |  0.7802   |\n",
      "|  31       | -1.074    |  0.4957   |  9.441    |  7.598    |  0.000350 |  0.5559   |\n",
      "|  32       | -1.07     |  0.4413   |  12.99    |  7.617    |  0.000255 |  0.4526   |\n",
      "|  33       | -1.065    |  0.5495   |  13.23    |  4.866    |  0.000548 |  0.781    |\n",
      "|  34       | -1.082    |  0.6204   |  3.68     |  6.615    |  0.000948 |  0.5926   |\n",
      "|  35       | -1.08     |  0.7729   |  4.049    |  8.417    |  0.000590 |  0.4237   |\n",
      "|  36       | -1.069    |  0.5746   |  12.77    |  4.319    |  0.000859 |  0.5239   |\n",
      "|  37       | -1.077    |  0.3133   |  7.849    |  2.24     |  0.000981 |  0.5758   |\n",
      "|  38       | -1.083    |  0.3939   |  3.292    |  6.404    |  0.00084  |  0.4713   |\n",
      "|  39       | -1.074    |  0.7386   |  9.639    |  5.781    |  0.000736 |  0.3522   |\n",
      "|  40       | -1.072    |  0.7823   |  10.73    |  7.775    |  0.000757 |  0.7541   |\n",
      "|  41       | -1.067    |  0.7885   |  13.53    |  6.731    |  0.000465 |  0.5595   |\n",
      "|  42       | -1.071    |  0.4954   |  11.43    |  7.518    |  0.000149 |  0.5077   |\n",
      "|  43       | -1.068    |  0.5084   |  13.04    |  3.969    |  0.000139 |  0.3364   |\n",
      "|  44       | -1.077    |  0.6263   |  7.345    |  1.675    |  0.000826 |  0.6479   |\n",
      "|  45       | -1.082    |  0.7243   |  3.73     |  6.899    |  0.000188 |  0.5016   |\n",
      "|  46       | -1.079    |  0.3224   |  6.591    |  8.31     |  8.812e-0 |  0.5092   |\n",
      "|  47       | -1.075    |  0.6665   |  8.251    |  5.769    |  0.000798 |  0.4824   |\n",
      "|  48       | -1.073    |  0.5557   |  10.91    |  5.293    |  0.000192 |  0.6648   |\n",
      "|  49       | -1.067    |  0.3705   |  14.55    |  5.167    |  0.000932 |  0.4171   |\n",
      "|  50       | -1.074    |  0.4961   |  9.73     |  2.474    |  0.000616 |  0.4416   |\n",
      "|  51       | -1.06     |  0.7679   |  15.0     |  1.372    |  0.000781 |  0.7645   |\n",
      "|  52       | -1.06     |  0.7665   |  14.93    |  1.273    |  0.000158 |  0.7606   |\n",
      "|  53       | -1.06     |  0.7749   |  14.96    |  1.363    |  0.000143 |  0.7857   |\n",
      "|  54       | -1.061    |  0.7743   |  14.98    |  1.117    |  0.000728 |  0.6936   |\n",
      "|  55       | -1.06     |  0.7453   |  14.99    |  1.452    |  0.000145 |  0.7694   |\n",
      "|  56       | -1.06     |  0.7052   |  14.96    |  1.034    |  9.054e-0 |  0.7704   |\n",
      "|  57       | -1.06     |  0.7864   |  14.88    |  1.11     |  0.000835 |  0.7758   |\n",
      "|  58       | -1.06     |  0.751    |  14.97    |  1.186    |  0.000769 |  0.794    |\n",
      "|  59       | -1.06     |  0.7253   |  14.97    |  1.144    |  0.000197 |  0.7987   |\n",
      "|  60       | -1.06     |  0.7885   |  14.95    |  1.804    |  0.000906 |  0.789    |\n",
      "|  61       | -1.06     |  0.7873   |  14.78    |  1.239    |  0.000390 |  0.795    |\n",
      "|  62       | -1.06     |  0.7647   |  14.99    |  1.075    |  0.000660 |  0.7698   |\n",
      "|  63       | -1.06     |  0.7498   |  14.89    |  1.035    |  0.000350 |  0.7829   |\n",
      "|  64       | -1.06     |  0.7548   |  14.94    |  1.123    |  0.000664 |  0.7607   |\n",
      "|  65       | -1.06     |  0.6823   |  14.96    |  1.015    |  0.000579 |  0.7693   |\n",
      "|  66       | -1.06     |  0.6335   |  14.95    |  1.145    |  0.000243 |  0.7847   |\n",
      "|  67       | -1.06     |  0.7615   |  14.98    |  1.346    |  9.53e-05 |  0.7727   |\n",
      "|  68       | -1.06     |  0.7268   |  14.91    |  1.321    |  3.364e-0 |  0.7803   |\n",
      "|  69       | -1.061    |  0.7876   |  14.91    |  1.098    |  0.000627 |  0.7302   |\n",
      "|  70       | -1.06     |  0.7378   |  14.96    |  1.298    |  0.000298 |  0.792    |\n",
      "|  71       | -1.061    |  0.7557   |  14.93    |  1.217    |  0.000734 |  0.7299   |\n",
      "|  72       | -1.06     |  0.7721   |  14.89    |  1.071    |  0.000134 |  0.7868   |\n",
      "|  73       | -1.06     |  0.7328   |  14.97    |  1.233    |  9.865e-0 |  0.7589   |\n",
      "|  74       | -1.061    |  0.7573   |  14.99    |  1.037    |  6.497e-0 |  0.6636   |\n",
      "|  75       | -1.06     |  0.7877   |  14.96    |  1.208    |  0.000963 |  0.7407   |\n",
      "|  76       | -1.06     |  0.7243   |  14.99    |  1.644    |  0.000864 |  0.7991   |\n",
      "|  77       | -1.06     |  0.7922   |  14.9     |  1.369    |  0.000912 |  0.7816   |\n",
      "|  78       | -1.06     |  0.792    |  14.9     |  1.025    |  0.000932 |  0.739    |\n",
      "|  79       | -1.06     |  0.7781   |  14.96    |  1.53     |  0.000281 |  0.77     |\n",
      "|  80       | -1.061    |  0.7836   |  14.96    |  1.027    |  0.000971 |  0.6912   |\n",
      "|  81       | -1.06     |  0.7739   |  14.96    |  1.09     |  0.000182 |  0.7926   |\n",
      "|  82       | -1.06     |  0.7817   |  14.98    |  1.171    |  0.000975 |  0.7655   |\n",
      "|  83       | -1.06     |  0.7805   |  14.99    |  1.218    |  0.000690 |  0.7992   |\n",
      "|  84       | -1.06     |  0.7055   |  14.99    |  1.3      |  0.000837 |  0.7979   |\n",
      "|  85       | -1.06     |  0.6412   |  14.96    |  1.049    |  0.000898 |  0.7992   |\n",
      "|  86       | -1.06     |  0.791    |  14.92    |  1.182    |  0.000799 |  0.7989   |\n",
      "|  87       | -1.06     |  0.7868   |  14.87    |  1.019    |  0.000640 |  0.7758   |\n",
      "|  88       | -1.06     |  0.7597   |  14.88    |  1.096    |  5.11e-06 |  0.7719   |\n",
      "|  89       | -1.06     |  0.5336   |  14.95    |  1.074    |  0.000592 |  0.7853   |\n",
      "|  90       | -1.061    |  0.7569   |  14.99    |  1.067    |  2.107e-0 |  0.6893   |\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "bo_tuning_gbm = bayesian_opt_gbm(train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def bayesian_opt_drf(X_train, X_valid):\n",
    "        \n",
    "    def train_drf_model(min_rows, sample, depth, col_sample, min_split):\n",
    "     \n",
    "        params = {}\n",
    "\n",
    "        params['min_rows']        = max(min_rows, 0.3)        \n",
    "        params['max_depth']       = int(depth)\n",
    "        params['sample_rate']     = max(sample, 0.3)\n",
    "        params['min_split_improvement']   = min_split\n",
    "        params['col_sample_rate_per_tree'] = col_sample\n",
    "\n",
    "        param_const = {\n",
    "            'ntrees'               : 400,\n",
    "            'score_each_iteration' : True, # scoring at each iteration\n",
    "            'score_tree_interval'  : 5, # score after each 5 tree built\n",
    "#             'nbins'                : 63, # Specify the number of bins for the histogram to build, then split at the best point.\n",
    "            'stopping_rounds'      : 2, # wait for n(25) itrs for early stopping\n",
    "            'stopping_metric'      : 'rmse', # [deviance, logloss, mse, rmse, mae, rmsle, auc, misclassification, mean_per_class_error]\n",
    "            'stopping_tolerance'   : 0.01, # tolerance factor for wait till stopping\n",
    "            'seed'                 : 1234,\n",
    "            'categorical_encoding' : 'AUTO', #[AUTO, enum, enum_limited, one_hot_explicit, binary, eigen, label_encoder, sort_by_response (Reorders the levels by the mean response)]\n",
    "            'histogram_type'       : 'AUTO', # [AUTO, UniformAdaptive, Random ==> (Extremely Randomized Trees), QuantilesGlobal, RoundRobin]\n",
    "        }\n",
    "\n",
    "        for key, item in param_const.items():\n",
    "            params[key] = item\n",
    "    \n",
    "        \n",
    "\n",
    "        model = H2ORandomForestEstimator(**params)\n",
    "        model.train(x, y, training_frame=X_train, validation_frame=X_valid)\n",
    "        score = model.rmse()\n",
    "\n",
    "        return -score\n",
    "\n",
    "    _bo = BayesianOptimization(train_drf_model, {\n",
    "\n",
    "        'min_rows'   : (1,9),\n",
    "        'sample'     : (0.3,0.8),\n",
    "        'depth'      : (3,15), # int\n",
    "        'col_sample' : (0.3,0.8),\n",
    "        'min_split'  : (1e-8,1e-3),\n",
    "\n",
    "    }, random_state=23456)\n",
    "    _bo.maximize(init_points=50, n_iter=40, acq='ei')\n",
    "    \n",
    "    return _bo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | col_sa... |   depth   | min_rows  | min_split |  sample   |\n",
      "-------------------------------------------------------------------------------------\n",
      "|  1        | -1.016    |  0.4609   |  6.928    |  8.419    |  0.000311 |  0.381    |\n",
      "|  2        | -1.01     |  0.482    |  9.351    |  7.313    |  0.000875 |  0.6183   |\n",
      "|  3        | -1.012    |  0.7949   |  12.79    |  1.33     |  0.000202 |  0.3762   |\n",
      "|  4        | -1.008    |  0.6765   |  8.462    |  8.637    |  0.000937 |  0.3677   |\n",
      "|  5        | -1.021    |  0.418    |  6.778    |  6.382    |  0.000620 |  0.6111   |\n",
      "|  6        | -1.008    |  0.7189   |  10.57    |  5.488    |  0.000986 |  0.6416   |\n",
      "|  7        | -1.017    |  0.3507   |  14.28    |  6.211    |  0.000227 |  0.5159   |\n",
      "|  8        | -1.021    |  0.495    |  12.07    |  2.141    |  9.871e-0 |  0.7189   |\n",
      "|  9        | -1.024    |  0.3544   |  5.991    |  4.807    |  0.000883 |  0.3887   |\n",
      "|  10       | -1.051    |  0.693    |  3.421    |  5.869    |  0.000308 |  0.6849   |\n",
      "|  11       | -1.036    |  0.5983   |  4.133    |  4.548    |  0.000804 |  0.6304   |\n",
      "|  12       | -1.018    |  0.4084   |  13.96    |  5.72     |  0.000835 |  0.5957   |\n",
      "|  13       | -1.052    |  0.4415   |  3.68     |  3.333    |  0.000331 |  0.7414   |\n",
      "|  14       | -1.009    |  0.7126   |  11.34    |  5.703    |  0.000243 |  0.5475   |\n",
      "|  15       | -1.01     |  0.7108   |  14.06    |  5.278    |  0.000562 |  0.3106   |\n",
      "|  16       | -1.048    |  0.4583   |  3.085    |  5.623    |  0.000578 |  0.6288   |\n",
      "|  17       | -1.01     |  0.3728   |  8.987    |  4.917    |  0.000124 |  0.5283   |\n",
      "|  18       | -1.008    |  0.3231   |  10.76    |  4.104    |  0.000566 |  0.4891   |\n",
      "|  19       | -1.006    |  0.7183   |  9.783    |  8.891    |  0.000794 |  0.5433   |\n",
      "|  20       | -1.033    |  0.3928   |  13.62    |  2.074    |  0.000880 |  0.773    |\n",
      "|  21       | -1.03     |  0.4265   |  14.08    |  7.128    |  0.000370 |  0.7423   |\n",
      "|  22       | -1.022    |  0.6973   |  5.137    |  7.252    |  0.000670 |  0.3828   |\n",
      "|  23       | -1.01     |  0.5548   |  10.08    |  7.997    |  0.000740 |  0.6452   |\n",
      "|  24       | -1.015    |  0.4629   |  8.828    |  4.861    |  0.000985 |  0.7856   |\n",
      "|  25       | -1.03     |  0.6821   |  5.198    |  2.387    |  4.033e-0 |  0.6471   |\n",
      "|  26       | -1.013    |  0.353    |  9.178    |  8.082    |  0.000960 |  0.7401   |\n",
      "|  27       | -1.005    |  0.623    |  11.73    |  8.014    |  0.000369 |  0.4252   |\n",
      "|  28       | -1.007    |  0.7805   |  8.022    |  6.247    |  0.000210 |  0.473    |\n",
      "|  29       | -1.034    |  0.376    |  4.878    |  7.226    |  0.000652 |  0.5788   |\n",
      "|  30       | -1.049    |  0.5094   |  3.873    |  3.878    |  0.000143 |  0.7802   |\n",
      "|  31       | -1.008    |  0.4957   |  9.441    |  7.598    |  0.000350 |  0.5559   |\n",
      "|  32       | -1.008    |  0.4413   |  12.99    |  7.617    |  0.000255 |  0.4526   |\n",
      "|  33       | -1.029    |  0.5495   |  13.23    |  4.866    |  0.000548 |  0.781    |\n",
      "|  34       | -1.042    |  0.6204   |  3.68     |  6.615    |  0.000948 |  0.5926   |\n",
      "|  35       | -1.03     |  0.7729   |  4.049    |  8.417    |  0.000590 |  0.4237   |\n",
      "|  36       | -1.012    |  0.5746   |  12.77    |  4.319    |  0.000859 |  0.5239   |\n",
      "|  37       | -1.015    |  0.3133   |  7.849    |  2.24     |  0.000981 |  0.5758   |\n",
      "|  38       | -1.042    |  0.3939   |  3.292    |  6.404    |  0.00084  |  0.4713   |\n",
      "|  39       | -1.005    |  0.7386   |  9.639    |  5.781    |  0.000736 |  0.3522   |\n",
      "|  40       | -1.008    |  0.7823   |  10.73    |  7.775    |  0.000757 |  0.7541   |\n",
      "|  41       | -1.014    |  0.7885   |  13.53    |  6.731    |  0.000465 |  0.5595   |\n",
      "|  42       | -1.006    |  0.4954   |  11.43    |  7.518    |  0.000149 |  0.5077   |\n",
      "|  43       | -1.011    |  0.5084   |  13.04    |  3.969    |  0.000139 |  0.3364   |\n",
      "|  44       | -1.017    |  0.6263   |  7.345    |  1.675    |  0.000826 |  0.6479   |\n",
      "|  45       | -1.042    |  0.7243   |  3.73     |  6.899    |  0.000188 |  0.5016   |\n",
      "|  46       | -1.018    |  0.3224   |  6.591    |  8.31     |  8.821e-0 |  0.5092   |\n",
      "|  47       | -1.01     |  0.6665   |  8.251    |  5.769    |  0.000798 |  0.4824   |\n",
      "|  48       | -1.011    |  0.5557   |  10.91    |  5.293    |  0.000192 |  0.6648   |\n",
      "|  49       | -1.015    |  0.3705   |  14.55    |  5.167    |  0.000932 |  0.4171   |\n",
      "|  50       | -1.008    |  0.4961   |  9.73     |  2.474    |  0.000616 |  0.4416   |\n",
      "|  51       | -1.003    |  0.761    |  11.73    |  7.206    |  0.000795 |  0.3038   |\n",
      "|  52       | -1.009    |  0.3      |  10.91    |  1.0      |  1e-08    |  0.3      |\n",
      "|  53       | -1.004    |  0.7965   |  10.47    |  3.65     |  0.000191 |  0.3019   |\n",
      "|  54       | -1.008    |  0.3444   |  12.7     |  6.601    |  0.000403 |  0.301    |\n",
      "|  55       | -1.003    |  0.7993   |  11.47    |  8.38     |  0.000852 |  0.3071   |\n",
      "|  56       | -1.008    |  0.7556   |  8.466    |  1.007    |  0.000888 |  0.3191   |\n",
      "|  57       | -1.002    |  0.7746   |  11.78    |  8.079    |  0.000942 |  0.3001   |\n",
      "|  58       | -1.016    |  0.5657   |  14.89    |  1.131    |  0.000642 |  0.305    |\n",
      "|  59       | -1.005    |  0.764    |  12.43    |  8.991    |  0.000754 |  0.319    |\n",
      "|  60       | -1.006    |  0.7813   |  8.328    |  2.84     |  0.000336 |  0.3043   |\n",
      "|  61       | -1.003    |  0.7872   |  11.13    |  8.159    |  0.000731 |  0.3048   |\n",
      "|  62       | -1.004    |  0.7808   |  12.45    |  7.963    |  0.000775 |  0.3037   |\n",
      "|  63       | -1.004    |  0.7796   |  10.97    |  8.987    |  0.000512 |  0.3307   |\n",
      "|  64       | -1.004    |  0.7939   |  10.22    |  4.681    |  0.000710 |  0.3047   |\n",
      "|  65       | -1.004    |  0.7842   |  10.83    |  7.244    |  9.019e-0 |  0.3003   |\n",
      "|  66       | -1.017    |  0.3026   |  6.893    |  1.105    |  0.000498 |  0.3014   |\n",
      "|  67       | -1.005    |  0.7911   |  10.75    |  1.969    |  0.000458 |  0.3073   |\n",
      "|  68       | -1.004    |  0.7828   |  11.2     |  4.202    |  0.000721 |  0.3004   |\n",
      "|  69       | -1.005    |  0.3718   |  11.92    |  8.983    |  0.000919 |  0.3139   |\n",
      "|  70       | -1.003    |  0.7877   |  10.21    |  6.447    |  0.00023  |  0.306    |\n",
      "|  71       | -1.003    |  0.7902   |  11.69    |  7.775    |  0.000723 |  0.3013   |\n",
      "|  72       | -1.008    |  0.305    |  11.77    |  3.345    |  0.000231 |  0.301    |\n",
      "|  73       | -1.007    |  0.622    |  14.91    |  8.796    |  0.000771 |  0.3089   |\n",
      "|  74       | -1.007    |  0.7724   |  14.96    |  6.339    |  0.000751 |  0.3013   |\n",
      "|  75       | -1.005    |  0.7981   |  9.94     |  7.292    |  0.000838 |  0.3044   |\n",
      "|  76       | -1.005    |  0.7994   |  10.4     |  8.995    |  0.000411 |  0.3058   |\n",
      "|  77       | -1.005    |  0.7847   |  9.878    |  1.078    |  0.000284 |  0.3019   |\n",
      "|  78       | -1.006    |  0.7953   |  9.707    |  3.552    |  0.000651 |  0.3103   |\n",
      "|  79       | -1.005    |  0.7979   |  12.34    |  7.33     |  5.135e-0 |  0.3008   |\n",
      "|  80       | -1.004    |  0.7991   |  10.68    |  5.233    |  0.000514 |  0.303    |\n",
      "|  81       | -1.008    |  0.794    |  8.912    |  2.052    |  0.000883 |  0.3039   |\n",
      "|  82       | -1.009    |  0.3231   |  13.26    |  8.934    |  0.000325 |  0.3008   |\n",
      "|  83       | -1.004    |  0.7619   |  11.41    |  4.662    |  0.000801 |  0.3024   |\n",
      "|  84       | -1.019    |  0.7961   |  7.788    |  8.979    |  0.000248 |  0.7723   |\n",
      "|  85       | -1.005    |  0.3147   |  10.98    |  7.958    |  0.00022  |  0.3013   |\n",
      "|  86       | -1.003    |  0.7965   |  12.03    |  8.323    |  0.000784 |  0.3003   |\n",
      "|  87       | -1.008    |  0.7944   |  8.69     |  6.9      |  0.000672 |  0.3176   |\n",
      "|  88       | -1.016    |  0.7978   |  14.99    |  2.952    |  0.000873 |  0.3068   |\n",
      "|  89       | -1.002    |  0.7834   |  11.36    |  6.533    |  0.000861 |  0.3014   |\n",
      "|  90       | -1.007    |  0.3031   |  10.68    |  5.646    |  0.000187 |  0.3      |\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "bo_tuning_drf = bayesian_opt_drf(train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'target': -1.0599026936288272,\n",
       "  'params': {'col_sample': 0.7242822312930688,\n",
       "   'depth': 14.993440885191694,\n",
       "   'min_rows': 1.6443264205577606,\n",
       "   'min_split': 0.0008642254019696913,\n",
       "   'sample': 0.7990592980041672}},\n",
       " {'target': -1.0019219750991446,\n",
       "  'params': {'col_sample': 0.774610595517792,\n",
       "   'depth': 11.779096559106708,\n",
       "   'min_rows': 8.07895045786083,\n",
       "   'min_split': 0.0009420702334867748,\n",
       "   'sample': 0.30006254152724415}})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo_tuning_gbm.max, bo_tuning_drf.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def bayesian_opt_gbm(X_train, X_valid):\n",
    "        \n",
    "    def train_gbm_model(min_rows, sample, depth, col_sample, min_split):\n",
    "     \n",
    "        params = {}\n",
    "\n",
    "        params['min_rows']        = max(min_rows, 0.3)        \n",
    "        params['max_depth']       = int(depth)\n",
    "        params['sample_rate']     = max(sample, 0.3)\n",
    "        params['col_sample_rate'] = col_sample\n",
    "        params['min_split_improvement'] = min_split\n",
    "\n",
    "        param_const = {\n",
    "            'ntrees'               : 5000,\n",
    "            'nbins'                :  63, # Specify the number of bins for the histogram to build, then split at the best point.\n",
    "            'seed'                 : 1234,\n",
    "            'learn_rate'           : 0.01,\n",
    "            'distribution'         :'gaussian', # Classification: binomial(binary), quasibinomial(binary), multinomial(Categorical) and for numeric: poisson, laplace, tweedie, gaussian, huber, gamma, quantile\n",
    "            'histogram_type'       : 'AUTO', # [AUTO, UniformAdaptive, Random ==> (Extremely Randomized Trees), QuantilesGlobal, RoundRobin]\n",
    "#             'min_split_improvement': 1e-5, # (1e-10…1e-3) need extensive tuning (the minimum relative improvement in squared error reduction in order for a split to happen. When properly tuned, this option can help reduce overfitting. Optimal values would be in the 1e-10…1e-3 range.)\n",
    "            'categorical_encoding' : 'sort_by_response', #[AUTO, enum, enum_limited, one_hot_explicit, binary, eigen, label_encoder, sort_by_response (Reorders the levels by the mean response)]\n",
    "            'score_each_iteration' : True, # scoring at each iteration\n",
    "            'score_tree_interval'  : 2, # score after each 5 tree built\n",
    "            'stopping_rounds'      : 5, # wait for n(25) itrs for early stopping\n",
    "            'stopping_metric'      : 'rmse', # [deviance, logloss, mse, rmse, mae, rmsle, auc, misclassification, mean_per_class_error]\n",
    "            'stopping_tolerance'   : 0.01, # tolerance factor for wait till stopping\n",
    "        }\n",
    "\n",
    "        for key, item in param_const.items():\n",
    "            params[key] = item\n",
    "    \n",
    "        \n",
    "\n",
    "        gbm_model = H2OGradientBoostingEstimator(**params)\n",
    "        gbm_model.train(x, y, training_frame=X_train, validation_frame=X_valid)\n",
    "        score = gbm_model.rmse()\n",
    "\n",
    "        return -score\n",
    "\n",
    "    _bo = BayesianOptimization(train_gbm_model, {\n",
    "\n",
    "        'min_rows'   : (1,9),\n",
    "        'sample'     : (0.3,0.8),\n",
    "        'depth'      : (3,15), # int\n",
    "        'col_sample' : (0.3,0.8),\n",
    "        'min_split'  : (1e-9,1e-3),\n",
    "\n",
    "    }, random_state=23456)\n",
    "    _bo.maximize(init_points=50, n_iter=40, acq='ei')\n",
    "    \n",
    "    return _bo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | col_sa... |   depth   | min_rows  | min_split |  sample   |\n",
      "-------------------------------------------------------------------------------------\n",
      "|  1        | -1.078    |  0.4609   |  6.928    |  8.419    |  0.000311 |  0.381    |\n",
      "|  2        | -1.074    |  0.482    |  9.351    |  7.313    |  0.000875 |  0.6183   |\n",
      "|  3        | -1.069    |  0.7949   |  12.79    |  1.33     |  0.000202 |  0.3762   |\n",
      "|  4        | -1.075    |  0.6765   |  8.462    |  8.637    |  0.000937 |  0.3677   |\n",
      "|  5        | -1.078    |  0.418    |  6.778    |  6.382    |  0.000620 |  0.6111   |\n",
      "|  6        | -1.072    |  0.7189   |  10.57    |  5.488    |  0.000986 |  0.6416   |\n",
      "|  7        | -1.067    |  0.3507   |  14.28    |  6.211    |  0.000227 |  0.5159   |\n",
      "|  8        | -1.068    |  0.495    |  12.07    |  2.141    |  9.862e-0 |  0.7189   |\n",
      "|  9        | -1.08     |  0.3544   |  5.991    |  4.807    |  0.000883 |  0.3887   |\n",
      "|  10       | -1.082    |  0.693    |  3.421    |  5.869    |  0.000308 |  0.6849   |\n",
      "|  11       | -1.08     |  0.5983   |  4.133    |  4.548    |  0.000804 |  0.6304   |\n",
      "|  12       | -1.067    |  0.4084   |  13.96    |  5.72     |  0.000835 |  0.5957   |\n",
      "|  13       | -1.083    |  0.4415   |  3.68     |  3.333    |  0.000331 |  0.7414   |\n",
      "|  14       | -1.071    |  0.7126   |  11.34    |  5.703    |  0.000243 |  0.5475   |\n",
      "|  15       | -1.067    |  0.7108   |  14.06    |  5.278    |  0.000562 |  0.3106   |\n",
      "|  16       | -1.083    |  0.4583   |  3.085    |  5.623    |  0.000578 |  0.6288   |\n",
      "|  17       | -1.076    |  0.3728   |  8.987    |  4.917    |  0.000124 |  0.5283   |\n",
      "|  18       | -1.073    |  0.3231   |  10.76    |  4.104    |  0.000566 |  0.4891   |\n",
      "|  19       | -1.074    |  0.7183   |  9.783    |  8.891    |  0.000794 |  0.5433   |\n",
      "|  20       | -1.065    |  0.3928   |  13.62    |  2.074    |  0.000880 |  0.773    |\n",
      "|  21       | -1.065    |  0.4265   |  14.08    |  7.128    |  0.000370 |  0.7423   |\n",
      "|  22       | -1.079    |  0.6973   |  5.137    |  7.252    |  0.000670 |  0.3828   |\n",
      "|  23       | -1.073    |  0.5548   |  10.08    |  7.997    |  0.000740 |  0.6452   |\n",
      "|  24       | -1.076    |  0.4629   |  8.828    |  4.861    |  0.000985 |  0.7856   |\n",
      "|  25       | -1.079    |  0.6821   |  5.198    |  2.387    |  4.032e-0 |  0.6471   |\n",
      "|  26       | -1.075    |  0.353    |  9.178    |  8.082    |  0.000960 |  0.7401   |\n",
      "|  27       | -1.072    |  0.623    |  11.73    |  8.014    |  0.000369 |  0.4252   |\n",
      "|  28       | -1.075    |  0.7805   |  8.022    |  6.247    |  0.000210 |  0.473    |\n",
      "|  29       | -1.081    |  0.376    |  4.878    |  7.226    |  0.000652 |  0.5788   |\n",
      "|  30       | -1.083    |  0.5094   |  3.873    |  3.878    |  0.000143 |  0.7802   |\n",
      "|  31       | -1.074    |  0.4957   |  9.441    |  7.598    |  0.000350 |  0.5559   |\n",
      "|  32       | -1.07     |  0.4413   |  12.99    |  7.617    |  0.000255 |  0.4526   |\n",
      "|  33       | -1.065    |  0.5495   |  13.23    |  4.866    |  0.000548 |  0.781    |\n",
      "|  34       | -1.082    |  0.6204   |  3.68     |  6.615    |  0.000948 |  0.5926   |\n",
      "|  35       | -1.08     |  0.7729   |  4.049    |  8.417    |  0.000590 |  0.4237   |\n",
      "|  36       | -1.069    |  0.5746   |  12.77    |  4.319    |  0.000859 |  0.5239   |\n",
      "|  37       | -1.077    |  0.3133   |  7.849    |  2.24     |  0.000981 |  0.5758   |\n",
      "|  38       | -1.083    |  0.3939   |  3.292    |  6.404    |  0.00084  |  0.4713   |\n",
      "|  39       | -1.074    |  0.7386   |  9.639    |  5.781    |  0.000736 |  0.3522   |\n",
      "|  40       | -1.072    |  0.7823   |  10.73    |  7.775    |  0.000757 |  0.7541   |\n",
      "|  41       | -1.067    |  0.7885   |  13.53    |  6.731    |  0.000465 |  0.5595   |\n",
      "|  42       | -1.071    |  0.4954   |  11.43    |  7.518    |  0.000149 |  0.5077   |\n",
      "|  43       | -1.068    |  0.5084   |  13.04    |  3.969    |  0.000139 |  0.3364   |\n",
      "|  44       | -1.077    |  0.6263   |  7.345    |  1.675    |  0.000826 |  0.6479   |\n",
      "|  45       | -1.082    |  0.7243   |  3.73     |  6.899    |  0.000188 |  0.5016   |\n",
      "|  46       | -1.079    |  0.3224   |  6.591    |  8.31     |  8.812e-0 |  0.5092   |\n",
      "|  47       | -1.075    |  0.6665   |  8.251    |  5.769    |  0.000798 |  0.4824   |\n",
      "|  48       | -1.073    |  0.5557   |  10.91    |  5.293    |  0.000192 |  0.6648   |\n",
      "|  49       | -1.067    |  0.3705   |  14.55    |  5.167    |  0.000932 |  0.4171   |\n",
      "|  50       | -1.074    |  0.4961   |  9.73     |  2.474    |  0.000616 |  0.4416   |\n",
      "|  51       | -1.06     |  0.7679   |  15.0     |  1.372    |  0.000781 |  0.7645   |\n",
      "|  52       | -1.06     |  0.7665   |  14.93    |  1.273    |  0.000158 |  0.7606   |\n",
      "|  53       | -1.06     |  0.7749   |  14.96    |  1.363    |  0.000143 |  0.7857   |\n",
      "|  54       | -1.061    |  0.7743   |  14.98    |  1.117    |  0.000728 |  0.6936   |\n",
      "|  55       | -1.06     |  0.7453   |  14.99    |  1.452    |  0.000145 |  0.7694   |\n",
      "|  56       | -1.06     |  0.7052   |  14.96    |  1.034    |  9.054e-0 |  0.7704   |\n",
      "|  57       | -1.06     |  0.7864   |  14.88    |  1.11     |  0.000835 |  0.7758   |\n",
      "|  58       | -1.06     |  0.751    |  14.97    |  1.186    |  0.000769 |  0.794    |\n",
      "|  59       | -1.06     |  0.7253   |  14.97    |  1.144    |  0.000197 |  0.7987   |\n",
      "|  60       | -1.06     |  0.7885   |  14.95    |  1.804    |  0.000906 |  0.789    |\n",
      "|  61       | -1.06     |  0.7873   |  14.78    |  1.239    |  0.000390 |  0.795    |\n",
      "|  62       | -1.06     |  0.7647   |  14.99    |  1.075    |  0.000660 |  0.7698   |\n",
      "|  63       | -1.06     |  0.7498   |  14.89    |  1.035    |  0.000350 |  0.7829   |\n",
      "|  64       | -1.06     |  0.7548   |  14.94    |  1.123    |  0.000664 |  0.7607   |\n",
      "|  65       | -1.06     |  0.6823   |  14.96    |  1.015    |  0.000579 |  0.7693   |\n",
      "|  66       | -1.06     |  0.6335   |  14.95    |  1.145    |  0.000243 |  0.7847   |\n",
      "|  67       | -1.06     |  0.7615   |  14.98    |  1.346    |  9.53e-05 |  0.7727   |\n",
      "|  68       | -1.06     |  0.7268   |  14.91    |  1.321    |  3.364e-0 |  0.7803   |\n",
      "|  69       | -1.061    |  0.7876   |  14.91    |  1.098    |  0.000627 |  0.7302   |\n",
      "|  70       | -1.06     |  0.7378   |  14.96    |  1.298    |  0.000298 |  0.792    |\n",
      "|  71       | -1.061    |  0.7557   |  14.93    |  1.217    |  0.000734 |  0.7299   |\n",
      "|  72       | -1.06     |  0.7721   |  14.89    |  1.071    |  0.000134 |  0.7868   |\n",
      "|  73       | -1.06     |  0.7328   |  14.97    |  1.233    |  9.865e-0 |  0.7589   |\n",
      "|  74       | -1.061    |  0.7573   |  14.99    |  1.037    |  6.497e-0 |  0.6636   |\n",
      "|  75       | -1.06     |  0.7877   |  14.96    |  1.208    |  0.000963 |  0.7407   |\n",
      "|  76       | -1.06     |  0.7243   |  14.99    |  1.644    |  0.000864 |  0.7991   |\n",
      "|  77       | -1.06     |  0.7922   |  14.9     |  1.369    |  0.000912 |  0.7816   |\n",
      "|  78       | -1.06     |  0.792    |  14.9     |  1.025    |  0.000932 |  0.739    |\n",
      "|  79       | -1.06     |  0.7781   |  14.96    |  1.53     |  0.000281 |  0.77     |\n",
      "|  80       | -1.061    |  0.7836   |  14.96    |  1.027    |  0.000971 |  0.6912   |\n",
      "|  81       | -1.06     |  0.7739   |  14.96    |  1.09     |  0.000182 |  0.7926   |\n",
      "|  82       | -1.06     |  0.7817   |  14.98    |  1.171    |  0.000975 |  0.7655   |\n",
      "|  83       | -1.06     |  0.7805   |  14.99    |  1.218    |  0.000690 |  0.7992   |\n",
      "|  84       | -1.06     |  0.7055   |  14.99    |  1.3      |  0.000837 |  0.7979   |\n",
      "|  85       | -1.06     |  0.6412   |  14.96    |  1.049    |  0.000898 |  0.7992   |\n",
      "|  86       | -1.06     |  0.791    |  14.92    |  1.182    |  0.000799 |  0.7989   |\n",
      "|  87       | -1.06     |  0.7868   |  14.87    |  1.019    |  0.000640 |  0.7758   |\n",
      "|  88       | -1.06     |  0.7597   |  14.88    |  1.096    |  5.11e-06 |  0.7719   |\n",
      "|  89       | -1.06     |  0.5336   |  14.95    |  1.074    |  0.000592 |  0.7853   |\n",
      "|  90       | -1.061    |  0.7569   |  14.99    |  1.067    |  2.107e-0 |  0.6893   |\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#     bo_tuning_gbm2 = bayesian_opt_gbm(train, test)\n",
    "# except:\n",
    "#     print(\"failed\")\n",
    "bo_tuning_gbm2 = bayesian_opt_gbm(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def bayesian_opt_drf(X_train, X_valid):\n",
    "        \n",
    "    def train_drf_model(min_rows, sample, depth, col_sample, min_split):\n",
    "     \n",
    "        params = {}\n",
    "\n",
    "        params['min_rows']        = max(min_rows, 0.3)        \n",
    "        params['max_depth']       = int(depth)\n",
    "        params['sample_rate']     = max(sample, 0.3)\n",
    "        params['min_split_improvement']   = min_split\n",
    "        params['col_sample_rate_per_tree'] = col_sample\n",
    "\n",
    "        param_const = {\n",
    "            'ntrees'               : 400,\n",
    "            'score_each_iteration' : True, # scoring at each iteration\n",
    "            'score_tree_interval'  : 5, # score after each 5 tree built\n",
    "#             'nbins'                : 63, # Specify the number of bins for the histogram to build, then split at the best point.\n",
    "            'stopping_rounds'      : 2, # wait for n(25) itrs for early stopping\n",
    "            'stopping_metric'      : 'rmse', # [deviance, logloss, mse, rmse, mae, rmsle, auc, misclassification, mean_per_class_error]\n",
    "            'stopping_tolerance'   : 0.01, # tolerance factor for wait till stopping\n",
    "            'seed'                 : 1234,\n",
    "            'categorical_encoding' : 'sort_by_response', #[AUTO, enum, enum_limited, one_hot_explicit, binary, eigen, label_encoder, sort_by_response (Reorders the levels by the mean response)]\n",
    "            'histogram_type'       : 'AUTO', # [AUTO, UniformAdaptive, Random ==> (Extremely Randomized Trees), QuantilesGlobal, RoundRobin]\n",
    "        }\n",
    "\n",
    "        for key, item in param_const.items():\n",
    "            params[key] = item\n",
    "    \n",
    "        \n",
    "\n",
    "        model = H2ORandomForestEstimator(**params)\n",
    "        model.train(x, y, training_frame=X_train, validation_frame=X_valid)\n",
    "        score = model.rmse()\n",
    "\n",
    "        return -score\n",
    "\n",
    "    _bo = BayesianOptimization(train_drf_model, {\n",
    "\n",
    "        'min_rows'   : (1,9),\n",
    "        'sample'     : (0.3,0.8),\n",
    "        'depth'      : (3,15), # int\n",
    "        'col_sample' : (0.3,0.8),\n",
    "        'min_split'  : (1e-8,1e-3),\n",
    "\n",
    "    }, random_state=23456)\n",
    "    _bo.maximize(init_points=50, n_iter=40, acq='ei')\n",
    "    \n",
    "    return _bo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | col_sa... |   depth   | min_rows  | min_split |  sample   |\n",
      "-------------------------------------------------------------------------------------\n",
      "|  1        | -1.016    |  0.4609   |  6.928    |  8.419    |  0.000311 |  0.381    |\n",
      "|  2        | -1.01     |  0.482    |  9.351    |  7.313    |  0.000875 |  0.6183   |\n",
      "|  3        | -1.012    |  0.7949   |  12.79    |  1.33     |  0.000202 |  0.3762   |\n",
      "|  4        | -1.008    |  0.6765   |  8.462    |  8.637    |  0.000937 |  0.3677   |\n",
      "|  5        | -1.021    |  0.418    |  6.778    |  6.382    |  0.000620 |  0.6111   |\n",
      "|  6        | -1.008    |  0.7189   |  10.57    |  5.488    |  0.000986 |  0.6416   |\n",
      "|  7        | -1.017    |  0.3507   |  14.28    |  6.211    |  0.000227 |  0.5159   |\n",
      "|  8        | -1.021    |  0.495    |  12.07    |  2.141    |  9.871e-0 |  0.7189   |\n",
      "|  9        | -1.024    |  0.3544   |  5.991    |  4.807    |  0.000883 |  0.3887   |\n",
      "|  10       | -1.051    |  0.693    |  3.421    |  5.869    |  0.000308 |  0.6849   |\n",
      "|  11       | -1.036    |  0.5983   |  4.133    |  4.548    |  0.000804 |  0.6304   |\n",
      "|  12       | -1.018    |  0.4084   |  13.96    |  5.72     |  0.000835 |  0.5957   |\n",
      "|  13       | -1.052    |  0.4415   |  3.68     |  3.333    |  0.000331 |  0.7414   |\n",
      "|  14       | -1.009    |  0.7126   |  11.34    |  5.703    |  0.000243 |  0.5475   |\n",
      "|  15       | -1.01     |  0.7108   |  14.06    |  5.278    |  0.000562 |  0.3106   |\n",
      "|  16       | -1.048    |  0.4583   |  3.085    |  5.623    |  0.000578 |  0.6288   |\n",
      "|  17       | -1.01     |  0.3728   |  8.987    |  4.917    |  0.000124 |  0.5283   |\n",
      "|  18       | -1.008    |  0.3231   |  10.76    |  4.104    |  0.000566 |  0.4891   |\n",
      "|  19       | -1.006    |  0.7183   |  9.783    |  8.891    |  0.000794 |  0.5433   |\n",
      "|  20       | -1.033    |  0.3928   |  13.62    |  2.074    |  0.000880 |  0.773    |\n",
      "|  21       | -1.03     |  0.4265   |  14.08    |  7.128    |  0.000370 |  0.7423   |\n",
      "|  22       | -1.022    |  0.6973   |  5.137    |  7.252    |  0.000670 |  0.3828   |\n",
      "|  23       | -1.01     |  0.5548   |  10.08    |  7.997    |  0.000740 |  0.6452   |\n",
      "|  24       | -1.015    |  0.4629   |  8.828    |  4.861    |  0.000985 |  0.7856   |\n",
      "|  25       | -1.03     |  0.6821   |  5.198    |  2.387    |  4.033e-0 |  0.6471   |\n",
      "|  26       | -1.013    |  0.353    |  9.178    |  8.082    |  0.000960 |  0.7401   |\n",
      "|  27       | -1.005    |  0.623    |  11.73    |  8.014    |  0.000369 |  0.4252   |\n",
      "|  28       | -1.007    |  0.7805   |  8.022    |  6.247    |  0.000210 |  0.473    |\n",
      "|  29       | -1.034    |  0.376    |  4.878    |  7.226    |  0.000652 |  0.5788   |\n",
      "|  30       | -1.049    |  0.5094   |  3.873    |  3.878    |  0.000143 |  0.7802   |\n",
      "|  31       | -1.008    |  0.4957   |  9.441    |  7.598    |  0.000350 |  0.5559   |\n",
      "|  32       | -1.008    |  0.4413   |  12.99    |  7.617    |  0.000255 |  0.4526   |\n",
      "|  33       | -1.029    |  0.5495   |  13.23    |  4.866    |  0.000548 |  0.781    |\n",
      "|  34       | -1.042    |  0.6204   |  3.68     |  6.615    |  0.000948 |  0.5926   |\n",
      "|  35       | -1.03     |  0.7729   |  4.049    |  8.417    |  0.000590 |  0.4237   |\n",
      "|  36       | -1.012    |  0.5746   |  12.77    |  4.319    |  0.000859 |  0.5239   |\n",
      "|  37       | -1.015    |  0.3133   |  7.849    |  2.24     |  0.000981 |  0.5758   |\n",
      "|  38       | -1.042    |  0.3939   |  3.292    |  6.404    |  0.00084  |  0.4713   |\n",
      "|  39       | -1.005    |  0.7386   |  9.639    |  5.781    |  0.000736 |  0.3522   |\n",
      "|  40       | -1.008    |  0.7823   |  10.73    |  7.775    |  0.000757 |  0.7541   |\n",
      "|  41       | -1.014    |  0.7885   |  13.53    |  6.731    |  0.000465 |  0.5595   |\n",
      "|  42       | -1.006    |  0.4954   |  11.43    |  7.518    |  0.000149 |  0.5077   |\n",
      "|  43       | -1.011    |  0.5084   |  13.04    |  3.969    |  0.000139 |  0.3364   |\n",
      "|  44       | -1.017    |  0.6263   |  7.345    |  1.675    |  0.000826 |  0.6479   |\n",
      "|  45       | -1.042    |  0.7243   |  3.73     |  6.899    |  0.000188 |  0.5016   |\n",
      "|  46       | -1.018    |  0.3224   |  6.591    |  8.31     |  8.821e-0 |  0.5092   |\n",
      "|  47       | -1.01     |  0.6665   |  8.251    |  5.769    |  0.000798 |  0.4824   |\n",
      "|  48       | -1.011    |  0.5557   |  10.91    |  5.293    |  0.000192 |  0.6648   |\n",
      "|  49       | -1.015    |  0.3705   |  14.55    |  5.167    |  0.000932 |  0.4171   |\n",
      "|  50       | -1.008    |  0.4961   |  9.73     |  2.474    |  0.000616 |  0.4416   |\n",
      "|  51       | -1.003    |  0.761    |  11.73    |  7.206    |  0.000795 |  0.3038   |\n",
      "|  52       | -1.009    |  0.3      |  10.91    |  1.0      |  1e-08    |  0.3      |\n",
      "|  53       | -1.004    |  0.7965   |  10.47    |  3.65     |  0.000191 |  0.3019   |\n",
      "|  54       | -1.008    |  0.3444   |  12.7     |  6.601    |  0.000403 |  0.301    |\n",
      "|  55       | -1.003    |  0.7993   |  11.47    |  8.38     |  0.000852 |  0.3071   |\n",
      "|  56       | -1.008    |  0.7556   |  8.466    |  1.007    |  0.000888 |  0.3191   |\n",
      "|  57       | -1.002    |  0.7746   |  11.78    |  8.079    |  0.000942 |  0.3001   |\n",
      "|  58       | -1.016    |  0.5657   |  14.89    |  1.131    |  0.000642 |  0.305    |\n",
      "|  59       | -1.005    |  0.764    |  12.43    |  8.991    |  0.000754 |  0.319    |\n",
      "|  60       | -1.006    |  0.7813   |  8.328    |  2.84     |  0.000336 |  0.3043   |\n",
      "|  61       | -1.003    |  0.7872   |  11.13    |  8.159    |  0.000731 |  0.3048   |\n",
      "|  62       | -1.004    |  0.7808   |  12.45    |  7.963    |  0.000775 |  0.3037   |\n",
      "|  63       | -1.004    |  0.7796   |  10.97    |  8.987    |  0.000512 |  0.3307   |\n",
      "|  64       | -1.004    |  0.7939   |  10.22    |  4.681    |  0.000710 |  0.3047   |\n",
      "|  65       | -1.004    |  0.7842   |  10.83    |  7.244    |  9.019e-0 |  0.3003   |\n",
      "|  66       | -1.017    |  0.3026   |  6.893    |  1.105    |  0.000498 |  0.3014   |\n",
      "|  67       | -1.005    |  0.7911   |  10.75    |  1.969    |  0.000458 |  0.3073   |\n",
      "|  68       | -1.004    |  0.7828   |  11.2     |  4.202    |  0.000721 |  0.3004   |\n",
      "|  69       | -1.005    |  0.3718   |  11.92    |  8.983    |  0.000919 |  0.3139   |\n",
      "|  70       | -1.003    |  0.7877   |  10.21    |  6.447    |  0.00023  |  0.306    |\n",
      "|  71       | -1.003    |  0.7902   |  11.69    |  7.775    |  0.000723 |  0.3013   |\n",
      "|  72       | -1.008    |  0.305    |  11.77    |  3.345    |  0.000231 |  0.301    |\n",
      "|  73       | -1.007    |  0.622    |  14.91    |  8.796    |  0.000771 |  0.3089   |\n",
      "|  74       | -1.007    |  0.7724   |  14.96    |  6.339    |  0.000751 |  0.3013   |\n",
      "|  75       | -1.005    |  0.7981   |  9.94     |  7.292    |  0.000838 |  0.3044   |\n",
      "|  76       | -1.005    |  0.7994   |  10.4     |  8.995    |  0.000411 |  0.3058   |\n",
      "|  77       | -1.005    |  0.7847   |  9.878    |  1.078    |  0.000284 |  0.3019   |\n",
      "|  78       | -1.006    |  0.7953   |  9.707    |  3.552    |  0.000651 |  0.3103   |\n",
      "|  79       | -1.005    |  0.7979   |  12.34    |  7.33     |  5.135e-0 |  0.3008   |\n",
      "|  80       | -1.004    |  0.7991   |  10.68    |  5.233    |  0.000514 |  0.303    |\n",
      "|  81       | -1.008    |  0.794    |  8.912    |  2.052    |  0.000883 |  0.3039   |\n",
      "|  82       | -1.009    |  0.3231   |  13.26    |  8.934    |  0.000325 |  0.3008   |\n",
      "|  83       | -1.004    |  0.7619   |  11.41    |  4.662    |  0.000801 |  0.3024   |\n",
      "|  84       | -1.019    |  0.7961   |  7.788    |  8.979    |  0.000248 |  0.7723   |\n",
      "|  85       | -1.005    |  0.3147   |  10.98    |  7.958    |  0.00022  |  0.3013   |\n",
      "|  86       | -1.003    |  0.7965   |  12.03    |  8.323    |  0.000784 |  0.3003   |\n",
      "|  87       | -1.008    |  0.7944   |  8.69     |  6.9      |  0.000672 |  0.3176   |\n",
      "|  88       | -1.016    |  0.7978   |  14.99    |  2.952    |  0.000873 |  0.3068   |\n",
      "|  89       | -1.002    |  0.7834   |  11.36    |  6.533    |  0.000861 |  0.3014   |\n",
      "|  90       | -1.007    |  0.3031   |  10.68    |  5.646    |  0.000187 |  0.3      |\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#     bo_tuning_drf2 = bayesian_opt_drf(train, test)\n",
    "# except:\n",
    "#     print(\"failed\")\n",
    "bo_tuning_drf2 = bayesian_opt_drf(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'target': -1.0599026936288272,\n",
       "  'params': {'col_sample': 0.7242822312930688,\n",
       "   'depth': 14.993440885191694,\n",
       "   'min_rows': 1.6443264205577606,\n",
       "   'min_split': 0.0008642254019696913,\n",
       "   'sample': 0.7990592980041672}},\n",
       " {'target': -1.0019219750991446,\n",
       "  'params': {'col_sample': 0.774610595517792,\n",
       "   'depth': 11.779096559106708,\n",
       "   'min_rows': 8.07895045786083,\n",
       "   'min_split': 0.0009420702334867748,\n",
       "   'sample': 0.30006254152724415}})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo_tuning_gbm2.max, bo_tuning_drf2.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] in <ipython-input-79-02ff3806b010> line 1:\n",
      "    >>> h2o.shutdown(prompt=False)\n",
      "        ^^^^ Deprecated, use ``h2o.cluster().shutdown()``.\n",
      "H2O session _sid_934b closed.\n"
     ]
    }
   ],
   "source": [
    "h2o.shutdown(prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def bayesian_opt_cat(X_train, y_train, X_valid, y_valid, features):\n",
    "        \n",
    "    def train_cat_model(r_str, b_temp, l2, depth):\n",
    "    \n",
    "        params = {}\n",
    "\n",
    "        params['random_strength']     = max(min(r_str, 1), 0)\n",
    "        params['bagging_temperature'] = max(b_temp, 0)\n",
    "        \n",
    "        params['l2_leaf_reg'] = max(l2, 0)\n",
    "#         params['min_data_in_leaf'] = max(data_leaf,1)\n",
    "        \n",
    "#         params['subsample'] = subsample\n",
    "        params['depth']     = int(depth)\n",
    "\n",
    "        param_const = {\n",
    "            'border_count'          : 63,\n",
    "            'early_stopping_rounds' : 50,\n",
    "            'random_seed'           : 1337,\n",
    "            'task_type'             : 'CPU', \n",
    "            'loss_function'         : \"RMSE\", \n",
    "    #         'subsample'             = 0.7, \n",
    "            'iterations'            : 10000, \n",
    "            'learning_rate'         : 0.01,\n",
    "            'thread_count'          : 4,\n",
    "#             'bootstrap_type'        : 'No'\n",
    "        }\n",
    "\n",
    "        for key, item in param_const.items():\n",
    "            params[key] = item\n",
    "    \n",
    "        \n",
    "\n",
    "        _train = Pool(X_train[features], label=y_train)#, cat_features=cate_features_index)\n",
    "        _valid = Pool(X_valid[features], label=y_valid)#, cat_features=cate_features_index)\n",
    "\n",
    "        watchlist = [_train, _valid]\n",
    "        clf = CatBoostRegressor(**params)\n",
    "        clf.fit(_train, \n",
    "                eval_set=watchlist, \n",
    "                verbose=0,\n",
    "                use_best_model=True)\n",
    "\n",
    "        oof  = clf.predict(X_valid[features])\n",
    "\n",
    "        score = mean_squared_error(y_valid, oof)\n",
    "\n",
    "        return -score\n",
    "\n",
    "    _bo = BayesianOptimization(train_cat_model, {\n",
    "\n",
    "        'r_str'      : (1, 5),\n",
    "        'b_temp'     : (0.01, 100),\n",
    "        'depth'      : (3,8), # int\n",
    "#         'subsample'  : (0.3, 0.8),\n",
    "#         'data_leaf'  : (2,7),\n",
    "        'l2'         : (0, 5),\n",
    "\n",
    "    }, random_state=23456)\n",
    "    _bo.maximize(init_points=50, n_iter=35, acq='ei')\n",
    "    \n",
    "    return _bo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  b_temp   |   depth   |    l2     |   r_str   |\n",
      "-------------------------------------------------------------------------\n",
      "|  1        | -0.9559   |  32.19    |  4.637    |  4.637    |  2.247    |\n",
      "|  2        | -0.9538   |  16.2     |  4.82     |  2.646    |  4.156    |\n",
      "|  3        | -1.183    |  87.54    |  6.183    |  4.949    |  4.264    |\n",
      "|  4        | -0.9494   |  4.141    |  4.015    |  0.762    |  4.012    |\n",
      "|  5        | -1.183    |  45.52    |  7.773    |  4.687    |  1.542    |\n",
      "|  6        | -0.9549   |  23.6     |  4.574    |  3.363    |  3.482    |\n",
      "|  7        | -1.183    |  62.22    |  7.189    |  3.154    |  3.244    |\n",
      "|  8        | -1.183    |  98.69    |  6.416    |  0.5066   |  4.76     |\n",
      "|  9        | -1.183    |  65.14    |  4.137    |  2.159    |  2.56     |\n",
      "|  10       | -1.183    |  75.58    |  3.713    |  0.04931  |  4.352    |\n",
      "|  11       | -0.9532   |  10.89    |  4.246    |  2.379    |  4.536    |\n",
      "|  12       | -0.9463   |  17.76    |  6.93     |  0.1753   |  3.435    |\n",
      "|  13       | -0.9478   |  30.85    |  6.849    |  2.983    |  1.378    |\n",
      "|  14       | -1.183    |  44.35    |  7.023    |  3.304    |  1.868    |\n",
      "|  15       | -1.183    |  91.32    |  5.95     |  4.178    |  3.365    |\n",
      "|  16       | -0.9624   |  28.3     |  3.283    |  1.458    |  2.327    |\n",
      "|  17       | -1.183    |  88.28    |  7.126    |  3.473    |  3.351    |\n",
      "|  18       | -0.95     |  24.37    |  5.475    |  4.108    |  4.685    |\n",
      "|  19       | -1.183    |  53.48    |  5.814    |  0.1062   |  2.266    |\n",
      "|  20       | -0.9375   |  0.7165   |  5.889    |  2.892    |  3.63     |\n",
      "|  21       | -0.9489   |  14.57    |  5.495    |  2.448    |  1.496    |\n",
      "|  22       | -1.183    |  45.67    |  3.231    |  3.235    |  2.552    |\n",
      "|  23       | -1.183    |  56.69    |  4.891    |  4.183    |  3.261    |\n",
      "|  24       | -1.183    |  98.64    |  6.971    |  2.433    |  1.742    |\n",
      "|  25       | -1.183    |  88.52    |  3.671    |  4.405    |  4.784    |\n",
      "|  26       | -0.9463   |  25.31    |  7.616    |  3.83     |  2.481    |\n",
      "|  27       | -1.183    |  88.46    |  6.973    |  0.8904   |  4.126    |\n",
      "|  28       | -1.183    |  67.09    |  3.828    |  2.548    |  3.36     |\n",
      "|  29       | -1.183    |  87.47    |  6.703    |  3.452    |  2.303    |\n",
      "|  30       | -1.183    |  48.57    |  5.413    |  4.928    |  4.885    |\n",
      "|  31       | -1.183    |  76.42    |  3.916    |  0.8667   |  1.161    |\n",
      "|  32       | -1.183    |  69.42    |  3.53     |  2.574    |  4.541    |\n",
      "|  33       | -1.183    |  96.02    |  7.401    |  3.23     |  3.91     |\n",
      "|  34       | -1.183    |  87.67    |  4.846    |  1.252    |  4.844    |\n",
      "|  35       | -1.183    |  41.86    |  6.279    |  1.05     |  2.384    |\n",
      "|  36       | -0.9609   |  15.21    |  3.783    |  3.891    |  3.611    |\n",
      "|  37       | -1.183    |  55.76    |  5.094    |  0.3636   |  2.439    |\n",
      "|  38       | -0.9443   |  14.31    |  7.802    |  1.957    |  3.147    |\n",
      "|  39       | -1.183    |  82.47    |  4.755    |  2.559    |  2.13     |\n",
      "|  40       | -1.183    |  83.22    |  7.136    |  1.276    |  2.221    |\n",
      "|  41       | -1.183    |  49.91    |  7.262    |  2.416    |  3.194    |\n",
      "|  42       | -1.183    |  96.2     |  6.204    |  0.2832   |  3.807    |\n",
      "|  43       | -1.183    |  94.82    |  5.926    |  4.729    |  1.35     |\n",
      "|  44       | -1.183    |  92.72    |  5.953    |  1.237    |  3.197    |\n",
      "|  45       | -1.183    |  81.4     |  5.074    |  4.297    |  2.791    |\n",
      "|  46       | -0.9431   |  2.674    |  5.02     |  0.7751   |  4.924    |\n",
      "|  47       | -1.183    |  55.16    |  3.939    |  0.1216   |  3.702    |\n",
      "|  48       | -1.183    |  84.0     |  4.713    |  4.386    |  3.213    |\n",
      "|  49       | -1.183    |  59.77    |  6.68     |  0.5222   |  4.858    |\n",
      "|  50       | -1.183    |  64.41    |  7.234    |  3.785    |  4.633    |\n",
      "|  51       | -0.9367   |  0.1705   |  7.955    |  1.039    |  1.099    |\n",
      "|  52       | -0.9362   |  0.2514   |  7.864    |  4.547    |  4.979    |\n",
      "|  53       | -0.9501   |  28.69    |  5.92     |  4.807    |  1.017    |\n",
      "|  54       | -0.9368   |  0.05702  |  7.947    |  1.17     |  4.972    |\n",
      "|  55       | -0.9467   |  28.92    |  7.892    |  4.18     |  4.656    |\n",
      "|  56       | -0.9406   |  0.3137   |  3.147    |  4.697    |  4.84     |\n",
      "|  57       | -0.9428   |  7.476    |  7.889    |  0.1696   |  1.052    |\n",
      "|  58       | -0.9367   |  0.2203   |  7.896    |  3.861    |  1.08     |\n",
      "|  59       | -0.9376   |  0.1123   |  6.659    |  2.999    |  4.796    |\n",
      "|  60       | -0.9406   |  0.2554   |  3.775    |  4.87     |  1.027    |\n",
      "|  61       | -0.942    |  3.964    |  7.744    |  4.494    |  4.979    |\n",
      "|  62       | -0.937    |  0.04535  |  6.484    |  4.723    |  3.162    |\n",
      "|  63       | -0.9459   |  25.55    |  7.982    |  0.9279   |  1.092    |\n",
      "|  64       | -0.9405   |  0.02066  |  3.519    |  0.4149   |  1.892    |\n",
      "|  65       | -0.9364   |  0.04893  |  7.879    |  3.326    |  3.658    |\n",
      "|  66       | -0.9366   |  0.02496  |  7.038    |  2.363    |  1.081    |\n",
      "|  67       | -0.9451   |  14.82    |  7.911    |  0.204    |  1.249    |\n",
      "|  68       | -0.9427   |  6.897    |  7.967    |  4.749    |  1.027    |\n",
      "|  69       | -0.9372   |  0.0729   |  7.497    |  0.1591   |  2.177    |\n",
      "|  70       | -0.9404   |  2.058    |  8.0      |  2.921    |  1.388    |\n",
      "|  71       | -0.9459   |  23.07    |  7.988    |  1.219    |  4.903    |\n",
      "|  72       | -0.9364   |  0.05285  |  7.905    |  4.616    |  3.856    |\n",
      "|  73       | -0.9377   |  0.03899  |  6.363    |  0.1433   |  4.821    |\n",
      "|  74       | -0.9371   |  0.01653  |  7.799    |  4.157    |  4.667    |\n",
      "|  75       | -0.9369   |  0.01718  |  7.411    |  4.86     |  4.705    |\n",
      "|  76       | -0.9371   |  0.01039  |  7.062    |  1.861    |  3.087    |\n",
      "|  77       | -0.9369   |  0.05109  |  7.6      |  4.162    |  1.022    |\n",
      "|  78       | -0.9369   |  0.1792   |  7.928    |  0.1557   |  1.015    |\n",
      "|  79       | -0.9375   |  0.1185   |  6.858    |  0.2514   |  1.033    |\n",
      "|  80       | -0.937    |  0.0671   |  7.963    |  0.05822  |  3.398    |\n",
      "|  81       | -0.937    |  0.02109  |  7.733    |  2.295    |  1.641    |\n",
      "|  82       | -0.9431   |  8.797    |  7.948    |  0.03786  |  4.741    |\n",
      "|  83       | -0.9368   |  0.03635  |  6.959    |  4.809    |  1.31     |\n",
      "|  84       | -0.9366   |  0.06494  |  7.86     |  4.798    |  2.377    |\n",
      "|  85       | -0.938    |  0.01462  |  5.767    |  3.116    |  4.99     |\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "bo_tuning_cat = bayesian_opt_cat(train_, y_tr, valid_, y_val, list(train_.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def bayesian_opt_lgb(X_train, y_train, X_valid, y_valid, features):\n",
    "        \n",
    "    def train_lgb_model(f_frac, b_frac, \n",
    "                        l1, l2, split_gain,\n",
    "                        leaves, data_in_leaf, hessian):\n",
    "    \n",
    "        param = {}\n",
    "\n",
    "        param['feature_fraction'] = max(min(f_frac, 1), 0)\n",
    "        param['bagging_fraction'] = max(min(b_frac, 1), 0)\n",
    "\n",
    "        param['lambda_l1'] = max(l1, 0)\n",
    "        param['lambda_l2'] = max(l2, 0)\n",
    "        param['min_split_gain'] = split_gain\n",
    "#     #     params['min_child_weight'] = min_child_weight\n",
    "\n",
    "        param['num_leaves'] = int(leaves)\n",
    "        param['min_data_in_leaf'] = int(data_in_leaf)\n",
    "        param['min_sum_hessian_in_leaf'] = max(hessian, 0)\n",
    "\n",
    "        param_const = {\n",
    "            'max_bins'               : 63,\n",
    "            'learning_rate'          : 0.01,\n",
    "            'num_threads'            : 4,\n",
    "            'metric'                 : 'rmse',\n",
    "            'boost'                  : 'gbdt',\n",
    "            'tree_learner'           : 'serial',\n",
    "            'objective'              : 'root_mean_squared_error',\n",
    "            'verbosity'              : 0,\n",
    "        }\n",
    "\n",
    "        for key, item in param_const.items():\n",
    "            param[key] = item\n",
    "    \n",
    "#         print(param)\n",
    "\n",
    "        _train = lgb.Dataset(X_train[features], label=y_train, feature_name=list(features))\n",
    "        _valid = lgb.Dataset(X_valid[features], label=y_valid,feature_name=list(features))\n",
    "\n",
    "        clf = lgb.train(param, _train, 10000, \n",
    "                        valid_sets = [_train, _valid], \n",
    "                        verbose_eval=0, \n",
    "                        early_stopping_rounds = 25)                  \n",
    "\n",
    "        oof = clf.predict(X_valid[features], num_iteration=clf.best_iteration)\n",
    "        score = mean_squared_error(y_valid, oof)\n",
    "\n",
    "        return -score\n",
    "\n",
    "\n",
    "    _bo = BayesianOptimization(train_lgb_model, {\n",
    " \n",
    "        'b_frac'       : (0.35, 0.7),\n",
    "        'f_frac'       : (0.35, 0.8),\n",
    "        'leaves'       : (40,90), # int\n",
    "        'split_gain'   : (0, 2),\n",
    "        'l1'           : (0, 3),\n",
    "        'l2'           : (0, 3),\n",
    "        'data_in_leaf' : (20, 1000), # int\n",
    "        'hessian'      : (0, 20),\n",
    "\n",
    "\n",
    "    }, random_state=23456)\n",
    "    _bo.maximize(init_points=40, n_iter=35, acq='ei')\n",
    "    \n",
    "    return _bo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  b_frac   | data_i... |  f_frac   |  hessian  |    l1     |    l2     |  leaves   | split_... |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "|  1        | -0.9387   |  0.4626   |  340.8    |  0.7673   |  6.234    |  0.4859   |  1.092    |  66.46    |  1.578    |\n",
      "|  2        | -0.9386   |  0.6564   |  643.9    |  0.7954   |  16.32    |  0.1239   |  0.6088   |  47.62    |  1.506    |\n",
      "|  3        | -0.9384   |  0.5093   |  955.5    |  0.7718   |  2.708    |  0.7077   |  0.9445   |  73.63    |  1.241    |\n",
      "|  4        | -0.9393   |  0.5677   |  841.1    |  0.6339   |  11.22    |  2.961    |  2.049    |  45.07    |  1.88     |\n",
      "|  5        | -0.9387   |  0.578    |  242.9    |  0.5443   |  7.802    |  2.267    |  0.4279   |  40.49    |  1.676    |\n",
      "|  6        | -0.9377   |  0.3881   |  264.2    |  0.5642   |  17.68    |  0.5325   |  2.358    |  41.75    |  1.217    |\n",
      "|  7        | -0.9368   |  0.4579   |  774.5    |  0.6185   |  1.889    |  1.33     |  2.414    |  73.04    |  0.4338   |\n",
      "|  8        | -0.9372   |  0.6696   |  598.2    |  0.726    |  11.83    |  0.8488   |  0.1699   |  54.58    |  0.6635   |\n",
      "|  9        | -0.9392   |  0.659    |  828.8    |  0.6626   |  11.76    |  0.7308   |  1.485    |  81.08    |  1.843    |\n",
      "|  10       | -0.9378   |  0.5372   |  571.6    |  0.3596   |  6.332    |  0.0212   |  1.733    |  68.92    |  1.315    |\n",
      "|  11       | -0.9369   |  0.401    |  508.9    |  0.5703   |  2.479    |  1.37     |  0.1387   |  72.35    |  0.7759   |\n",
      "|  12       | -0.9373   |  0.5484   |  390.6    |  0.7265   |  11.3     |  2.959    |  2.382    |  64.33    |  0.3712   |\n",
      "|  13       | -0.9373   |  0.6598   |  151.5    |  0.7464   |  18.92    |  0.7591   |  2.77     |  78.3     |  0.7405   |\n",
      "|  14       | -0.9378   |  0.6596   |  798.7    |  0.4301   |  15.63    |  2.013    |  0.4968   |  65.48    |  1.18     |\n",
      "|  15       | -0.9393   |  0.6561   |  745.9    |  0.6607   |  6.516    |  1.457    |  1.448    |  89.28    |  1.942    |\n",
      "|  16       | -0.9382   |  0.6175   |  199.5    |  0.428    |  0.8064   |  2.083    |  0.3178   |  65.74    |  1.77     |\n",
      "|  17       | -0.9394   |  0.6861   |  882.5    |  0.6407   |  14.55    |  2.63     |  1.108    |  52.52    |  1.922    |\n",
      "|  18       | -0.9381   |  0.4965   |  662.8    |  0.4445   |  6.918    |  0.4559   |  0.4696   |  78.91    |  1.305    |\n",
      "|  19       | -0.9373   |  0.5451   |  430.3    |  0.3827   |  7.194    |  0.4292   |  2.881    |  59.57    |  1.073    |\n",
      "|  20       | -0.937    |  0.6386   |  363.9    |  0.5803   |  5.652    |  2.497    |  2.481    |  52.76    |  0.6104   |\n",
      "|  21       | -0.9385   |  0.5247   |  855.3    |  0.5675   |  10.97    |  2.886    |  1.922    |  42.83    |  1.404    |\n",
      "|  22       | -0.9379   |  0.6819   |  593.6    |  0.7756   |  1.749    |  2.781    |  1.772    |  52.37    |  1.098    |\n",
      "|  23       | -0.9392   |  0.6349   |  426.5    |  0.7367   |  8.955    |  0.07992  |  1.212    |  47.75    |  1.962    |\n",
      "|  24       | -0.9371   |  0.5431   |  204.1    |  0.3609   |  13.51    |  2.52     |  1.028    |  83.86    |  1.107    |\n",
      "|  25       | -0.9389   |  0.5592   |  741.3    |  0.397    |  19.29    |  1.932    |  2.541    |  77.85    |  1.816    |\n"
     ]
    }
   ],
   "source": [
    "bo_tuning_lgb = bayesian_opt_lgb(train_, y_tr, valid_, y_val, list(train_.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'target': -0.9362267064104927,\n",
    " 'params': {'b_frac': 0.3677516358370858,\n",
    "  'data_in_leaf': 495.44417416221626,\n",
    "  'f_frac': 0.5422060360159515,\n",
    "  'hessian': 5.039378213231793,\n",
    "  'l1': 1.0642598045225304,\n",
    "  'l2': 2.564544963055539,\n",
    "  'leaves': 89.62655396835916,\n",
    "  'split_gain': 0.16542750189034394}}\n",
    "\n",
    "bo_tuning_cat.max\n",
    "\n",
    "{'target': -0.9366723276780314,\n",
    " 'params': {'b_temp': 0.08307474720468191,\n",
    "  'depth': 7.596402546589758,\n",
    "  'l2': 3.9791105400066655,\n",
    "  'r_str': 1.1206250787323229}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import h2o\n",
    "h2o.init()\n",
    "\n",
    "train = h2o.H2OFrame(pd.concat([train1, y_tr], axis=1))                     \n",
    "test  = h2o.H2OFrame(pd.concat([valid1, y_val], axis=1))                     \n",
    "\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "x = list(train_.columns)\n",
    "y = \"amount_spent_per_room_night_scaled\"\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_tuning_cat.max, bo_tuning_lgb.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def bayesian_opt_gbm(X_train, X_valid):\n",
    "        \n",
    "    def train_gbm_model(min_rows, sample, depth, col_sample, min_split):\n",
    "     \n",
    "        params = {}\n",
    "\n",
    "        params['min_rows']        = max(min_rows, 0.3)        \n",
    "        params['max_depth']       = int(depth)\n",
    "        params['sample_rate']     = max(sample, 0.3)\n",
    "        params['col_sample_rate'] = col_sample\n",
    "        params['min_split_improvement'] = min_split\n",
    "\n",
    "        param_const = {\n",
    "            'ntrees'               : 5000,\n",
    "#             'nbins'                :  63, # Specify the number of bins for the histogram to build, then split at the best point.\n",
    "            'seed'                 : 1234,\n",
    "            'learn_rate'           : 0.01,\n",
    "            'distribution'         :'gaussian', # Classification: binomial(binary), quasibinomial(binary), multinomial(Categorical) and for numeric: poisson, laplace, tweedie, gaussian, huber, gamma, quantile\n",
    "            'histogram_type'       : 'AUTO', # [AUTO, UniformAdaptive, Random ==> (Extremely Randomized Trees), QuantilesGlobal, RoundRobin]\n",
    "#             'min_split_improvement': 1e-5, # (1e-10…1e-3) need extensive tuning (the minimum relative improvement in squared error reduction in order for a split to happen. When properly tuned, this option can help reduce overfitting. Optimal values would be in the 1e-10…1e-3 range.)\n",
    "            'categorical_encoding' : 'one_hot_explicit', #[AUTO, enum, enum_limited, one_hot_explicit, binary, eigen, label_encoder, sort_by_response (Reorders the levels by the mean response)]\n",
    "            'score_each_iteration' : True, # scoring at each iteration\n",
    "            'score_tree_interval'  : 2, # score after each 5 tree built\n",
    "            'stopping_rounds'      : 5, # wait for n(25) itrs for early stopping\n",
    "            'stopping_metric'      : 'rmse', # [deviance, logloss, mse, rmse, mae, rmsle, auc, misclassification, mean_per_class_error]\n",
    "            'stopping_tolerance'   : 0.01, # tolerance factor for wait till stopping\n",
    "        }\n",
    "\n",
    "        for key, item in param_const.items():\n",
    "            params[key] = item\n",
    "    \n",
    "        \n",
    "\n",
    "        gbm_model = H2OGradientBoostingEstimator(**params)\n",
    "        gbm_model.train(x, y, training_frame=X_train, validation_frame=X_valid)\n",
    "        score = gbm_model.rmse()\n",
    "\n",
    "        return -score\n",
    "\n",
    "    _bo = BayesianOptimization(train_gbm_model, {\n",
    "\n",
    "        'min_rows'   : (1,9),\n",
    "        'sample'     : (0.3,0.8),\n",
    "        'depth'      : (3,15), # int\n",
    "        'col_sample' : (0.3,0.8),\n",
    "        'min_split'  : (1e-9,1e-3),\n",
    "\n",
    "    }, random_state=23456)\n",
    "    _bo.maximize(init_points=50, n_iter=40, acq='ei')\n",
    "    \n",
    "    return _bo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def bayesian_opt_drf(X_train, X_valid):\n",
    "        \n",
    "    def train_drf_model(min_rows, sample, depth, col_sample, min_split):\n",
    "     \n",
    "        params = {}\n",
    "\n",
    "        params['min_rows']        = max(min_rows, 0.3)        \n",
    "        params['max_depth']       = int(depth)\n",
    "        params['sample_rate']     = max(sample, 0.3)\n",
    "        params['min_split_improvement']   = min_split\n",
    "        params['col_sample_rate_per_tree'] = col_sample\n",
    "\n",
    "        param_const = {\n",
    "            'ntrees'               : 400,\n",
    "            'score_each_iteration' : True, # scoring at each iteration\n",
    "            'score_tree_interval'  : 5, # score after each 5 tree built\n",
    "#             'nbins'                : 63, # Specify the number of bins for the histogram to build, then split at the best point.\n",
    "            'stopping_rounds'      : 2, # wait for n(25) itrs for early stopping\n",
    "            'stopping_metric'      : 'rmse', # [deviance, logloss, mse, rmse, mae, rmsle, auc, misclassification, mean_per_class_error]\n",
    "            'stopping_tolerance'   : 0.01, # tolerance factor for wait till stopping\n",
    "            'seed'                 : 1234,\n",
    "            'categorical_encoding' : 'one_hot_explicit', #[AUTO, enum, enum_limited, one_hot_explicit, binary, eigen, label_encoder, sort_by_response (Reorders the levels by the mean response)]\n",
    "            'histogram_type'       : 'AUTO', # [AUTO, UniformAdaptive, Random ==> (Extremely Randomized Trees), QuantilesGlobal, RoundRobin]\n",
    "        }\n",
    "\n",
    "        for key, item in param_const.items():\n",
    "            params[key] = item\n",
    "    \n",
    "        \n",
    "\n",
    "        model = H2ORandomForestEstimator(**params)\n",
    "        model.train(x, y, training_frame=X_train, validation_frame=X_valid)\n",
    "        score = model.rmse()\n",
    "\n",
    "        return -score\n",
    "\n",
    "    _bo = BayesianOptimization(train_drf_model, {\n",
    "\n",
    "        'min_rows'   : (1,9),\n",
    "        'sample'     : (0.3,0.8),\n",
    "        'depth'      : (3,15), # int\n",
    "        'col_sample' : (0.3,0.8),\n",
    "        'min_split'  : (1e-8,1e-3),\n",
    "\n",
    "    }, random_state=23456)\n",
    "    _bo.maximize(init_points=50, n_iter=40, acq='ei')\n",
    "    \n",
    "    return _bo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_tuning_gbm3 = bayesian_opt_gbm(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_tuning_gbm3.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_tuning_drf3 = bayesian_opt_drf(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_tuning_drf3.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, gc\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from catboost import Pool, CatBoostClassifier, CatBoostRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def train_lgb_model(X_train, y_train, X_valid, y_valid, features, param, X_test, num_round):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X_train, X_valid: training and valid data\n",
    "        y_train, y_valid: training and valid target\n",
    "        X_test: test-data\n",
    "        features: training features\n",
    "    Return:\n",
    "        oof-pred, test_preds model, model_imp\n",
    "    \"\"\"\n",
    "    _train = lgb.Dataset(X_train[features], label=y_train, feature_name=list(features))\n",
    "    _valid = lgb.Dataset(X_valid[features], label=y_valid,feature_name=list(features))\n",
    "    \n",
    "    clf = lgb.train(param, _train, num_round, \n",
    "                    valid_sets = [_train, _valid], \n",
    "                    verbose_eval=200, \n",
    "                    early_stopping_rounds = 25)                  \n",
    "    \n",
    "    oof = clf.predict(X_valid[features], num_iteration=clf.best_iteration)\n",
    "    test_pred = clf.predict(X_test[features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    lgb_imp = pd.DataFrame(data=[clf.feature_name(), list(clf.feature_importance())]).T\n",
    "    lgb_imp.columns = ['feature','imp']\n",
    "    \n",
    "    return oof, test_pred, clf, lgb_imp\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def run_cv_lgb(train_df, target, test_df, leaves=None):\n",
    "\n",
    "    param = {\n",
    "        'bagging_freq'           : 5,\n",
    "        'bagging_fraction'       : 0.33,\n",
    "        'boost_from_average'     : 'false',\n",
    "        'boost'                  : 'gbdt',\n",
    "        'feature_fraction'       : 0.3,\n",
    "        'learning_rate'          : 0.01,\n",
    "        'max_depth'              : -1,\n",
    "        'metric'                 : 'rmse',\n",
    "        'min_data_in_leaf'       : 100,\n",
    "#         'min_sum_hessian_in_leaf': 10.0,\n",
    "        'num_leaves'             : 30,\n",
    "        'num_threads'            : 4,\n",
    "        'tree_learner'           : 'serial',\n",
    "        'objective'              : 'root_mean_squared_error',\n",
    "        'verbosity'              : 1,\n",
    "    #     'lambda_l1'              : 0.001,\n",
    "        'lambda_l2'              : 0.1\n",
    "    }   \n",
    "    if leaves is not None:\n",
    "        param['num_leaves'] = leaves\n",
    "        print(\"using leaves: \", param['num_leaves'])\n",
    "\n",
    "    random_seed = 1234\n",
    "    n_splits = 3\n",
    "    num_round = 10000\n",
    "    feature_imp = pd.DataFrame()\n",
    "    \n",
    "    folds = KFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "    oof_lgb = np.zeros(len(train_df))\n",
    "    predictions = np.zeros((len(test_df),n_splits))\n",
    "\n",
    "    clfs = []\n",
    "    \n",
    "    for fold_, (train_index, valid_index) in enumerate(folds.split(train_df, target)):\n",
    "        print(train_index.shape, valid_index.shape)\n",
    "        print(\"Fold {}\".format(fold_))\n",
    "    \n",
    "        y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n",
    "        X_train, X_valid = train_df.iloc[train_index,:], train_df.iloc[valid_index,:]\n",
    "        features = X_train.columns\n",
    "        \n",
    "#         X_train.drop(['disbursal_week','disbursal_day'], axis=1, inplace=True)\n",
    "#         X_valid.drop(['disbursal_week','disbursal_day'], axis=1, inplace=True)\n",
    "\n",
    "        num_round = 10000\n",
    "        oof, test_pred, clf, lgb_imp = train_lgb_model(X_train, y_train, \n",
    "                                                       X_valid, y_valid, \n",
    "                                                       features, param, \n",
    "                                                       test_df, num_round)\n",
    "        lgb_imp['fold'] = fold_\n",
    "        feature_imp = pd.concat([feature_imp, lgb_imp], axis=0)\n",
    "    \n",
    "        oof_lgb[valid_index] = oof\n",
    "        predictions[:,fold_] = test_pred\n",
    "        clfs.append(clf)\n",
    "        \n",
    "        score = mean_squared_error(y_valid, oof)\n",
    "        print( \"  rmse = \", 100*np.sqrt(score) )\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    feature_imp.imp = feature_imp.imp.astype('float')\n",
    "    feature_imp = feature_imp.groupby(['feature'])['imp'].mean()\n",
    "    feature_imp = pd.DataFrame(data=[feature_imp.index, feature_imp.values]).T\n",
    "    feature_imp.columns=['feature','imp']\n",
    "    feature_imp = feature_imp.sort_values(by='imp')\n",
    "\n",
    "    return clfs, feature_imp, oof_lgb, predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "train_df1 = copy.deepcopy(train_df)\n",
    "test_df1 = copy.deepcopy(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1.drop(['reservation_id', 'memberid'], axis=1, inplace=True)\n",
    "test_df1.drop(['reservation_id', 'memberid'], axis=1, inplace=True)\n",
    "\n",
    "target = train_df.amount_spent_per_room_night_scaled\n",
    "train_df1.drop('amount_spent_per_room_night_scaled', axis=1, inplace=True)\n",
    "\n",
    "train_df1 = train_df1.astype('object')\n",
    "test_df1  = test_df1.astype('object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_df = pd.concat([train_df1, test_df1], axis=0).reset_index(drop=True)\n",
    "\n",
    "for col in complete_df.columns:\n",
    "    complete_df[col] = complete_df[col].astype('category').cat.codes\n",
    "    \n",
    "del train_df1, test_df1\n",
    "gc.collect()\n",
    "\n",
    "train_df1 = complete_df.iloc[:train_df.shape[0]]\n",
    "test_df1  = complete_df.iloc[train_df.shape[0]:]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using leaves:  50\n",
      "(227616,) (113808,)\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's rmse: 1.46449\tvalid_1's rmse: 1.46804\n",
      "[400]\ttraining's rmse: 1.03143\tvalid_1's rmse: 1.0345\n",
      "[600]\ttraining's rmse: 1.0144\tvalid_1's rmse: 1.01885\n",
      "[800]\ttraining's rmse: 1.00885\tvalid_1's rmse: 1.01489\n",
      "[1000]\ttraining's rmse: 1.00497\tvalid_1's rmse: 1.01259\n",
      "[1200]\ttraining's rmse: 1.00098\tvalid_1's rmse: 1.01027\n",
      "[1400]\ttraining's rmse: 0.9973\tvalid_1's rmse: 1.00813\n",
      "[1600]\ttraining's rmse: 0.992678\tvalid_1's rmse: 1.00504\n",
      "[1800]\ttraining's rmse: 0.990019\tvalid_1's rmse: 1.0038\n",
      "[2000]\ttraining's rmse: 0.986463\tvalid_1's rmse: 1.00165\n",
      "[2200]\ttraining's rmse: 0.984166\tvalid_1's rmse: 1.00078\n",
      "[2400]\ttraining's rmse: 0.981085\tvalid_1's rmse: 0.99911\n",
      "[2600]\ttraining's rmse: 0.978644\tvalid_1's rmse: 0.998053\n",
      "[2800]\ttraining's rmse: 0.97589\tvalid_1's rmse: 0.996625\n",
      "[3000]\ttraining's rmse: 0.973695\tvalid_1's rmse: 0.995726\n",
      "[3200]\ttraining's rmse: 0.9711\tvalid_1's rmse: 0.994485\n",
      "Early stopping, best iteration is:\n",
      "[3210]\ttraining's rmse: 0.971034\tvalid_1's rmse: 0.994475\n",
      "  rmse =  99.44753942014867\n",
      "============================================================\n",
      "(227616,) (113808,)\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's rmse: 1.46519\tvalid_1's rmse: 1.46417\n",
      "[400]\ttraining's rmse: 1.03128\tvalid_1's rmse: 1.03456\n",
      "[600]\ttraining's rmse: 1.01442\tvalid_1's rmse: 1.01995\n",
      "[800]\ttraining's rmse: 1.00897\tvalid_1's rmse: 1.01628\n",
      "[1000]\ttraining's rmse: 1.00472\tvalid_1's rmse: 1.01375\n",
      "[1200]\ttraining's rmse: 1.00103\tvalid_1's rmse: 1.01172\n",
      "[1400]\ttraining's rmse: 0.997395\tvalid_1's rmse: 1.00963\n",
      "[1600]\ttraining's rmse: 0.992898\tvalid_1's rmse: 1.0067\n",
      "[1800]\ttraining's rmse: 0.99003\tvalid_1's rmse: 1.00533\n",
      "[2000]\ttraining's rmse: 0.986363\tvalid_1's rmse: 1.00307\n",
      "[2200]\ttraining's rmse: 0.983957\tvalid_1's rmse: 1.00212\n",
      "[2400]\ttraining's rmse: 0.98092\tvalid_1's rmse: 1.00049\n",
      "[2600]\ttraining's rmse: 0.978431\tvalid_1's rmse: 0.999354\n",
      "[2800]\ttraining's rmse: 0.975428\tvalid_1's rmse: 0.997633\n",
      "[3000]\ttraining's rmse: 0.973423\tvalid_1's rmse: 0.996914\n",
      "[3200]\ttraining's rmse: 0.970738\tvalid_1's rmse: 0.995524\n",
      "Early stopping, best iteration is:\n",
      "[3281]\ttraining's rmse: 0.970127\tvalid_1's rmse: 0.995428\n",
      "  rmse =  99.54279057051576\n",
      "============================================================\n",
      "(227616,) (113808,)\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[200]\ttraining's rmse: 1.46463\tvalid_1's rmse: 1.46606\n",
      "[400]\ttraining's rmse: 1.03098\tvalid_1's rmse: 1.03555\n",
      "[600]\ttraining's rmse: 1.01426\tvalid_1's rmse: 1.02085\n",
      "[800]\ttraining's rmse: 1.00842\tvalid_1's rmse: 1.01674\n",
      "[1000]\ttraining's rmse: 1.0045\tvalid_1's rmse: 1.01455\n",
      "[1200]\ttraining's rmse: 1.00061\tvalid_1's rmse: 1.01237\n",
      "[1400]\ttraining's rmse: 0.996984\tvalid_1's rmse: 1.01037\n",
      "[1600]\ttraining's rmse: 0.99255\tvalid_1's rmse: 1.0075\n",
      "[1800]\ttraining's rmse: 0.989354\tvalid_1's rmse: 1.00587\n",
      "[2000]\ttraining's rmse: 0.985719\tvalid_1's rmse: 1.00366\n",
      "[2200]\ttraining's rmse: 0.983359\tvalid_1's rmse: 1.00276\n",
      "[2400]\ttraining's rmse: 0.980249\tvalid_1's rmse: 1.00107\n",
      "[2600]\ttraining's rmse: 0.977779\tvalid_1's rmse: 1.00002\n",
      "[2800]\ttraining's rmse: 0.974875\tvalid_1's rmse: 0.998407\n",
      "[3000]\ttraining's rmse: 0.972785\tvalid_1's rmse: 0.997596\n",
      "[3200]\ttraining's rmse: 0.970076\tvalid_1's rmse: 0.996178\n",
      "Early stopping, best iteration is:\n",
      "[3349]\ttraining's rmse: 0.968769\tvalid_1's rmse: 0.995815\n",
      "  rmse =  99.58148568546977\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "clfs_lgb, imp_lgb, oof_lgb, pred_lgb = run_cv_lgb(train_df1, target, \n",
    "                                                  test_df1, leaves=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "def train_cat_model(X_train, y_train, X_valid, y_valid, features, param, X_test, \n",
    "                    num_round):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X_train, X_valid: training and valid data\n",
    "        y_train, y_valid: training and valid target\n",
    "        X_test: test-data\n",
    "        features: training features\n",
    "    Return:\n",
    "        oof-pred, test_preds, model, model_imp\n",
    "    \"\"\"\n",
    "    param['iterations'] = num_round\n",
    "    \n",
    "    _train = Pool(X_train[features], label=y_train)#, cat_features=cate_features_index)\n",
    "    _valid = Pool(X_valid[features], label=y_valid)#, cat_features=cate_features_index)\n",
    "\n",
    "    watchlist = [_train, _valid]\n",
    "    clf = CatBoostRegressor(**param)\n",
    "    clf.fit(_train, \n",
    "            eval_set=watchlist, \n",
    "            verbose=200,\n",
    "            use_best_model=True)\n",
    "        \n",
    "    oof  = clf.predict(X_valid[features])\n",
    "    test_pred  = clf.predict(X_test[features])\n",
    "\n",
    "    cat_imp = pd.DataFrame(data=[clf.feature_names_, \n",
    "                                 list(clf.feature_importances_)]).T\n",
    "    cat_imp.columns = ['feature','imp']\n",
    "    \n",
    "    return oof, test_pred, clf, cat_imp\n",
    "\n",
    "\n",
    "def run_cv_cat(train_df, target, test_df, depth):\n",
    "\n",
    "    params = {\n",
    "        'loss_function'         : \"RMSE\", \n",
    "#         'eval_metric'           : \"AUC\",\n",
    "        'random_strength'       : 1.5,\n",
    "        'border_count'          : 128,\n",
    "#         'scale_pos_weight'      : 3.507,\n",
    "        'depth'                 : depth, \n",
    "        'early_stopping_rounds' : 50,\n",
    "        'random_seed'           : 1337,\n",
    "        'task_type'             : 'CPU', \n",
    "#         'subsample'             = 0.7, \n",
    "        'iterations'            : 10000, \n",
    "        'learning_rate'         : 0.09,\n",
    "        'thread_count'          : 4\n",
    "    }\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    n_splits = 3\n",
    "    random_seed = 1234\n",
    "    feature_imp = pd.DataFrame()\n",
    "    \n",
    "    folds = KFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "    oof_cat = np.zeros(len(train_df))\n",
    "    predictions = np.zeros((len(test_df),n_splits))\n",
    "    clfs = []\n",
    "##########################\n",
    "    for fold_, (train_index, valid_index) in enumerate(folds.split(train_df, target)):\n",
    "        print(train_index.shape, valid_index.shape)\n",
    "        print(\"Fold {}\".format(fold_))\n",
    "    \n",
    "        y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n",
    "        X_train, X_valid = train_df.iloc[train_index,:], train_df.iloc[valid_index,:]\n",
    "        features = X_train.columns\n",
    "        \n",
    "        num_rounds = 10000\n",
    "        oof, test_pred, clf, cat_imp = train_cat_model(X_train, y_train, \n",
    "                                                       X_valid, y_valid, \n",
    "                                                       features, params, \n",
    "                                                       test_df, num_rounds)\n",
    "    \n",
    "        oof_cat[valid_index] = oof\n",
    "        predictions[:,fold_] = test_pred\n",
    "        \n",
    "        cat_imp['fold'] = fold_\n",
    "        feature_imp = pd.concat([feature_imp, cat_imp], axis=0)\n",
    "        clfs.append(clf)\n",
    "        \n",
    "        score = mean_squared_error(y_valid, oof)\n",
    "        print( \"  auc = \", 100*np.sqrt(score) )\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    feature_imp.imp = feature_imp.imp.astype('float')\n",
    "    feature_imp = feature_imp.groupby(['feature'])['imp'].mean()\n",
    "    feature_imp = pd.DataFrame(data=[feature_imp.index, feature_imp.values]).T\n",
    "    feature_imp.columns=['feature','imp']\n",
    "    feature_imp = feature_imp.sort_values(by='imp')\n",
    "\n",
    "    return clfs, feature_imp, oof_cat, predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227616,) (113808,)\n",
      "Fold 0\n",
      "0:\tlearn: 7.1087168\ttest: 7.1087168\ttest1: 7.1130523\tbest: 7.1130523 (0)\ttotal: 20.8ms\tremaining: 3m 27s\n",
      "200:\tlearn: 1.0182283\ttest: 1.0182283\ttest1: 1.0183900\tbest: 1.0183900 (200)\ttotal: 6.07s\tremaining: 4m 56s\n",
      "400:\tlearn: 1.0051033\ttest: 1.0051033\ttest1: 1.0070166\tbest: 1.0070166 (400)\ttotal: 12.1s\tremaining: 4m 49s\n",
      "600:\tlearn: 0.9972679\ttest: 0.9972679\ttest1: 1.0008241\tbest: 1.0008216 (599)\ttotal: 18s\tremaining: 4m 41s\n",
      "800:\tlearn: 0.9920203\ttest: 0.9920203\ttest1: 0.9968622\tbest: 0.9968622 (800)\ttotal: 23.8s\tremaining: 4m 33s\n",
      "1000:\tlearn: 0.9881551\ttest: 0.9881551\ttest1: 0.9941965\tbest: 0.9941965 (1000)\ttotal: 29.6s\tremaining: 4m 26s\n",
      "1200:\tlearn: 0.9847933\ttest: 0.9847933\ttest1: 0.9921452\tbest: 0.9921452 (1200)\ttotal: 35.5s\tremaining: 4m 20s\n",
      "1400:\tlearn: 0.9824931\ttest: 0.9824931\ttest1: 0.9911621\tbest: 0.9911561 (1399)\ttotal: 41.4s\tremaining: 4m 14s\n",
      "1600:\tlearn: 0.9803233\ttest: 0.9803233\ttest1: 0.9900051\tbest: 0.9900044 (1599)\ttotal: 47.2s\tremaining: 4m 7s\n",
      "1800:\tlearn: 0.9784058\ttest: 0.9784058\ttest1: 0.9892871\tbest: 0.9892871 (1800)\ttotal: 53s\tremaining: 4m 1s\n",
      "2000:\tlearn: 0.9765982\ttest: 0.9765982\ttest1: 0.9885996\tbest: 0.9885976 (1997)\ttotal: 58.8s\tremaining: 3m 55s\n",
      "2200:\tlearn: 0.9751652\ttest: 0.9751652\ttest1: 0.9883719\tbest: 0.9883625 (2183)\ttotal: 1m 4s\tremaining: 3m 48s\n",
      "2400:\tlearn: 0.9738581\ttest: 0.9738581\ttest1: 0.9880961\tbest: 0.9880961 (2400)\ttotal: 1m 10s\tremaining: 3m 42s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.9880593117\n",
      "bestIteration = 2427\n",
      "\n",
      "Shrink model to first 2428 iterations.\n",
      "  auc =  98.80593103868149\n",
      "============================================================\n",
      "(227616,) (113808,)\n",
      "Fold 1\n",
      "0:\tlearn: 7.1110609\ttest: 7.1110609\ttest1: 7.1076075\tbest: 7.1076075 (0)\ttotal: 31.7ms\tremaining: 5m 16s\n",
      "200:\tlearn: 1.0189467\ttest: 1.0189467\ttest1: 1.0210699\tbest: 1.0210699 (200)\ttotal: 6.4s\tremaining: 5m 12s\n",
      "400:\tlearn: 1.0057477\ttest: 1.0057477\ttest1: 1.0097422\tbest: 1.0097422 (400)\ttotal: 12.5s\tremaining: 4m 59s\n",
      "600:\tlearn: 0.9973958\ttest: 0.9973958\ttest1: 1.0028644\tbest: 1.0028644 (600)\ttotal: 18.7s\tremaining: 4m 52s\n",
      "800:\tlearn: 0.9914702\ttest: 0.9914702\ttest1: 0.9983306\tbest: 0.9983306 (800)\ttotal: 24.9s\tremaining: 4m 45s\n",
      "1000:\tlearn: 0.9872964\ttest: 0.9872964\ttest1: 0.9955084\tbest: 0.9955084 (1000)\ttotal: 31.1s\tremaining: 4m 39s\n",
      "1200:\tlearn: 0.9842590\ttest: 0.9842590\ttest1: 0.9938824\tbest: 0.9938824 (1200)\ttotal: 37.3s\tremaining: 4m 33s\n",
      "1400:\tlearn: 0.9816380\ttest: 0.9816380\ttest1: 0.9924363\tbest: 0.9924320 (1398)\ttotal: 43.6s\tremaining: 4m 27s\n",
      "1600:\tlearn: 0.9794547\ttest: 0.9794547\ttest1: 0.9915221\tbest: 0.9915221 (1600)\ttotal: 50s\tremaining: 4m 22s\n",
      "1800:\tlearn: 0.9775295\ttest: 0.9775295\ttest1: 0.9908846\tbest: 0.9908846 (1800)\ttotal: 57.1s\tremaining: 4m 19s\n",
      "2000:\tlearn: 0.9757515\ttest: 0.9757515\ttest1: 0.9903230\tbest: 0.9903208 (1996)\ttotal: 1m 3s\tremaining: 4m 12s\n",
      "2200:\tlearn: 0.9742005\ttest: 0.9742005\ttest1: 0.9898959\tbest: 0.9898818 (2168)\ttotal: 1m 9s\tremaining: 4m 5s\n",
      "2400:\tlearn: 0.9728313\ttest: 0.9728313\ttest1: 0.9896550\tbest: 0.9896520 (2397)\ttotal: 1m 15s\tremaining: 3m 58s\n",
      "2600:\tlearn: 0.9715913\ttest: 0.9715913\ttest1: 0.9894769\tbest: 0.9894713 (2597)\ttotal: 1m 21s\tremaining: 3m 53s\n",
      "2800:\tlearn: 0.9702650\ttest: 0.9702650\ttest1: 0.9891203\tbest: 0.9891203 (2800)\ttotal: 1m 27s\tremaining: 3m 43s\n",
      "3000:\tlearn: 0.9691323\ttest: 0.9691323\ttest1: 0.9888510\tbest: 0.9888468 (2999)\ttotal: 1m 32s\tremaining: 3m 35s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.9887649269\n",
      "bestIteration = 3077\n",
      "\n",
      "Shrink model to first 3078 iterations.\n",
      "  auc =  98.87649254496156\n",
      "============================================================\n",
      "(227616,) (113808,)\n",
      "Fold 2\n",
      "0:\tlearn: 7.1102802\ttest: 7.1102802\ttest1: 7.1094043\tbest: 7.1094043 (0)\ttotal: 19.8ms\tremaining: 3m 17s\n",
      "200:\tlearn: 1.0178079\ttest: 1.0178079\ttest1: 1.0206242\tbest: 1.0206242 (200)\ttotal: 5.3s\tremaining: 4m 18s\n",
      "400:\tlearn: 1.0040467\ttest: 1.0040467\ttest1: 1.0081294\tbest: 1.0081294 (400)\ttotal: 11.3s\tremaining: 4m 31s\n",
      "600:\tlearn: 0.9964114\ttest: 0.9964114\ttest1: 1.0019758\tbest: 1.0019758 (600)\ttotal: 17.3s\tremaining: 4m 30s\n",
      "800:\tlearn: 0.9915294\ttest: 0.9915294\ttest1: 0.9984527\tbest: 0.9984527 (800)\ttotal: 23.1s\tremaining: 4m 25s\n",
      "1000:\tlearn: 0.9874123\ttest: 0.9874123\ttest1: 0.9956293\tbest: 0.9956293 (1000)\ttotal: 29s\tremaining: 4m 20s\n",
      "1200:\tlearn: 0.9843637\ttest: 0.9843637\ttest1: 0.9936438\tbest: 0.9936438 (1200)\ttotal: 34.7s\tremaining: 4m 14s\n",
      "1400:\tlearn: 0.9815983\ttest: 0.9815983\ttest1: 0.9919727\tbest: 0.9919727 (1400)\ttotal: 40.6s\tremaining: 4m 9s\n",
      "1600:\tlearn: 0.9795270\ttest: 0.9795270\ttest1: 0.9910646\tbest: 0.9910602 (1594)\ttotal: 46.4s\tremaining: 4m 3s\n",
      "1800:\tlearn: 0.9776755\ttest: 0.9776755\ttest1: 0.9903219\tbest: 0.9903194 (1798)\ttotal: 52.3s\tremaining: 3m 58s\n",
      "2000:\tlearn: 0.9759971\ttest: 0.9759971\ttest1: 0.9896849\tbest: 0.9896849 (2000)\ttotal: 58.1s\tremaining: 3m 52s\n",
      "2200:\tlearn: 0.9746167\ttest: 0.9746167\ttest1: 0.9893845\tbest: 0.9893745 (2189)\ttotal: 1m 3s\tremaining: 3m 46s\n",
      "2400:\tlearn: 0.9732578\ttest: 0.9732578\ttest1: 0.9890173\tbest: 0.9890134 (2370)\ttotal: 1m 9s\tremaining: 3m 40s\n",
      "2600:\tlearn: 0.9719481\ttest: 0.9719481\ttest1: 0.9886998\tbest: 0.9886998 (2600)\ttotal: 1m 15s\tremaining: 3m 35s\n",
      "2800:\tlearn: 0.9707770\ttest: 0.9707770\ttest1: 0.9884234\tbest: 0.9884219 (2796)\ttotal: 1m 21s\tremaining: 3m 29s\n",
      "3000:\tlearn: 0.9696502\ttest: 0.9696502\ttest1: 0.9882936\tbest: 0.9882849 (2970)\ttotal: 1m 27s\tremaining: 3m 23s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.9882833501\n",
      "bestIteration = 3019\n",
      "\n",
      "Shrink model to first 3020 iterations.\n",
      "  auc =  98.82833502067156\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "clfs_cat, imp_cat, oof_cat, pred_cat = run_cv_cat(train_df1, target, test_df1, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_xgb_model(X_train, y_train, X_valid, y_valid, features, param, X_test, \n",
    "                    num_round):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X_train, X_valid: training and valid data\n",
    "        y_train, y_valid: training and valid target\n",
    "        X_test: test-data\n",
    "        features: training features\n",
    "    Return:\n",
    "        oof-pred, test_preds, model, model_imp\n",
    "    \"\"\"\n",
    "    _train = xgb.DMatrix(X_train[features], label=y_train, feature_names=list(features))\n",
    "    _valid = xgb.DMatrix(X_valid[features], label=y_valid,feature_names=list(features))\n",
    "    \n",
    "    watchlist = [(_valid, 'valid')]\n",
    "    clf = xgb.train(dtrain=_train, \n",
    "                    num_boost_round=num_round, \n",
    "                    evals=watchlist,\n",
    "                    early_stopping_rounds=25, \n",
    "                    verbose_eval=200, \n",
    "                    params=param)\n",
    "    \n",
    "    valid_frame = xgb.DMatrix(X_valid[features],feature_names=list(features))\n",
    "    oof  = clf.predict(valid_frame, ntree_limit=clf.best_ntree_limit)\n",
    "\n",
    "\n",
    "    test_frame = xgb.DMatrix(X_test[features],feature_names=list(features))\n",
    "    test_pred = clf.predict(test_frame, ntree_limit=clf.best_ntree_limit)\n",
    "\n",
    "    \n",
    "    xgb_imp = pd.DataFrame(data=[list(clf.get_fscore().keys()), \n",
    "                                 list(clf.get_fscore().values())]).T\n",
    "    xgb_imp.columns = ['feature','imp']\n",
    "    xgb_imp.imp = xgb_imp.imp.astype('float')\n",
    "    \n",
    "    return oof, test_pred, clf, xgb_imp\n",
    "\n",
    "\n",
    "def run_cv_xgb(train_df, target, test_df, depth):\n",
    "\n",
    "    features = train_df.columns\n",
    "    params = {\n",
    "        'eval_metric'     : 'rmse',\n",
    "        'seed'            : 1337,\n",
    "        'eta'             : 0.05,\n",
    "        'subsample'       : 0.7,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'silent'          : 1,\n",
    "        'nthread'         : 4,\n",
    "#         'Scale_pos_weight': 3.607,\n",
    "#         'objective'       : 'reg:squarederror',\n",
    "        'max_depth'       : depth,\n",
    "        'alpha'           : 0.05\n",
    "    }\n",
    "    \n",
    "    n_splits = 3\n",
    "    random_seed = 1234\n",
    "    feature_imp = pd.DataFrame()\n",
    "    \n",
    "    folds = KFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "    oof_xgb = np.zeros(len(train_df))\n",
    "    predictions = np.zeros((len(test_df),n_splits))\n",
    "    clfs = []\n",
    "##########################\n",
    "    for fold_, (train_index, valid_index) in enumerate(folds.split(train_df, target)):\n",
    "        print(train_index.shape, valid_index.shape)\n",
    "        print(\"Fold {}\".format(fold_))\n",
    "    \n",
    "        y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n",
    "        X_train, X_valid = train_df.iloc[train_index,:], train_df.iloc[valid_index,:]\n",
    "        features = X_train.columns\n",
    "        \n",
    "\n",
    "        num_rounds = 10000\n",
    "        oof, test_pred, clf, xgb_imp = train_xgb_model(X_train, y_train, \n",
    "                                                       X_valid, y_valid, \n",
    "                                                       features, params, \n",
    "                                                       test_df, num_rounds)\n",
    "        \n",
    "        xgb_imp['fold'] = fold_\n",
    "        feature_imp = pd.concat([feature_imp, xgb_imp], axis=0)\n",
    "    \n",
    "        oof_xgb[valid_index] = oof\n",
    "        predictions[:,fold_] = test_pred\n",
    "        clfs.append(clf)\n",
    "        \n",
    "        score = mean_squared_error(y_valid, oof)\n",
    "        print( \"  auc = \", 100*np.sqrt(score) )\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    feature_imp.imp = feature_imp.imp.astype('float')\n",
    "    feature_imp = feature_imp.groupby(['feature'])['imp'].mean()\n",
    "    feature_imp = pd.DataFrame(data=[feature_imp.index, feature_imp.values]).T\n",
    "    feature_imp.columns=['feature','imp']\n",
    "    feature_imp = feature_imp.sort_values(by='imp')\n",
    "\n",
    "\n",
    "    return clfs, feature_imp, oof_xgb, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227616,) (113808,)\n",
      "Fold 0\n",
      "[0]\tvalid-rmse:6.94846\n",
      "Will train until valid-rmse hasn't improved in 25 rounds.\n",
      "[200]\tvalid-rmse:1.02016\n",
      "[400]\tvalid-rmse:1.01442\n",
      "[600]\tvalid-rmse:1.01005\n",
      "[800]\tvalid-rmse:1.00453\n",
      "[1000]\tvalid-rmse:0.999952\n",
      "[1200]\tvalid-rmse:0.996541\n",
      "[1400]\tvalid-rmse:0.994686\n",
      "[1600]\tvalid-rmse:0.992758\n",
      "[1800]\tvalid-rmse:0.991263\n",
      "[2000]\tvalid-rmse:0.990132\n",
      "[2200]\tvalid-rmse:0.989238\n",
      "[2400]\tvalid-rmse:0.988517\n",
      "[2600]\tvalid-rmse:0.987852\n",
      "Stopping. Best iteration:\n",
      "[2596]\tvalid-rmse:0.987844\n",
      "\n",
      "  auc =  98.78438239224694\n",
      "============================================================\n",
      "(227616,) (113808,)\n",
      "Fold 1\n",
      "[0]\tvalid-rmse:6.94318\n",
      "Will train until valid-rmse hasn't improved in 25 rounds.\n",
      "[200]\tvalid-rmse:1.02129\n",
      "[400]\tvalid-rmse:1.01675\n",
      "[600]\tvalid-rmse:1.01156\n",
      "[800]\tvalid-rmse:1.00631\n",
      "[1000]\tvalid-rmse:1.00209\n",
      "[1200]\tvalid-rmse:0.998696\n",
      "[1400]\tvalid-rmse:0.9965\n",
      "[1600]\tvalid-rmse:0.994389\n",
      "[1800]\tvalid-rmse:0.99262\n",
      "[2000]\tvalid-rmse:0.991479\n",
      "[2200]\tvalid-rmse:0.990637\n",
      "[2400]\tvalid-rmse:0.989865\n",
      "Stopping. Best iteration:\n",
      "[2461]\tvalid-rmse:0.989666\n",
      "\n",
      "  auc =  98.96655200822629\n",
      "============================================================\n",
      "(227616,) (113808,)\n",
      "Fold 2\n",
      "[0]\tvalid-rmse:6.94497\n",
      "Will train until valid-rmse hasn't improved in 25 rounds.\n",
      "[200]\tvalid-rmse:1.0221\n",
      "[400]\tvalid-rmse:1.0174\n",
      "[600]\tvalid-rmse:1.01219\n",
      "[800]\tvalid-rmse:1.00663\n",
      "[1000]\tvalid-rmse:1.00233\n",
      "[1200]\tvalid-rmse:0.998127\n",
      "[1400]\tvalid-rmse:0.996168\n",
      "[1600]\tvalid-rmse:0.99423\n",
      "[1800]\tvalid-rmse:0.992731\n",
      "[2000]\tvalid-rmse:0.991304\n",
      "[2200]\tvalid-rmse:0.990327\n",
      "[2400]\tvalid-rmse:0.989608\n",
      "[2600]\tvalid-rmse:0.98916\n",
      "Stopping. Best iteration:\n",
      "[2578]\tvalid-rmse:0.989159\n",
      "\n",
      "  auc =  98.91587490762562\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "clfs_xgb, imp_xgb, oof_xgb, pred_xgb = run_cv_xgb(train_df1, target, test_df1, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x7f43b8b77e10>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.pairplot(train_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
