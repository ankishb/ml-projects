{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5959, 3), (2553, 2), (5, 3))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('Dataset/train.csv')\n",
    "test  = pd.read_csv('Dataset/test.csv')\n",
    "sub   = pd.read_csv('Dataset/Sample_Submission.csv')\n",
    "\n",
    "train.shape, test.shape, sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Review Text'] = train['Review Text'].str.lower()\n",
    "test['Review Text']  = test['Review Text'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 404)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect = []\n",
    "# def correct_contraction(x):\n",
    "#     for word in contraction_mapping.keys():\n",
    "#         if word in x:\n",
    "#             collect.append(word)\n",
    "#             x = x.replace(word, dic[word])\n",
    "#     return x\n",
    "for text in train['Review Text'].values:\n",
    "    for x in text.split(\" \"):\n",
    "        for word in contraction_mapping.keys():\n",
    "            if word in x:\n",
    "                collect.append(word)\n",
    "# train['Review Text'].apply(correct_contraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping1 = {}\n",
    "for word in collect:\n",
    "    contraction_mapping1[word] = contraction_mapping[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('contraction_mapping.txt', 'w') as file:\n",
    "     file.write(json.dumps(contraction_mapping1)) # use `json.loads` to do the reverse\n",
    "\n",
    "\n",
    "# In case of serialization\n",
    "# import cPickle as pickle\n",
    "\n",
    "# with open('file.txt', 'w') as file:\n",
    "#      file.write(pickle.dumps(exDict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"didn't\": 'did not',\n",
       " \"don't\": 'do not',\n",
       " \"they're\": 'they are',\n",
       " 'don’t': 'do not',\n",
       " 'it’s': 'it is',\n",
       " \"can't\": 'cannot',\n",
       " 'doesn’t': 'does not',\n",
       " 'isn’t': 'is not',\n",
       " \"couldn't\": 'could not',\n",
       " \"it's\": 'it is',\n",
       " 'they’re': 'they are',\n",
       " 'we’ve': 'we have',\n",
       " \"It's\": 'it is',\n",
       " \"that's\": 'that is',\n",
       " \"doesn't\": 'does not',\n",
       " 'didn’t': 'did not',\n",
       " \"they've\": 'they have',\n",
       " 'wouldn’t': 'would not',\n",
       " 'that’s': 'that is',\n",
       " 'couldn’t': 'could not',\n",
       " \"what's\": 'what is',\n",
       " \"haven't\": 'have not',\n",
       " 'can’t': 'cannot',\n",
       " \"wasn't\": 'was not',\n",
       " \"weren't\": 'were not',\n",
       " 'wasn’t': 'was not',\n",
       " \"isn't\": 'is not',\n",
       " \"won't\": 'will not',\n",
       " 'you’re': 'you are',\n",
       " \"you're\": 'you are',\n",
       " \"u're\": 'you are',\n",
       " \"you'r\": 'you are',\n",
       " \"wouldn't\": 'would not',\n",
       " 'won’t': 'will not',\n",
       " \"hadn't\": 'had not',\n",
       " \"you'll\": 'you will',\n",
       " \"there's\": 'there is',\n",
       " 'here’s': 'here is',\n",
       " \"aren't\": 'are not',\n",
       " \"they'd\": 'they would',\n",
       " \"you've\": 'you have',\n",
       " \"we'll\": 'we will',\n",
       " 'weren’t': 'were not',\n",
       " 'hadn’t': 'had not',\n",
       " 'what’s': 'what is',\n",
       " \"it'd\": 'it would',\n",
       " 'haven’t': 'have not',\n",
       " 'he’s': 'he is',\n",
       " \"hasn't\": 'has not',\n",
       " \"he's\": 'he is',\n",
       " \"she's\": 'she is',\n",
       " 'you’ve': 'you have',\n",
       " \"who's\": 'who is',\n",
       " 'aren’t': 'are not',\n",
       " 'they’ve': 'they have',\n",
       " \"we've\": 'we have',\n",
       " 'i’m': 'i am',\n",
       " \"should've\": 'should have',\n",
       " \"could've\": 'could have',\n",
       " 'there’s': 'there is',\n",
       " 'you’ll': 'you will',\n",
       " \"you'd\": 'you would',\n",
       " \"must've\": 'must have',\n",
       " \"i'm\": 'i am',\n",
       " 'we’re': 'we are',\n",
       " \"i'll\": 'i will',\n",
       " \"i'l\": 'i will',\n",
       " \"shouldn't\": 'should not',\n",
       " 'shouldn’t': 'should not',\n",
       " 'hasn’t': 'has not',\n",
       " \"i've\": 'i have',\n",
       " \"i'v\": 'i have',\n",
       " 'we’ll': 'we will',\n",
       " 'let’s': 'let us',\n",
       " \"they'll\": 'they will',\n",
       " \"would've\": 'would have',\n",
       " \"he'd\": 'he would',\n",
       " 'you’d': 'you would',\n",
       " 'it’ll': 'it will',\n",
       " \"we're\": 'we are',\n",
       " \"let's\": 'let us',\n",
       " \"it'll\": 'it will',\n",
       " 'she’s': 'she is',\n",
       " \"we'd\": 'we would',\n",
       " 'should’ve': 'should have',\n",
       " 'that;s': 'that is',\n",
       " \"there'd\": 'there had',\n",
       " 'they’d': 'they would'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('contraction_mapping.txt') as file:\n",
    "    m = json.loads(file.read())\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5959/5959 [00:00<00:00, 11269.78it/s]\n",
      "100%|██████████| 2553/2553 [00:00<00:00, 11254.94it/s]\n"
     ]
    }
   ],
   "source": [
    "contraction_mapping = {\n",
    "    'ain;t': 'am not','ain´t': 'am not','ain’t': 'am not',\"aren't\": 'are not','â€“': '-','â€œ':'\"',\n",
    "    'aren,t': 'are not','aren;t': 'are not','aren´t': 'are not','aren’t': 'are not',\"can't\": 'cannot',\"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',\n",
    "    'can´t': 'cannot','can´t´ve': 'cannot have','can’t': 'cannot','can’t’ve': 'cannot have',\n",
    "    \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n",
    "    'couldn;t;ve': 'could not have','couldn´t': 'could not',\n",
    "    'couldn´t´ve': 'could not have','couldn’t': 'could not','couldn’t’ve': 'could not have','could´ve': 'could have',\n",
    "    'could’ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didn´t': 'did not',\n",
    "    'didn’t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesn´t': 'does not',\n",
    "    'doesn’t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','don´t': 'do not','don’t': 'do not',\n",
    "    \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n",
    "    'hadn;t;ve': 'had not have','hadn´t': 'had not','hadn´t´ve': 'had not have','hadn’t': 'had not','hadn’t’ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasn´t': 'has not','hasn’t': 'has not',\n",
    "    \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','haven´t': 'have not','haven’t': 'have not',\"he'd\": 'he would',\n",
    "    \"he'd've\": 'he would have',\"he'll\": 'he will',\n",
    "    \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',\n",
    "    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','he´d': 'he would','he´d´ve': 'he would have','he´ll': 'he will',\n",
    "    'he´s': 'he is','he’d': 'he would','he’d’ve': 'he would have','he’ll': 'he will','he’s': 'he is',\"how'd\": 'how did',\"how'll\": 'how will',\n",
    "    \"how's\": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',\n",
    "    'how;s': 'how is','how´d': 'how did','how´ll': 'how will','how´s': 'how is','how’d': 'how did','how’ll': 'how will',\n",
    "    'how’s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will',\n",
    "    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not',\n",
    "    'isn,t': 'is not','isn;t': 'is not','isn´t': 'is not','isn’t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is',\n",
    "    \"it's\": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','it´d': 'it would','it´ll': 'it will','it´s': 'it is',\n",
    "    'it’d': 'it would','it’ll': 'it will','it’s': 'it is',\n",
    "    'i´d': 'i would','i´ll': 'i will','i´m': 'i am','i´ve': 'i have','i’d': 'i would','i’ll': 'i will','i’m': 'i am',\n",
    "    'i’ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','let´s': 'let us',\n",
    "    'let’s': 'let us',\"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not','mayn;t': 'may not',\n",
    "    'mayn´t': 'may not','mayn’t': 'may not','ma´am': 'madam','ma’am': 'madam',\"might've\": 'might have','might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightn´t': 'might not',\n",
    "    'mightn’t': 'might not','might´ve': 'might have','might’ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n",
    "    \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustn´t': 'must not','mustn’t': 'must not','must´ve': 'must have',\n",
    "    'must’ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not','needn´t': 'need not','needn’t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n",
    "    'oughtn´t': 'ought not','oughtn’t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\"shan't\": 'shall not',\n",
    "    'shan,t': 'shall not','shan;t': 'shall not','shan´t': 'shall not','shan’t': 'shall not','sha´n´t': 'shall not','sha’n’t': 'shall not',\n",
    "    \"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is','she,d': 'she would','she,ll': 'she will',\n",
    "    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','she´d': 'she would','she´ll': 'she will',\n",
    "    'she´s': 'she is','she’d': 'she would','she’ll': 'she will','she’s': 'she is',\"should've\": 'should have','should,ve': 'should have','should;ve': 'should have',\n",
    "    \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldn´t': 'should not','shouldn’t': 'should not','should´ve': 'should have',\n",
    "    'should’ve': 'should have',\"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n",
    "    'that;s': 'that is','that´d': 'that would','that´s': 'that is','that’d': 'that would','that’s': 'that is',\"there'd\": 'there had',\n",
    "    \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',\n",
    "    'there´d': 'there had','there´s': 'there is','there’d': 'there had','there’s': 'there is',\n",
    "    \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have',\n",
    "    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',\n",
    "    'they;ve': 'they have','they´d': 'they would','they´ll': 'they will','they´re': 'they are','they´ve': 'they have','they’d': 'they would','they’ll': 'they will',\n",
    "    'they’re': 'they are','they’ve': 'they have',\"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasn´t': 'was not',\n",
    "    'wasn’t': 'was not',\"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n",
    "    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n",
    "    \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','weren´t': 'were not','weren’t': 'were not','we´d': 'we would','we´ll': 'we will',\n",
    "    'we´re': 'we are','we´ve': 'we have','we’d': 'we would','we’ll': 'we will','we’re': 'we are','we’ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\"what's\": 'what is',\n",
    "    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',\n",
    "    'what;s': 'what is','what;ve': 'what have','what´ll': 'what will',\n",
    "    'what´re': 'what are','what´s': 'what is','what´ve': 'what have','what’ll': 'what will','what’re': 'what are','what’s': 'what is',\n",
    "    'what’ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',\n",
    "    'where;s': 'where is','where´d': 'where did','where´s': 'where is','where’d': 'where did','where’s': 'where is',\n",
    "    \"who'll\": 'who will',\"who's\": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',\n",
    "    'who´ll': 'who will','who´s': 'who is','who’ll': 'who will','who’s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n",
    "    'won´t': 'will not','won’t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldn´t': 'would not',\n",
    "    'wouldn’t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are','you,d': 'you would','you,ll': 'you will',\n",
    "    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n",
    "    'you;re': 'you are','you´d': 'you would','you´ll': 'you will','you´re': 'you are','you’d': 'you would','you’ll': 'you will','you’re': 'you are',\n",
    "    '´cause': 'because','’cause': 'because',\"you've\": \"you have\",\"could'nt\": 'could not',\n",
    "    \"havn't\": 'have not',\"here’s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\"who've\": \"who have\",\"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n",
    "    \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",'don\"\"t': \"do not\",\"dosen't\": \"does not\",\n",
    "    \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\"there'll\": \"there will\",\"there're\": \"there are\",\n",
    "    \"this'll\": \"this all\",\"u're\": \"you are\", \"ya'll\": \"you all\",\"you'r\": \"you are\",\"you’ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\"dont't\": \"do not\",\"gov't\": \"government\",\n",
    "    \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"‘I\":'I'\n",
    "}\n",
    "\n",
    "\n",
    "def correct_contraction(x, dic):\n",
    "    for word in dic.keys():\n",
    "        if word in x:\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "train['Review Text'] = train['Review Text'].progress_apply(lambda x: correct_contraction(x, contraction_mapping))\n",
    "test['Review Text']  = test['Review Text'].progress_apply(lambda x: correct_contraction(x, contraction_mapping))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'title'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.rename(columns={'Review Text':'text', 'Review Title':'title'}, inplace=True)\n",
    "test.rename(columns={'Review Text':'text', 'Review Title':'title'}, inplace=True)\n",
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5959/5959 [00:00<00:00, 101100.89it/s]\n",
      "100%|██████████| 2553/2553 [00:00<00:00, 99763.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import os,operator\n",
    "\n",
    "extra_punct = [\n",
    "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
    "\n",
    "\n",
    "import string\n",
    "my_punct = list(string.punctuation)\n",
    "all_punct = list(set(my_punct + extra_punct))\n",
    "\n",
    "special_punc_mappings = {\"—\": \"-\", \"–\": \"-\", \"_\": \"-\", '”': '\"', \"″\": '\"', '“': '\"', '•': '.', '−': '-',\n",
    "                         \"’\": \"'\", \"‘\": \"'\", \"´\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','،':'','„':'',\n",
    "                         '…': ' ... ', '\\ufeff': ''}\n",
    "\n",
    "def spacing_punctuation(text):\n",
    "    \"\"\"\n",
    "    add space before and after punctuation and symbols\n",
    "    \"\"\"\n",
    "    for punc in all_punct:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, f' {punc} ')\n",
    "    return text\n",
    "\n",
    "def clean_special_punctuations(text):\n",
    "    for punc in special_punc_mappings:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, special_punc_mappings[punc])\n",
    "    # remove_diacritics don´t' ->  'don t'\n",
    "    #text = remove_diacritics(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = spacing_punctuation(text)\n",
    "    text = clean_special_punctuations(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "train[\"text\"] = train[\"text\"].progress_apply(preprocess)\n",
    "test[\"text\"]  = test[\"text\"].progress_apply(preprocess)\n",
    "\n",
    "train['text'] = train['text'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')\n",
    "test['text']  = test['text'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')\n",
    "\n",
    "train['text'].replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "test['text'].replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "\n",
    "train['text'].replace({'  ':' '}, regex=True, inplace=True)\n",
    "test['text'].replace({'  ':' '}, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5959 [00:00<?, ?it/s]\u001b[A\n",
      " 42%|████▏     | 2502/5959 [00:00<00:00, 25014.84it/s]\u001b[A\n",
      " 82%|████████▏ | 4906/5959 [00:00<00:00, 24712.90it/s]\u001b[A\n",
      "100%|██████████| 5959/5959 [00:00<00:00, 24284.20it/s]\u001b[A\n",
      "  0%|          | 0/2553 [00:00<?, ?it/s]\u001b[A\n",
      " 89%|████████▊ | 2262/2553 [00:00<00:00, 22616.35it/s]\u001b[A\n",
      "100%|██████████| 2553/2553 [00:00<00:00, 22637.21it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@[a-zA-Z0-9_]+', '', text)   \n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)   \n",
    "    text = re.sub(r'www.[^ ]+', '', text)  \n",
    "    text = re.sub(r'[a-zA-Z0-9]*www[a-zA-Z0-9]*com[a-zA-Z0-9]*', '', text)  \n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)   \n",
    "    text = [token for token in text.split() if len(token) > 2]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "train['text'] = train['text'].progress_apply(clean_text)\n",
    "test['text']  = test['text'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "100%|██████████| 8512/8512 [00:00<00:00, 203193.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Found embeddings for 91.36% of vocab\n",
      "Found embeddings for  99.55% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov_glove = vocab_check_coverage(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('solimo', 101),\n",
       " ('prenatals', 88),\n",
       " ('vitafusion', 52),\n",
       " ('softgels', 42),\n",
       " ('cetaphil', 42),\n",
       " ('muscletech', 35),\n",
       " ('softgel', 34),\n",
       " ('preworkout', 29),\n",
       " ('bcaa', 29),\n",
       " ('revly', 22),\n",
       " ('500mg', 15),\n",
       " ('idk', 15),\n",
       " ('gummie', 14),\n",
       " ('bcaas', 13),\n",
       " ('videoid', 13),\n",
       " ('3mg', 13),\n",
       " ('5mg', 13),\n",
       " ('swallowable', 13),\n",
       " ('1mg', 12),\n",
       " ('cellucor', 12),\n",
       " ('methylcobalamin', 11),\n",
       " ('1000mg', 11),\n",
       " ('naturewise', 10),\n",
       " ('protien', 10),\n",
       " ('mykind', 10),\n",
       " ('preworkouts', 9),\n",
       " ('shakeology', 9),\n",
       " ('asorbic', 9),\n",
       " ('250mg', 9),\n",
       " ('isopure', 8),\n",
       " ('125mg', 8),\n",
       " ('wholefoods', 8),\n",
       " ('petrolatum', 8),\n",
       " ('nutribullet', 7),\n",
       " ('phenoxyethanol', 7),\n",
       " ('naturemade', 7),\n",
       " ('stawberry', 7),\n",
       " ('octinoxate', 6),\n",
       " ('oxybenzone', 6),\n",
       " ('dimethicone', 6),\n",
       " ('natrue', 6),\n",
       " ('fishiness', 6),\n",
       " ('5lbs', 6),\n",
       " ('bleh', 6),\n",
       " ('rxbars', 6),\n",
       " ('benedryl', 6),\n",
       " ('lasership', 6),\n",
       " ('solimno', 6),\n",
       " ('100mg', 6),\n",
       " ('burmannii', 6)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_glove['oov_words'][:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "train['text'] = train['text'].str.replace('# ','')\n",
    "train['text'] = train['text'].str.replace(' - ','')\n",
    "train['text'] = train['text'].str.replace(' : ','')\n",
    "\n",
    "test['text'] = test['text'].str.replace('#','')\n",
    "test['text'] = test['text'].str.replace(' - ','')\n",
    "test['text'] = test['text'].str.replace(' : ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def splitting(data):\n",
    "#     new=[]\n",
    "#     for sentences in data:\n",
    "#         yes = sentences.split(\". \")\n",
    "#         new.append(yes) \n",
    "#     return new\n",
    "    \n",
    "# train['new_text'] = splitting(train['text'])\n",
    "# test['new_text'] = splitting(test['text'])\n",
    "\n",
    "\n",
    "# def cleaning1(data):\n",
    "#     new_text=[]\n",
    "#     for sentences in data:\n",
    "#         matching = [s for s in sentences if 'reply posted' not in s]\n",
    "#         new_text.append(matching)\n",
    "        \n",
    "#     return new_text\n",
    "\n",
    "# train['new_text'] = cleaning1(train['new_text'])\n",
    "# test['new_text'] = cleaning1(test['new_text'])\n",
    "\n",
    "\n",
    "# def cleaning2(data):\n",
    "#     new_text=[]\n",
    "#     for sentences in data:\n",
    "#         matching = [s for s in sentences if 'help center' not in s]\n",
    "#         new_text.append(matching)\n",
    "        \n",
    "#     return new_text\n",
    "\n",
    "# train['new_text'] = cleaning2(train['new_text'])\n",
    "# test['new_text'] = cleaning2(test['new_text'])\n",
    "\n",
    "\n",
    "# def cleaning3(data):\n",
    "#     new_text=[]\n",
    "    \n",
    "#     for sentences in data:\n",
    "#         if(len(sentences)>1):\n",
    "#             matching = [s for s in sentences if len(s) >= 15]\n",
    "#             new_text.append(matching)\n",
    "            \n",
    "#         else:\n",
    "#             matching = [s for s in sentences if len(s) >= 2]\n",
    "#             new_text.append(matching)\n",
    "        \n",
    "#     return new_text\n",
    "\n",
    "# train['new_text'] = cleaning3(train['new_text'])\n",
    "# test['new_text'] = cleaning3(test['new_text'])\n",
    "\n",
    "# def don(d):\n",
    "#     n=[]\n",
    "#     for s in d:\n",
    "#         res = \".\".join(s)\n",
    "#         n.append(res)\n",
    "#     return n\n",
    "\n",
    "# train['text'] = don(train['new_text'])\n",
    "# test['text']  = don(test['new_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5959 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 5959/5959 [00:00<00:00, 98919.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/2553 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 2553/2553 [00:00<00:00, 95665.74it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x\n",
    "\n",
    "train['text'] = train['text'].progress_apply(clean_text)\n",
    "test['text']  = test['text'].progress_apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5959, 3), (2553, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.makedirs('new_dataset')\n",
    "train.to_csv('new_dataset/train1.csv', index=None)\n",
    "test.to_csv('new_dataset/test1.csv', index=None)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "\n",
    "def tfidf_feature(train, test, col_name, min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=3, stopwords='english', \n",
    "                  n_component=120, decom_flag=False, which_method='svd', \n",
    "                  max_features=None):\n",
    "    \"\"\"return tfidf feature\n",
    "    Args:\n",
    "        train, test: dataframe\n",
    "        col_name: column name of text feature\n",
    "        min_df: if Int, then it represent count of the minimum words in corpus (remove very rare word)\n",
    "        analyzer: [‘word’, ‘char’]\n",
    "        ngram: max range of ngram\n",
    "        token_pattern: [using: r'\\w{1,}'] [by default: '(?u)\\b\\w\\w+\\b']\n",
    "        stopwords: ['english' or customized by remove specific words]\n",
    "        n_component: n_component of svd feature transform\n",
    "        decom_flag: Wheteher to run svd/nmf on top of that or not (by default: False)\n",
    "        which_method: which to run [svd or nmf] on top of tfidf (by default: False)\n",
    "        max_features: max no of features to keep, based on frequency. It will keep words with higher freq\n",
    "    return:\n",
    "        Transformed feature space of the text data, as well as tfidf function instance\n",
    "        if svd_flag== True : train_tf, test_tf, tfv, svd\n",
    "        else : train_tf, test_tf, tfv\n",
    "    example:\n",
    "        train_tfv, test_tfv, tfv = tfidf_feature(X_train, X_test, ['text'], min_df=3)\n",
    "        train_svd, test_svd, complete_tfv, tfv, svd = tfidf_feature(X_train, X_test, ['text'], \n",
    "            min_df=3, svd_component=3, svd_flag=True)\n",
    "\n",
    "    \"\"\"\n",
    "    tfv = TfidfVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, max_df=1.0, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                stop_words = stopwords)\n",
    "\n",
    "    complete_df = pd.concat([train[col_name], test[col_name]], axis=0)\n",
    "#         return complete_df\n",
    "#         print(complete_df.shape, complete_df.columns)\n",
    "\n",
    "    tfv.fit(list(complete_df[:].values))\n",
    "\n",
    "    if decom_flag is False:\n",
    "        train_tfv =  tfv.transform(train[col_name].values.ravel()) \n",
    "        test_tfv  = tfv.transform(test[col_name].values.ravel())\n",
    "\n",
    "        del complete_df\n",
    "        gc.collect()\n",
    "        return train_tfv, test_tfv, tfv\n",
    "    else:\n",
    "        complete_tfv = tfv.transform(complete_df[:].values.ravel())\n",
    "        \n",
    "        if which_method is 'svd':\n",
    "            svd = TruncatedSVD(n_components=n_component)\n",
    "            svd.fit(complete_tfv)\n",
    "            complete_dec = svd.transform(complete_tfv)\n",
    "        else:\n",
    "            nmf = NMF(n_components=n_component, random_state=1234, alpha=0, l1_ratio=0)\n",
    "            nmf.fit(complete_tfv)            \n",
    "            complete_dec = nmf.fit_transform(complete_tfv)            \n",
    "        \n",
    "        \n",
    "        complete_dec = pd.DataFrame(data=complete_dec)\n",
    "        complete_dec.columns = [which_method+'_'+str(i) for i in range(n_component)]\n",
    "\n",
    "        train_dec = complete_dec.iloc[:train.shape[0]]\n",
    "        test_dec = complete_dec.iloc[train.shape[0]:].reset_index(drop=True)\n",
    "\n",
    "        del complete_dec, complete_df\n",
    "        gc.collect()\n",
    "        return train_dec, test_dec, complete_tfv, tfv\n",
    "\n",
    "def countvect_feature(train, test, col_name, min_df=3, analyzer='word', token_pattern=r'\\w{1,}', ngram=3, stopwords='english', max_features=None):\n",
    "    \"\"\"return CountVectorizer feature\n",
    "    Args:\n",
    "        train, test: dataset\n",
    "        col_name: columns name of the text feature\n",
    "        min_df: if Int, then it represent count of the minimum words in corpus (remove very rare word)\n",
    "        analyzer: [‘word’, ‘char’]\n",
    "        ngram: max range of ngram\n",
    "        token_pattern: [using: r'\\w{1,}'] [by default: '(?u)\\b\\w\\w+\\b']\n",
    "        stopwords: ['english' or customized by remove specific words]\n",
    "        max_features: max no of features to keep, based on frequency. It will keep words with higher freq\n",
    "    return:\n",
    "        Count feature space of the text data, as well as its function instance\n",
    "    \"\"\"\n",
    "    ctv = CountVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                stop_words = stopwords)\n",
    "\n",
    "    complete_df = pd.concat([train[col_name], test[col_name]], axis=0)\n",
    "    ctv.fit(list(complete_df[:].values))\n",
    "\n",
    "    train_tf =  ctv.transform(train[col_name].values.ravel()) \n",
    "    test_tf  = ctv.transform(test[col_name].values.ravel())\n",
    "\n",
    "    del complete_df\n",
    "    gc.collect()\n",
    "    return train_tf, test_tf, ctv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidfs = []\n",
    "for ngram in [1,2,3,4,5]:\n",
    "    out_tfidfs.append(tfidf_feature(train, test, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf features:  (5959, 5101) (5959, 24307) (5959, 40708) (5959, 70881)\n",
      "count-vect features:  (5959, 5101) (5959, 24307) (5959, 40708) (5959, 70881)\n"
     ]
    }
   ],
   "source": [
    "out_tfidf1 = tfidf_feature(train, test, 'text', ngram=1)\n",
    "out_tfidf2 = tfidf_feature(train, test, 'text', ngram=2)\n",
    "out_tfidf3 = tfidf_feature(train, test, 'text', ngram=3)\n",
    "out_tfidf4 = tfidf_feature(train, test, 'text', ngram=5)\n",
    "\n",
    "out_vect1 = countvect_feature(train, test, 'text', ngram=1)\n",
    "out_vect2 = countvect_feature(train, test, 'text', ngram=2)\n",
    "out_vect3 = countvect_feature(train, test, 'text', ngram=3)\n",
    "out_vect4 = countvect_feature(train, test, 'text', ngram=5)\n",
    "\n",
    "print('tf-idf features: ', out_tfidf1[0].shape, \n",
    "     out_tfidf2[0].shape, out_tfidf3[0].shape, out_tfidf4[0].shape)\n",
    "print('count-vect features: ', out_vect1[0].shape, \n",
    "     out_vect2[0].shape, out_vect3[0].shape, out_vect4[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "train['target'] = train['topic'].astype('category').cat.codes\n",
    "train['target'] = train['target'].astype('int')\n",
    "all_class = list(train['target'].unique())\n",
    "print(len(all_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-30a61719cda7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tfidf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train[train['target'] == 1].shape, out_tfidf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4171, 5101), (1788, 5101), (4171,), (1788,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf1[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1788,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "# pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf1 :  0.36577181208053694\n",
      "tfidf2 :  0.32606263982102907\n",
      "tfidf3 :  0.3143176733780761\n",
      "tfidf4 :  0.3070469798657718\n",
      "==================================================\n",
      "count-vect1 :  0.4222595078299776\n",
      "count-vect2 :  0.37472035794183445\n",
      "count-vect3 :  0.3691275167785235\n",
      "count-vect4 :  0.3635346756152125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf1[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"tfidf1 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf2[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"tfidf2 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf3[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"tfidf3 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf4[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"tfidf4 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "print(\"==\"*25)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect1[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"count-vect1 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect2[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"count-vect2 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect3[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"count-vect3 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect4[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"count-vect4 : \", clf.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48266219239373603\n"
     ]
    }
   ],
   "source": [
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "    C=0.01, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "    random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(logistic_reg.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf1 :  0.2595078299776286\n",
      "tfidf2 :  0.2225950782997763\n",
      "tfidf3 :  0.21756152125279643\n",
      "tfidf4 :  0.2197986577181208\n",
      "==================================================\n",
      "count-vect1 :  0.48210290827740493\n",
      "count-vect2 :  0.48769574944071586\n",
      "count-vect3 :  0.4949664429530201\n",
      "count-vect4 :  0.46364653243847875\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf1[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"tfidf1 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf2[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"tfidf2 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf3[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"tfidf3 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf4[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"tfidf4 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "print(\"==\"*25)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect1[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"count-vect1 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect2[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"count-vect2 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect3[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"count-vect3 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect4[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"count-vect4 : \", logistic_reg.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not terrible, but not good. Tastes burnt and a...</td>\n",
       "      <td>Not my cup o’ joe</td>\n",
       "      <td>Burnt/ Over -roast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am so disappointed, it has no flavor, doesn'...</td>\n",
       "      <td>I am so disappointed, it has no flavor</td>\n",
       "      <td>Bad Flavor/Taste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I recently gave up my daily 6 cups of coffee, ...</td>\n",
       "      <td>Flavor was dissapointing</td>\n",
       "      <td>Bitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meh. I've adored Peruvian coffee for 20 years....</td>\n",
       "      <td>Smooth but majorly bland. Won't repurchase.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meh. I've adored Peruvian coffee for 20 years....</td>\n",
       "      <td>Smooth but majorly bland. Won't repurchase.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review Text  \\\n",
       "0  Not terrible, but not good. Tastes burnt and a...   \n",
       "1  I am so disappointed, it has no flavor, doesn'...   \n",
       "2  I recently gave up my daily 6 cups of coffee, ...   \n",
       "3  Meh. I've adored Peruvian coffee for 20 years....   \n",
       "4  Meh. I've adored Peruvian coffee for 20 years....   \n",
       "\n",
       "                                  Review Title               topic  \n",
       "0                            Not my cup o’ joe  Burnt/ Over -roast  \n",
       "1       I am so disappointed, it has no flavor    Bad Flavor/Taste  \n",
       "2                     Flavor was dissapointing              Bitter  \n",
       "3  Smooth but majorly bland. Won't repurchase.                 NaN  \n",
       "4  Smooth but majorly bland. Won't repurchase.                 NaN  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.read_csv('Dataset/Sample_Submission.csv')\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2553,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_reg.fit(out_vect3[0], train['target'])\n",
    "pred = logistic_reg.predict(out_vect3[1])\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>use chia seed protein shakes these tasted like...</td>\n",
       "      <td>Bad tast</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>use chia seed protein shakes these tasted like...</td>\n",
       "      <td>Bad tast</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not waste your money</td>\n",
       "      <td>No change. No results.</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>use the book fortify your life tieraona low do...</td>\n",
       "      <td>Good Vegan Choice, Poor Non Vegan Choice</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>use the book fortify your life tieraona low do...</td>\n",
       "      <td>Good Vegan Choice, Poor Non Vegan Choice</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  use chia seed protein shakes these tasted like...   \n",
       "1  use chia seed protein shakes these tasted like...   \n",
       "2                               not waste your money   \n",
       "3  use the book fortify your life tieraona low do...   \n",
       "4  use the book fortify your life tieraona low do...   \n",
       "\n",
       "                                      title  target  \n",
       "0                                  Bad tast       1  \n",
       "1                                  Bad tast       1  \n",
       "2                    No change. No results.      11  \n",
       "3  Good Vegan Choice, Poor Non Vegan Choice      10  \n",
       "4  Good Vegan Choice, Poor Non Vegan Choice      10  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def category_encoder(df)\n",
    "test.drop('topic', axis=1, inplace=True)\n",
    "test['target'] = pred\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['topic'] = test['target'].apply(lambda x: class_mapping_reverse[str(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('target', axis=1, inplace=True)\n",
    "# os.makedirs('submission')\n",
    "test.to_csv('submission/linear_model1.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping(df, col_name):\n",
    "    cat_codes = df[col_name].astype('category')\n",
    "    \n",
    "    class_mapping = {}\n",
    "    i = 0\n",
    "    for col in cat_codes.cat.categories:\n",
    "        class_mapping[col] = i\n",
    "        i += 1\n",
    "    \n",
    "    class_mapping_reverse = {}\n",
    "    for key, value in class_mapping.items():\n",
    "        class_mapping_reverse[value] = key\n",
    "\n",
    "    return class_mapping, class_mapping_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping, class_mapping_reverse = get_mapping(train, 'topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "params = {}\n",
    "params['alpha'] = 1\n",
    "passive_agg = PassiveAggressiveClassifier(C=params['alpha'], early_stopping=False, validation_fraction=0.3, n_iter_no_change=5, shuffle=True, verbose=0, n_jobs=-1, random_state=1234, loss='hinge', class_weight='balanced', average=False, n_iter=None)\n",
    "ridge_clf = RidgeClassifier(alpha=params['alpha'], fit_intercept=True, normalize=True, class_weight='balanced', random_state=1234)\n",
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, C=params['alpha'], fit_intercept=True, intercept_scaling=1, class_weight='balanced', random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07829977628635347\n",
      "==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37527964205816555\n",
      "==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/ankish/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42393736017897093\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "ridge_clf.fit(X_train, Y_train)\n",
    "print(ridge_clf.score(X_test, Y_test))\n",
    "print(\"==============\")\n",
    "\n",
    "passive_agg.fit(X_train, Y_train)\n",
    "print(passive_agg.score(X_test, Y_test))\n",
    "print(\"==============\")\n",
    "\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(logistic_reg.score(X_test, Y_test))\n",
    "print(\"==============\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48266219239373603\n",
      "0.4692393736017897\n",
      "0.43512304250559286\n",
      "0.42393736017897093\n",
      "0.40380313199105144\n",
      "0.39932885906040266\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for alpha in [0.01, 0.1, 0.5, 1, 5, 10]:\n",
    "    logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "        C=alpha, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "        random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "    logistic_reg.fit(X_train, Y_train)\n",
    "    pred = logistic_reg.score(X_test, Y_test)\n",
    "    print(pred)\n",
    "#     print(f1_score(ts_y, pred, average='micro', sample_weight=None))\n",
    "#     print(logistic_reg.score(ts_x, ts_y))\n",
    "print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for alpha in np.linspace(0.0001,0.1,10):\n",
    "    logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "        C=alpha, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "        random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "    logistic_reg.fit(X_train, Y_train)\n",
    "    pred = logistic_reg.score(X_test, Y_test)\n",
    "    print(alpha, \" : \", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Usage: plot_document_classification_20newsgroups.py [options]\n",
    "\n",
    "Options:\n",
    "  -h, --help            show this help message and exit\n",
    "  --report              Print a detailed classification report.\n",
    "  --chi2_select=SELECT_CHI2\n",
    "                        Select some number of features using a chi-squared\n",
    "                        test\n",
    "  --confusion_matrix    Print the confusion matrix.\n",
    "  --top10               Print ten most discriminative terms per class for\n",
    "                        every classifier.\n",
    "  --all_categories      Whether to use all categories or not.\n",
    "  --use_hashing         Use a hashing vectorizer.\n",
    "  --n_features=N_FEATURES\n",
    "                        n_features when using the hashing vectorizer.\n",
    "  --filtered            Remove newsgroup information that is easily overfit:\n",
    "                        headers, signatures, and quoting.\n",
    "\n",
    "Loading 20 newsgroups dataset for categories:\n",
    "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "data loaded\n",
    "2034 documents - 3.980MB (training set)\n",
    "1353 documents - 2.867MB (test set)\n",
    "4 categories\n",
    "\n",
    "Extracting features from the training data using a sparse vectorizer\n",
    "done in 0.412178s at 9.655MB/s\n",
    "n_samples: 2034, n_features: 33809\n",
    "\n",
    "Extracting features from the test data using the same vectorizer\n",
    "done in 0.351330s at 8.162MB/s\n",
    "n_samples: 1353, n_features: 33809\n",
    "\n",
    "================================================================================\n",
    "Ridge Classifier\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "RidgeClassifier(solver='sag', tol=0.01)\n",
    "train time: 0.132s\n",
    "test time:  0.001s\n",
    "accuracy:   0.896\n",
    "dimensionality: 33809\n",
    "density: 1.000000\n",
    "\n",
    "\n",
    "================================================================================\n",
    "Perceptron\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "Perceptron(max_iter=50)\n",
    "train time: 0.017s\n",
    "test time:  0.002s\n",
    "accuracy:   0.888\n",
    "dimensionality: 33809\n",
    "density: 0.255302\n",
    "\n",
    "\n",
    "================================================================================\n",
    "Passive-Aggressive\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "PassiveAggressiveClassifier(max_iter=50)\n",
    "train time: 0.031s\n",
    "test time:  0.002s\n",
    "accuracy:   0.904\n",
    "dimensionality: 33809\n",
    "density: 0.694674\n",
    "\n",
    "\n",
    "================================================================================\n",
    "kNN\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "KNeighborsClassifier(n_neighbors=10)\n",
    "train time: 0.002s\n",
    "test time:  0.317s\n",
    "accuracy:   0.858\n",
    "\n",
    "================================================================================\n",
    "Random forest\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "RandomForestClassifier(n_estimators=100)\n",
    "train time: 1.671s\n",
    "test time:  0.071s\n",
    "accuracy:   0.840\n",
    "\n",
    "================================================================================\n",
    "L2 penalty\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "LinearSVC(dual=False, tol=0.001)\n",
    "train time: 0.145s\n",
    "test time:  0.002s\n",
    "accuracy:   0.900\n",
    "dimensionality: 33809\n",
    "density: 1.000000\n",
    "\n",
    "\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "SGDClassifier(max_iter=50)\n",
    "train time: 0.030s\n",
    "test time:  0.002s\n",
    "accuracy:   0.902\n",
    "dimensionality: 33809\n",
    "density: 0.579380\n",
    "\n",
    "\n",
    "================================================================================\n",
    "L1 penalty\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "LinearSVC(dual=False, penalty='l1', tol=0.001)\n",
    "train time: 0.301s\n",
    "test time:  0.002s\n",
    "accuracy:   0.873\n",
    "dimensionality: 33809\n",
    "density: 0.005553\n",
    "\n",
    "\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "SGDClassifier(max_iter=50, penalty='l1')\n",
    "train time: 0.093s\n",
    "test time:  0.002s\n",
    "accuracy:   0.887\n",
    "dimensionality: 33809\n",
    "density: 0.022901\n",
    "\n",
    "\n",
    "================================================================================\n",
    "Elastic-Net penalty\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "SGDClassifier(max_iter=50, penalty='elasticnet')\n",
    "train time: 0.252s\n",
    "test time:  0.002s\n",
    "accuracy:   0.899\n",
    "dimensionality: 33809\n",
    "density: 0.187472\n",
    "\n",
    "\n",
    "================================================================================\n",
    "NearestCentroid (aka Rocchio classifier)\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "NearestCentroid()\n",
    "train time: 0.004s\n",
    "test time:  0.002s\n",
    "accuracy:   0.855\n",
    "\n",
    "================================================================================\n",
    "Naive Bayes\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "MultinomialNB(alpha=0.01)\n",
    "train time: 0.003s\n",
    "test time:  0.001s\n",
    "accuracy:   0.899\n",
    "dimensionality: 33809\n",
    "density: 1.000000\n",
    "\n",
    "\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "BernoulliNB(alpha=0.01)\n",
    "train time: 0.004s\n",
    "test time:  0.003s\n",
    "accuracy:   0.884\n",
    "dimensionality: 33809\n",
    "density: 1.000000\n",
    "\n",
    "\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "ComplementNB(alpha=0.1)\n",
    "train time: 0.004s\n",
    "test time:  0.001s\n",
    "accuracy:   0.911\n",
    "dimensionality: 33809\n",
    "density: 1.000000\n",
    "\n",
    "\n",
    "================================================================================\n",
    "LinearSVC with L1-based feature selection\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "Pipeline(steps=[('feature_selection',\n",
    "                 SelectFromModel(estimator=LinearSVC(dual=False, penalty='l1',\n",
    "                                                     tol=0.001))),\n",
    "                ('classification', LinearSVC())])\n",
    "train time: 0.252s\n",
    "test time:  0.002s\n",
    "accuracy:   0.880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_train[i] # feature count vector for training case i\n",
    "# y_train[i] # label for training case i\n",
    "\n",
    "# The count vectors are defined as:\n",
    "\n",
    "# p = sum of all feature count vectors with label 1\n",
    "\n",
    "# p = tf_train[y_train==1].sum(0) + 1\n",
    "\n",
    "# q = sum of all feature count vectors with label 0\n",
    "\n",
    "# q = tf_train[y_train==0].sum(0) + 1\n",
    "\n",
    "# Notice that we add 1 to both count vectors to ensure that every token appear at least one time in each class.\n",
    "\n",
    "# The log-count ratio r is:\n",
    "\n",
    "# r = np.log((p/p.sum()) / (q/q.sum()))\n",
    "\n",
    "# And b:\n",
    "\n",
    "# b = np.log(len(p) / len(q))\n",
    "\n",
    "# Just the ratio of number of positive and negative training cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_preds = tf_test @ r.T + b\n",
    "preds = pre_preds.T > 0\n",
    "accuracy = (preds == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_tfidf1[0][train['target'] == 5].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_tfidf1[0][train['target'] == 4].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5101"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.equal(out_tfidf1[0][train['target'] == 5].sum(0), \n",
    "                out_tfidf1[0][train['target'] == 3].sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5959, 5101)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_tfidf1[0][train['target'] == 5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5959, 5101)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_tfidf1[0][train['target'] == 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = train[train['target'] == 2].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.23786109, 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_tfidf1[0][idx].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts):\n",
    "    output = []\n",
    "    for i in texts:\n",
    "        s = [token.lemma_ for token in nlp(i)]\n",
    "        output.append(' '.join(s))\n",
    "    return output\n",
    "\n",
    "# train['text1'] = train['text'].progress_apply(lemmatization)\n",
    "# test['text1']  = test['text'].progress_apply(lemmatization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-fcdeeb9ff49e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-c84f4b0212f4>\u001b[0m in \u001b[0;36mlemmatization\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(seqs_in)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36muniqued_fwd\u001b[0;34m(X, drop)\u001b[0m\n\u001b[1;32m    377\u001b[0m         )\n\u001b[1;32m    378\u001b[0m         \u001b[0mX_uniq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mY_uniq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_Y_uniq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_uniq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_uniq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mY_uniq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(X, *a, **k)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfwd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfwd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msplitter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mto_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_sink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/neural/_classes/hash_embed.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, ids, drop)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"uint64\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dropout_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = train.append(test, ignore_index=True)\n",
    "df['text1'] = df['text'].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [d, i, d,  , n, o, t, h, i, n, g,  , f, o, r, ...\n",
       "1    [d, i, d,  , n, o, t, h, i, n, g,  , f, o, r, ...\n",
       "2    [ , h, a, v, e,  , b, o, u, g, h, t,  , t, h, ...\n",
       "3    [g, a, v, e,  , m, e,  , a, n,  , a, l, l, e, ...\n",
       "4    [t, h, e, s, e,  , d, o,  , n, o, t,  , c, o, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]['text'].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do nothing for -PRON- , do not help lose even ...</td>\n",
       "      <td>Useless</td>\n",
       "      <td>shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do nothing for -PRON- , do not help lose even ...</td>\n",
       "      <td>Useless</td>\n",
       "      <td>not effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>have buy these bag and immediately open one ...</td>\n",
       "      <td>trash ! ! ! Do not buy these bag -PRON- ’ a wa...</td>\n",
       "      <td>Customer Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>give -PRON- an allergic reaction on -PRON- fac...</td>\n",
       "      <td>Do not recommend</td>\n",
       "      <td>allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>these do not compare to the name brand wipe . ...</td>\n",
       "      <td>can not tackle big mess</td>\n",
       "      <td>texture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  do nothing for -PRON- , do not help lose even ...   \n",
       "1  do nothing for -PRON- , do not help lose even ...   \n",
       "2    have buy these bag and immediately open one ...   \n",
       "3  give -PRON- an allergic reaction on -PRON- fac...   \n",
       "4  these do not compare to the name brand wipe . ...   \n",
       "\n",
       "                                               title                  topic  \n",
       "0                                            Useless  shipment and delivery  \n",
       "1                                            Useless          not effective  \n",
       "2  trash ! ! ! Do not buy these bag -PRON- ’ a wa...       Customer Service  \n",
       "3                                   Do not recommend               allergic  \n",
       "4                            can not tackle big mess                texture  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [did, nothing, for, me, ,, did, not, help, los...\n",
       "1    [did, nothing, for, me, ,, did, not, help, los...\n",
       "2    [have, bought, these, bag, and, immediately, o...\n",
       "3    [gave, me, an, allergic, reaction, on, my, fac...\n",
       "4    [these, do, not, compare, to, the, name, brand...\n",
       "5    [these, do, not, compare, to, the, name, brand...\n",
       "6    [these, do, not, compare, to, the, name, brand...\n",
       "7                                           [no, good]\n",
       "8    [these, are, extremely, hard, to, swallow, ., ...\n",
       "9    [first, of, all, the, style, am, leaving, revi...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(text):\n",
    "    return []\n",
    "    for i in texts:\n",
    "        s = [token.lemma_ for token in nlp(i)]\n",
    "        output.append(' '.join(s))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenize(df1['text'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'help'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "porter.stem('helping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did\n",
      "noth\n",
      "for\n",
      "me\n",
      ",\n",
      "did\n",
      "not\n",
      "help\n",
      "lost\n",
      "even\n",
      "with\n",
      "work\n",
      "out\n",
      "and\n",
      "eat\n",
      "healthi\n",
      ".\n",
      "did\n",
      "not\n",
      "curb\n",
      "appetit\n",
      "or\n",
      "anyth\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in df1['text'][1].split(\" \"):\n",
    "    print(porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "865"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del df1, df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankish/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8512, 3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train.append(test, ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "cv = CountVectorizer(max_df=0.95,min_df=2,stop_words='english')\n",
    "term_matrix = cv.fit_transform(df['text'])\n",
    "# print(term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=5, n_jobs=4, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, n_jobs=4)\n",
    "lda.fit(term_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20114701, 6.74804017, 0.20373605, ..., 2.194052  , 0.46571526,\n",
       "       0.20000209])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seal\n",
      "br\n",
      "opened\n",
      "time\n",
      "just\n",
      "box\n",
      "broken\n",
      "product\n",
      "brand\n",
      "bottle\n"
     ]
    }
   ],
   "source": [
    "topic = lda.components_[0]\n",
    "top_words_indices = topic.argsort()[-10:]\n",
    "for index in top_words_indices:\n",
    "    print(cv.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for topic 0\n",
      "['seal', 'br', 'opened', 'time', 'just', 'box', 'broken', 'product', 'brand', 'bottle']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Top words for topic 1\n",
      "['bad', 'good', 'product', 'just', 'stomach', 'work', 'day', 'did', 'taking', 'br']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Top words for topic 2\n",
      "['br', 'bad', 'money', 'item', 'order', 'brand', 'ordered', 'did', 'received', 'product']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Top words for topic 3\n",
      "['good', 'does', 'powder', 'bad', 'just', 'br', 'protein', 'flavor', 'like', 'taste']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Top words for topic 4\n",
      "['pills', 'brand', 'did', 'product', 'gummy', 'good', 'vitamin', 'vitamins', 'like', 'br']\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "topic_word_dict = {}\n",
    "for index,topic in enumerate(lda.components_):\n",
    "    words = [cv.get_feature_names()[i] for i in topic.argsort()[-10:]]\n",
    "    topic_word_dict[index] = words\n",
    "    print('Top words for topic {}'.format(index))\n",
    "    print(words)\n",
    "    print('-'*120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics = lda.transform(term_matrix)\n",
    "data['topic'] = topics.argmax(axis=1)\n",
    "\n",
    "\n",
    "def assign_topics(row):\n",
    "    topic = row['topic']\n",
    "    words = topic_word_dict[topic]\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "data['topic words'] = data.apply(assign_topics,axis=1)\n",
    "print(data.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
