{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIgbgEoTozte"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os, gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnVUrr64Xn4s"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ASWuMEPFpcC7",
    "outputId": "be9eb0a3-a64a-4b98-9d1c-36eb96e56fe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ml-projects' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ankishb/ml-projects.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "A2KsHLEpqbjk",
    "outputId": "25fd1040-6e89-4abe-8158-cbcd6ca991b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4196,), (3727,), (5959, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('ml-projects/amazon-ml/Dataset/train.csv')\n",
    "test  = pd.read_csv('ml-projects/amazon-ml/Dataset/test.csv')\n",
    "\n",
    "train.columns = ['text','title','topic']\n",
    "test.columns  = ['text','title']\n",
    "train['text'].unique().shape, train['title'].unique().shape, train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCVbhwqJXJbZ"
   },
   "outputs": [],
   "source": [
    "df = train.append(test, ignore_index=True)\n",
    "df['text']  = df['text'].str.lower()\n",
    "df['title'] = df['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OpUGicaCXJPy"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "contract_file = 'ml-projects/amazon-ml/contraction_mapping.txt'\n",
    "with open(contract_file) as f:\n",
    "    contraction_mapping = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "I4stxEmxXJEc",
    "outputId": "02f2e5f7-04df-4e4f-868b-e025af59a2ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8512/8512 [00:00<00:00, 38900.72it/s]\n",
      "100%|██████████| 8512/8512 [00:00<00:00, 115629.62it/s]\n"
     ]
    }
   ],
   "source": [
    "def correct_contraction(x, dic):\n",
    "    for word in dic.keys():\n",
    "        if word in x:\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "df['text']  = df['text'].progress_apply(lambda x: correct_contraction(x, contraction_mapping))\n",
    "df['title'] = df['title'].progress_apply(lambda x: correct_contraction(x, contraction_mapping))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Cn2lzuDgXwDw",
    "outputId": "fb190dcb-72f7-4b0b-bfd9-8df9b0fa7303"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8512/8512 [00:00<00:00, 71819.81it/s]\n",
      "100%|██████████| 8512/8512 [00:00<00:00, 99091.89it/s]\n"
     ]
    }
   ],
   "source": [
    "import os,operator\n",
    "\n",
    "extra_punct = [\n",
    "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
    "\n",
    "\n",
    "import string\n",
    "my_punct = list(string.punctuation)\n",
    "all_punct = list(set(my_punct + extra_punct))\n",
    "\n",
    "special_punc_mappings = {\"—\": \"-\", \"–\": \"-\", \"_\": \"-\", '”': '\"', \"″\": '\"', '“': '\"', '•': '.', '−': '-',\n",
    "                         \"’\": \"'\", \"‘\": \"'\", \"´\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','،':'','„':'',\n",
    "                         '…': ' ... ', '\\ufeff': ''}\n",
    "\n",
    "def spacing_punctuation(text):\n",
    "    \"\"\"\n",
    "    add space before and after punctuation and symbols\n",
    "    \"\"\"\n",
    "    for punc in all_punct:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, f' {punc} ')\n",
    "    return text\n",
    "\n",
    "def clean_special_punctuations(text):\n",
    "    for punc in special_punc_mappings:\n",
    "        if punc in text:\n",
    "#             print(punc)\n",
    "            text = text.replace(punc, special_punc_mappings[punc])\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = spacing_punctuation(text)\n",
    "    text = clean_special_punctuations(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"text\"] = df[\"text\"].progress_apply(preprocess)\n",
    "df['text'] = df['text'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')\n",
    "df['text'].replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "df['text'].replace({'  ':' '}, regex=True, inplace=True)\n",
    "\n",
    "df[\"title\"] = df[\"title\"].progress_apply(preprocess)\n",
    "df['title'] = df['title'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')\n",
    "df['title'].replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "df['title'].replace({'  ':' '}, regex=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "gHaJJJLqXIs0",
    "outputId": "4601bde9-30c3-4c38-a8f3-f88d2f460d6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8512/8512 [00:00<00:00, 63752.51it/s]\n",
      "100%|██████████| 8512/8512 [00:00<00:00, 102358.46it/s]\n",
      "100%|██████████| 8512/8512 [00:00<00:00, 70954.27it/s]\n",
      "100%|██████████| 8512/8512 [00:00<00:00, 92911.31it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@[a-zA-Z0-9_]+', '', text)   \n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)   \n",
    "    text = re.sub(r'www.[^ ]+', '', text)  \n",
    "    text = re.sub(r'[a-zA-Z0-9]*www[a-zA-Z0-9]*com[a-zA-Z0-9]*', '', text)  \n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)   \n",
    "    text = [token for token in text.split() if len(token) > 2]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x\n",
    "\n",
    "\n",
    "df['text']   = df['text'].progress_apply(clean_text)\n",
    "df['title']  = df['title'].progress_apply(clean_text)\n",
    "\n",
    "df['text'] = df['text'].progress_apply(clean_text)\n",
    "df['title'] = df['title'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Orb0PA2KYCoP",
    "outputId": "9519bb82-c119-4ed1-bf7a-0309245f08f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5959, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "tr_len = train.shape[0]\n",
    "del train\n",
    "gc.collect()\n",
    "train = df[:tr_len]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "c_dHxZ1T0Mmm",
    "outputId": "659ed35a-7d71-4110-966d-fb8a80442bdf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300 mg of sodium per scoop serv...</td>\n",
       "      <td>way too much sodium per serving</td>\n",
       "      <td>Shipment_and_delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dont buy from here   the vitamins are sp...</td>\n",
       "      <td>spoiled vitamins phony expiration date</td>\n",
       "      <td>Expiry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all finally got damn flavor right  peach ma...</td>\n",
       "      <td>peach mango flavor is gooood</td>\n",
       "      <td>Bad_Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arginine increases uric acid in the blood a...</td>\n",
       "      <td>do not take if you are prone to gout</td>\n",
       "      <td>Wrong_Product_received</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arginine is supposed to be one of the safer...</td>\n",
       "      <td>good brand</td>\n",
       "      <td>Not_Effective</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  ...                   topic\n",
       "0                 300 mg of sodium per scoop serv...  ...   Shipment_and_delivery\n",
       "1        dont buy from here   the vitamins are sp...  ...                  Expiry\n",
       "2     all finally got damn flavor right  peach ma...  ...        Bad_Taste/Flavor\n",
       "3     arginine increases uric acid in the blood a...  ...  Wrong_Product_received\n",
       "4     arginine is supposed to be one of the safer...  ...           Not_Effective\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['topic'] = train['topic'].apply(lambda x: \"_\".join(x.split(\" \")))\n",
    "gp = train.groupby(['text','title']).agg({\n",
    "        \"topic\": lambda x: \" \".join(x.values)\n",
    "    })\n",
    "train1 = pd.DataFrame(gp.reset_index())\n",
    "train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZrQ6Hsn45FH0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8RAK8ts0cd0"
   },
   "outputs": [],
   "source": [
    "labels = train1['topic'].values\n",
    "\n",
    "train['target'] = train['topic'].astype('category').cat.codes\n",
    "train['target'] = train['target'].astype('int')\n",
    "\n",
    "def get_mapping(df, col_name):\n",
    "    cat_codes = df[col_name].astype('category')\n",
    "    \n",
    "    class_mapping = {}\n",
    "    i = 0\n",
    "    for col in cat_codes.cat.categories:\n",
    "        class_mapping[col] = i\n",
    "        i += 1\n",
    "    \n",
    "    class_mapping_reverse = {}\n",
    "    for key, value in class_mapping.items():\n",
    "        class_mapping_reverse[value] = key\n",
    "\n",
    "    return class_mapping, class_mapping_reverse\n",
    "\n",
    "cl_map, cl_map_inv = get_mapping(train, 'topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "tNdvKkJ161Z8",
    "outputId": "54743d33-bc7a-49c0-c58b-768b8a6dcd03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 567. 1193.  233.    8.  239.   31.  135.   37.    6.   44.  215.  608.\n",
      "  466.  107.  712.  387.  123.  410.   97.  228.   99.]\n"
     ]
    }
   ],
   "source": [
    "lt = np.zeros((train1.shape[0], 21))\n",
    "j = -1\n",
    "for key, value in cl_map.items():\n",
    "    j += 1\n",
    "    for i, label in enumerate(labels):\n",
    "        if key in label.split(\" \"):\n",
    "            lt[i][j] = 1\n",
    "\n",
    "print(lt.sum(axis=0))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0eljUk461Km"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "colab_type": "code",
    "id": "CYubNCxM7glN",
    "outputId": "14ff9d1e-f93f-499e-92a4-a2974a9e9faa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Allergic</th>\n",
       "      <th>Bad_Taste/Flavor</th>\n",
       "      <th>Color_and_texture</th>\n",
       "      <th>Customer_Issues</th>\n",
       "      <th>Customer_Service</th>\n",
       "      <th>Didn't_Like</th>\n",
       "      <th>Expiry</th>\n",
       "      <th>False_Advertisement</th>\n",
       "      <th>Hard_to_Chew</th>\n",
       "      <th>Inferior_to_competitors</th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Not_Effective</th>\n",
       "      <th>Packaging</th>\n",
       "      <th>Pricing</th>\n",
       "      <th>Quality/Contaminated</th>\n",
       "      <th>Shipment_and_delivery</th>\n",
       "      <th>Smells_Bad</th>\n",
       "      <th>Texture</th>\n",
       "      <th>Too_Sweet</th>\n",
       "      <th>Too_big_to_swallow</th>\n",
       "      <th>Wrong_Product_received</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Allergic  Bad_Taste/Flavor  ...  Too_big_to_swallow  Wrong_Product_received\n",
       "0         0                 0  ...                   0                       0\n",
       "1         0                 0  ...                   0                       0\n",
       "2         0                 1  ...                   0                       0\n",
       "3         0                 0  ...                   0                       1\n",
       "4         0                 0  ...                   0                       0\n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_table = pd.DataFrame(data=lt, columns=list(cl_map.keys()))\n",
    "l_table = l_table.astype('int')\n",
    "l_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_Ra-IfmJ8Mxm",
    "outputId": "4657d680-cfaf-437a-bd1c-918a9fded0b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4214, 3), (4214, 21))"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1.shape, l_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8P9j09mnYzm2"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "\n",
    "def tfidf_feature(train, test, col_name, min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=3, stopwords='english', \n",
    "                  n_component=120, decom_flag=False, which_method='svd', \n",
    "                  max_features=None, feat_col_name='svd'):\n",
    "\n",
    "    tfv = TfidfVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, max_df=1.0, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                stop_words = stopwords)\n",
    "\n",
    "    complete_df = pd.concat([train[col_name], test[col_name]], axis=0)\n",
    "#         return complete_df\n",
    "#         print(complete_df.shape, complete_df.columns)\n",
    "\n",
    "    tfv.fit(list(complete_df[:].values))\n",
    "\n",
    "    if decom_flag is False:\n",
    "        train_tfv =  tfv.transform(train[col_name].values.ravel()) \n",
    "        test_tfv  = tfv.transform(test[col_name].values.ravel())\n",
    "\n",
    "        del complete_df\n",
    "        gc.collect()\n",
    "        return train_tfv, test_tfv, tfv\n",
    "    else:\n",
    "        complete_tfv = tfv.transform(complete_df[:].values.ravel())\n",
    "        \n",
    "        if which_method is 'svd':\n",
    "            svd = TruncatedSVD(n_components=n_component)\n",
    "            svd.fit(complete_tfv)\n",
    "            complete_dec = svd.transform(complete_tfv)\n",
    "        else:\n",
    "            nmf = NMF(n_components=n_component, random_state=1234, alpha=0, l1_ratio=0)\n",
    "            nmf.fit(complete_tfv)            \n",
    "            complete_dec = nmf.fit_transform(complete_tfv)            \n",
    "        \n",
    "        \n",
    "        complete_dec = pd.DataFrame(data=complete_dec)\n",
    "        complete_dec.columns = [feat_col_name+'_'+str(i) for i in range(n_component)]\n",
    "\n",
    "        train_dec = complete_dec.iloc[:train.shape[0]]\n",
    "        test_dec = complete_dec.iloc[train.shape[0]:].reset_index(drop=True)\n",
    "\n",
    "        del complete_dec, complete_df\n",
    "        gc.collect()\n",
    "        print(\"=\"*15, \" done \", \"=\"*15)\n",
    "        return train_dec, test_dec, complete_tfv, tfv\n",
    "\n",
    "def countvect_feature(train, test, col_name, min_df=3, \n",
    "                      analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                      ngram=3, stopwords='english', max_features=None):\n",
    "\n",
    "    ctv = CountVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                stop_words = stopwords)\n",
    "\n",
    "    complete_df = pd.concat([train[col_name], test[col_name]], axis=0)\n",
    "    ctv.fit(list(complete_df[:].values))\n",
    "\n",
    "    train_tf =  ctv.transform(train[col_name].values.ravel()) \n",
    "    test_tf  = ctv.transform(test[col_name].values.ravel())\n",
    "\n",
    "    del complete_df\n",
    "    gc.collect()\n",
    "    return train_tf, test_tf, ctv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "HO93zfglY7Bc",
    "outputId": "58a05420-1803-4b73-c188-8553877666f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class count:  [   0 2911  959  277   53   11    3]\n",
      "class wise:   [ 567 1194  234    8  239   31  136   37    6   44  216  611  467  107\n",
      "  715  390  123  410   97  228   99]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd88be548d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA00AAAFACAYAAABgApfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len1 = l_table.sum(axis=1)\n",
    "# len2 = l_table.sum(axis=0)\n",
    "fig, ax = plt.subplots(1,2,figsize=(14,5))\n",
    "print(\"class count: \", np.bincount(len1))\n",
    "print(\"class wise:  \", np.bincount(train['target']))\n",
    "\n",
    "sns.countplot(len1, ax=ax[0])\n",
    "sns.countplot(train['target'], ax=ax[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1479MfNY6-b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PjGZJGtBY66j"
   },
   "outputs": [],
   "source": [
    "def get_count_vectorizer(df, col_name, min_df=3, analyzer='word', stopwords='english', \n",
    "                     token_pattern=r'\\w{1,}', ngram=3, max_features=None):\n",
    "    ctv = CountVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                stop_words = stopwords)\n",
    "\n",
    "    ctv.fit(list(df[col_name].values))\n",
    "\n",
    "    df_new =  ctv.transform(df[col_name].values.ravel()) \n",
    "    return df_new\n",
    "\n",
    "\n",
    "def get_tfidf_feature(df, col_name, min_df=3, analyzer='word', stopwords='english',\n",
    "                  token_pattern=r'\\w{1,}', ngram=3, max_features=None):\n",
    "\n",
    "    tfv = TfidfVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, max_df=1.0, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                stop_words = stopwords)\n",
    "\n",
    "    tfv.fit(list(df[col_name].values))\n",
    "    df_new =  tfv.transform(df[col_name].values.ravel()) \n",
    "    \n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "6tCfrHanY64f",
    "outputId": "6f7fdbf7-94b8-45f2-a8cf-c9901a0a35ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8512, 198403)\n",
      "(8512, 98154)\n",
      "(8512, 42302)\n",
      "(8512, 18840)\n",
      "(8512, 11203)\n",
      "(8512, 7797)\n",
      "(8512, 5610)\n",
      "(8512, 4712)\n",
      "(8512, 4074)\n",
      "(8512, 3576)\n",
      "(8512, 3209)\n",
      "(8512, 2928)\n",
      "(8512, 2693)\n",
      "(8512, 2511)\n",
      "(8512, 2312)\n"
     ]
    }
   ],
   "source": [
    "count_vect_text = []\n",
    "for ngram in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]:\n",
    "#     if ngram < 11 : continue\n",
    "    cvect = get_count_vectorizer(df, 'text', ngram)\n",
    "    print(cvect.shape)\n",
    "    count_vect_text.append(cvect)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "TlH79RQLY62C",
    "outputId": "cb07665a-43aa-45ae-9da0-0be9ce8cfd10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8512, 5329)\n",
      "(8512, 25168)\n",
      "(8512, 42302)\n",
      "(8512, 58359)\n",
      "(8512, 73871)\n",
      "(8512, 88889)\n",
      "(8512, 103421)\n",
      "(8512, 117475)\n",
      "(8512, 131055)\n",
      "(8512, 144168)\n"
     ]
    }
   ],
   "source": [
    "tfidf_text_store = []\n",
    "for ngram in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    tfidf = get_tfidf_feature(df, 'text', ngram=ngram)\n",
    "    print(tfidf.shape)\n",
    "    tfidf_text_store.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CEy6T-haY6zR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "colab_type": "code",
    "id": "yzMgCFi-Y6uT",
    "outputId": "16c33b9c-2dd7-477d-b042-dd882805ac50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf 0          acc: 0.3698\n",
      "tfidf 1          acc: 0.3396\n",
      "tfidf 2          acc: 0.3148\n",
      "tfidf 3          acc: 0.3282\n",
      "tfidf 4          acc: 0.3101\n",
      "tfidf 5          acc: 0.3094\n",
      "tfidf 6          acc: 0.3087\n",
      "tfidf 7          acc: 0.2919\n",
      "tfidf 8          acc: 0.3060\n",
      "tfidf 9          acc: 0.2940\n",
      "==============================\n",
      "count-vect 0          acc: 0.3174\n",
      "count-vect 1          acc: 0.3074\n",
      "count-vect 2          acc: 0.3611\n",
      "count-vect 3          acc: 0.3973\n",
      "count-vect 4          acc: 0.4195\n",
      "count-vect 5          acc: 0.4114\n",
      "count-vect 6          acc: 0.4248\n",
      "count-vect 7          acc: 0.4550\n",
      "count-vect 8          acc: 0.4450\n",
      "count-vect 9          acc: 0.4490\n",
      "count-vect 10         acc: 0.4423\n",
      "count-vect 11         acc: 0.4389\n",
      "count-vect 12         acc: 0.4349\n",
      "count-vect 13         acc: 0.4443\n",
      "count-vect 14         acc: 0.4624\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split    \n",
    "\n",
    "for i, data in enumerate(tfidf_text_store):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = MultinomialNB().fit(X_train, Y_train)\n",
    "    print('tfidf {:<10} acc: {:.4f}'.format(str(i), clf.score(X_test, Y_test)))\n",
    "\n",
    "print(\"=\"*30)\n",
    "for i, data in enumerate(count_vect_text):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = MultinomialNB().fit(X_train, Y_train)\n",
    "    print('count-vect {:<10} acc: {:.4f}'.format(str(i), clf.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 679
    },
    "colab_type": "code",
    "id": "MDekNq5zdm3P",
    "outputId": "79d47a94-dff5-47e9-dcda-a28a1e04281c"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-af2051442313>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtrain_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml_table\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moneVsAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2119\u001b[0m                      random_state=random_state)\n\u001b[1;32m   2120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2121\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2123\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \"\"\"\n\u001b[1;32m   1322\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1634\u001b[0m         \u001b[0mclass_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_counts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1636\u001b[0;31m             raise ValueError(\"The least populated class in y has only 1\"\n\u001b[0m\u001b[1;32m   1637\u001b[0m                              \u001b[0;34m\" member, which is too few. The minimum\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m                              \u001b[0;34m\" number of groups for any class cannot\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier as oneVsAll\n",
    "\n",
    "for i, data in enumerate(count_vect_text):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = oneVsAll(MultinomialNB()).fit(X_train, Y_train)\n",
    "    print('count-vect {:<10} acc: {:.4f}'.format(str(i), clf.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8nqXEqqd3wo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Pz4JBwYiOCK"
   },
   "outputs": [],
   "source": [
    "categories = list(data_raw.columns.values)\n",
    "sns.set(font_scale = 2)\n",
    "plt.figure(figsize=(15,8))ax= sns.barplot(categories, data_raw.iloc[:,2:].sum().values)plt.title(\"Comments in each category\", fontsize=24)\n",
    "plt.ylabel('Number of comments', fontsize=18)\n",
    "plt.xlabel('Comment Type ', fontsize=18)#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = data_raw.iloc[:,2:].sum().values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=18)plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zd4bISEhiN4V"
   },
   "outputs": [],
   "source": [
    "rowSums = data_raw.iloc[:,2:].sum(axis=1)\n",
    "multiLabel_counts = rowSums.value_counts()\n",
    "multiLabel_counts = multiLabel_counts.iloc[1:]sns.set(font_scale = 2)\n",
    "plt.figure(figsize=(15,8))ax = sns.barplot(multiLabel_counts.index, multiLabel_counts.values)plt.title(\"Comments having multiple labels \")\n",
    "plt.ylabel('Number of comments', fontsize=18)\n",
    "plt.xlabel('Number of labels', fontsize=18)#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = multiLabel_counts.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WiSzLLKiNwh"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import sys\n",
    "import warningsdata = data_rawif not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantextdef cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaneddef keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sentdata['comment_text'] = data['comment_text'].str.lower()\n",
    "data['comment_text'] = data['comment_text'].apply(cleanHtml)\n",
    "data['comment_text'] = data['comment_text'].apply(cleanPunc)\n",
    "data['comment_text'] = data['comment_text'].apply(keepAlpha)\n",
    "\n",
    "    Next we remove all the stop-words present in the comments using the default set of stop-words that can be downloaded from NLTK library. We also add few stop-words to the standard list.\n",
    "    Stop words are basically a set of commonly used words in any language, not just English. The reason why stop words are critical to many applications is that, if we remove the words that are very commonly used in a given language, we can focus on the important words instead.\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\" \", sentence)data['comment_text'] = data['comment_text'].apply(removeStopWords)\n",
    "\n",
    "    Next we do stemming. There exist different kinds of stemming which basically transform words with roughly the same semantics to one standard form. For example, for amusing, amusement, and amused, the stem would be amus.\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentencedata['comment_text'] = data['comment_text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dYdmyPf2iNod"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVop2ZbhhfXf"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])for category in categories:\n",
    "    print('**Processing {} comments...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(x_train, train[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(x_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-g2jJhXreJs1"
   },
   "outputs": [],
   "source": [
    "# jeremy naive bayes\n",
    "\n",
    "\n",
    "import re, string\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "It turns out that using TF-IDF gives even better priors than the binarized features used in the paper. I don't think this has been mentioned in any paper before, but it improves leaderboard score from 0.59 to 0.55.\n",
    "\n",
    "n = train.shape[0]\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "trn_term_doc = vec.fit_transform(train[COMMENT])\n",
    "\n",
    "test_term_doc = vec.transform(test[COMMENT])\n",
    "\n",
    "This creates a sparse matrix with only a small number of non-zero elements (stored elements in the representation below).\n",
    "\n",
    "trn_term_doc, test_term_doc\n",
    "\n",
    "(<159571x426005 sparse matrix of type '<class 'numpy.float64'>'\n",
    " \twith 17775104 stored elements in Compressed Sparse Row format>,\n",
    " <153164x426005 sparse matrix of type '<class 'numpy.float64'>'\n",
    " \twith 14765755 stored elements in Compressed Sparse Row format>)\n",
    "\n",
    "Here's the basic naive bayes feature equation:\n",
    "\n",
    "def pr(y_i, y):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "x = trn_term_doc\n",
    "test_x = test_term_doc\n",
    "\n",
    "Fit a model for one dependent at a time:\n",
    "\n",
    "def get_mdl(y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression(C=4, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r\n",
    "\n",
    "preds = np.zeros((len(test), len(label_cols)))\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    m,r = get_mdl(train[j])\n",
    "    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3OPKT88g5eY"
   },
   "outputs": [],
   "source": [
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=50000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])\n",
    "\n",
    "scores = []\n",
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    classifier = LogisticRegression(C=0.1, solver='sag')\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    scores.append(cv_score)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "    classifier.fit(train_features, train_target)\n",
    "    submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "moxXdlMGhJ0u"
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "amazon-ml-explore.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
