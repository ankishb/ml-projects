{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tqdm.pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os, gc, json\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "sns.set(context='notebook', style='whitegrid', \n",
    "        palette='deep', font='sans-serif', \n",
    "        font_scale=2, color_codes=True, rc=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5959, 3), (2553, 2), (5, 3))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('Dataset/train.csv')\n",
    "test  = pd.read_csv('Dataset/test.csv')\n",
    "sub   = pd.read_csv('Dataset/Sample_Submission.csv')\n",
    "\n",
    "train.shape, test.shape, sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['text', 'title'], dtype='object'),\n",
       " Index(['text', 'title', 'topic'], dtype='object'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.rename(columns={'Review Text':'text', 'Review Title':'title'}, inplace=True)\n",
    "test.rename(columns={'Review Text':'text', 'Review Title':'title'}, inplace=True)\n",
    "test.columns, train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.append(test, ignore_index=True)\n",
    "df['text']  = df['text'].str.lower()\n",
    "df['title'] = df['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('contraction_mapping.txt') as f:\n",
    "    contraction_mapping = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8512/8512 [00:00<00:00, 52461.00it/s]\n",
      "100%|██████████| 8512/8512 [00:00<00:00, 174336.83it/s]\n"
     ]
    }
   ],
   "source": [
    "def correct_contraction(x, dic):\n",
    "    for word in dic.keys():\n",
    "        if word in x:\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "df['text']  = df['text'].progress_apply(lambda x: correct_contraction(x, contraction_mapping))\n",
    "df['title'] = df['title'].progress_apply(lambda x: correct_contraction(x, contraction_mapping))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8512/8512 [00:00<00:00, 91795.46it/s]\n",
      "100%|██████████| 8512/8512 [00:00<00:00, 165371.63it/s]\n"
     ]
    }
   ],
   "source": [
    "import os,operator\n",
    "\n",
    "extra_punct = [\n",
    "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
    "\n",
    "\n",
    "import string\n",
    "my_punct = list(string.punctuation)\n",
    "all_punct = list(set(my_punct + extra_punct))\n",
    "\n",
    "special_punc_mappings = {\"—\": \"-\", \"–\": \"-\", \"_\": \"-\", '”': '\"', \"″\": '\"', '“': '\"', '•': '.', '−': '-',\n",
    "                         \"’\": \"'\", \"‘\": \"'\", \"´\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','،':'','„':'',\n",
    "                         '…': ' ... ', '\\ufeff': ''}\n",
    "\n",
    "def spacing_punctuation(text):\n",
    "    \"\"\"\n",
    "    add space before and after punctuation and symbols\n",
    "    \"\"\"\n",
    "    for punc in all_punct:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, f' {punc} ')\n",
    "    return text\n",
    "\n",
    "def clean_special_punctuations(text):\n",
    "    for punc in special_punc_mappings:\n",
    "        if punc in text:\n",
    "#             print(punc)\n",
    "            text = text.replace(punc, special_punc_mappings[punc])\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = spacing_punctuation(text)\n",
    "    text = clean_special_punctuations(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"text\"] = df[\"text\"].progress_apply(preprocess)\n",
    "df['text'] = df['text'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')\n",
    "df['text'].replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "df['text'].replace({'  ':' '}, regex=True, inplace=True)\n",
    "\n",
    "df[\"title\"] = df[\"title\"].progress_apply(preprocess)\n",
    "df['title'] = df['title'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')\n",
    "df['title'].replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "df['title'].replace({'  ':' '}, regex=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8512/8512 [00:00<00:00, 24679.42it/s]\n",
      "100%|██████████| 8512/8512 [00:00<00:00, 110714.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@[a-zA-Z0-9_]+', '', text)   \n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)   \n",
    "    text = re.sub(r'www.[^ ]+', '', text)  \n",
    "    text = re.sub(r'[a-zA-Z0-9]*www[a-zA-Z0-9]*com[a-zA-Z0-9]*', '', text)  \n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)   \n",
    "    text = [token for token in text.split() if len(token) > 2]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "df['text']   = df['text'].progress_apply(clean_text)\n",
    "df['title']  = df['title'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8512/8512 [00:00<00:00, 100333.06it/s]\n",
      "100%|██████████| 8512/8512 [00:00<00:00, 140914.34it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x\n",
    "\n",
    "df['text'] = df['text'].progress_apply(clean_text)\n",
    "df['title'] = df['title'].progress_apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for sent in df['title']:\n",
    "    for word in sent.split(\" \"):\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2831"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('not', 2041),\n",
       " ('taste', 759),\n",
       " ('the', 740),\n",
       " ('and', 665),\n",
       " ('bad', 610),\n",
       " ('for', 547),\n",
       " ('product', 486),\n",
       " ('good', 469),\n",
       " ('but', 399),\n",
       " ('flavor', 363),\n",
       " ('did', 308),\n",
       " ('this', 282),\n",
       " ('like', 265),\n",
       " ('work', 257),\n",
       " ('buy', 255),\n",
       " ('too', 245),\n",
       " ('very', 231),\n",
       " ('does', 209),\n",
       " ('received', 200),\n",
       " ('horrible', 199),\n",
       " ('tastes', 187),\n",
       " ('smell', 187),\n",
       " ('broken', 187),\n",
       " ('terrible', 183),\n",
       " ('wrong', 174),\n",
       " ('texture', 172),\n",
       " ('was', 167),\n",
       " ('with', 165),\n",
       " ('you', 161),\n",
       " ('never', 159),\n",
       " ('great', 156),\n",
       " ('item', 150),\n",
       " ('awful', 146),\n",
       " ('these', 142),\n",
       " ('are', 138),\n",
       " ('have', 136),\n",
       " ('packaging', 135),\n",
       " ('gross', 131),\n",
       " ('money', 129),\n",
       " ('they', 127),\n",
       " ('just', 124),\n",
       " ('sweet', 122),\n",
       " ('arrived', 121),\n",
       " ('quality', 120),\n",
       " ('brand', 120),\n",
       " ('bottle', 118),\n",
       " ('all', 118),\n",
       " ('poor', 115),\n",
       " ('what', 115),\n",
       " ('order', 112),\n",
       " ('pills', 107),\n",
       " ('weird', 107),\n",
       " ('from', 105),\n",
       " ('again', 102),\n",
       " ('get', 98),\n",
       " ('delivery', 96),\n",
       " ('stomach', 96),\n",
       " ('waste', 90),\n",
       " ('your', 88),\n",
       " ('disgusting', 88),\n",
       " ('made', 86),\n",
       " ('after', 84),\n",
       " ('really', 82),\n",
       " ('smells', 79),\n",
       " ('big', 76),\n",
       " ('that', 76),\n",
       " ('expired', 76),\n",
       " ('will', 76),\n",
       " ('sent', 76),\n",
       " ('shipping', 74),\n",
       " ('beware', 74),\n",
       " ('', 74),\n",
       " ('better', 72),\n",
       " ('sticky', 70),\n",
       " ('price', 69),\n",
       " ('disappointed', 69),\n",
       " ('fish', 69),\n",
       " ('well', 68),\n",
       " ('damaged', 67),\n",
       " ('receive', 67),\n",
       " ('package', 66),\n",
       " ('nothing', 64),\n",
       " ('would', 64),\n",
       " ('reaction', 63),\n",
       " ('fishy', 63),\n",
       " ('delivered', 63),\n",
       " ('has', 63),\n",
       " ('much', 62),\n",
       " ('one', 62),\n",
       " ('hard', 62),\n",
       " ('them', 61),\n",
       " ('date', 60),\n",
       " ('cannot', 59),\n",
       " ('allergic', 59),\n",
       " ('seal', 59),\n",
       " ('ingredients', 59),\n",
       " ('vitamin', 58),\n",
       " ('sick', 58),\n",
       " ('swallow', 58),\n",
       " ('time', 57),\n",
       " ('oil', 57),\n",
       " ('nasty', 57),\n",
       " ('sugar', 56),\n",
       " ('okay', 56),\n",
       " ('seller', 56),\n",
       " ('works', 54),\n",
       " ('yuck', 53),\n",
       " ('fake', 52),\n",
       " ('expiration', 51),\n",
       " ('vitamins', 51),\n",
       " ('old', 50),\n",
       " ('than', 50),\n",
       " ('difference', 50),\n",
       " ('worth', 49),\n",
       " ('only', 48),\n",
       " ('aftertaste', 48),\n",
       " ('got', 48),\n",
       " ('return', 48),\n",
       " ('huge', 47),\n",
       " ('out', 47),\n",
       " ('take', 47),\n",
       " ('service', 47),\n",
       " ('had', 46),\n",
       " ('effective', 46),\n",
       " ('other', 45),\n",
       " ('open', 44),\n",
       " ('side', 44),\n",
       " ('protein', 42),\n",
       " ('stuck', 42),\n",
       " ('together', 42),\n",
       " ('tasting', 42),\n",
       " ('use', 42),\n",
       " ('misleading', 41),\n",
       " ('pill', 41),\n",
       " ('melted', 40),\n",
       " ('way', 40),\n",
       " ('came', 39),\n",
       " ('gave', 39),\n",
       " ('when', 39),\n",
       " ('something', 38),\n",
       " ('chalky', 38),\n",
       " ('changed', 38),\n",
       " ('contains', 38),\n",
       " ('gummies', 37),\n",
       " ('gummy', 37),\n",
       " ('chocolate', 36),\n",
       " ('can', 36),\n",
       " ('were', 35),\n",
       " ('opened', 35),\n",
       " ('more', 35),\n",
       " ('ordered', 35),\n",
       " ('missing', 35),\n",
       " ('effects', 35),\n",
       " ('help', 35),\n",
       " ('expensive', 34),\n",
       " ('new', 34),\n",
       " ('batch', 34),\n",
       " ('formula', 34),\n",
       " ('plastic', 34),\n",
       " ('caused', 33),\n",
       " ('love', 33),\n",
       " ('customer', 33),\n",
       " ('best', 33),\n",
       " ('used', 33),\n",
       " ('strong', 32),\n",
       " ('stale', 32),\n",
       " ('makes', 32),\n",
       " ('some', 32),\n",
       " ('low', 31),\n",
       " ('box', 31),\n",
       " ('purchase', 31),\n",
       " ('large', 31),\n",
       " ('nope', 31),\n",
       " ('size', 30),\n",
       " ('now', 30),\n",
       " ('day', 30),\n",
       " ('different', 30),\n",
       " ('products', 30),\n",
       " ('skin', 30),\n",
       " ('refund', 30),\n",
       " ('tried', 29),\n",
       " ('little', 29),\n",
       " ('meh', 29),\n",
       " ('off', 29),\n",
       " ('rancid', 29),\n",
       " ('powder', 29),\n",
       " ('kids', 29),\n",
       " ('two', 29),\n",
       " ('dont', 28),\n",
       " ('could', 28),\n",
       " ('supplement', 28),\n",
       " ('issue', 28),\n",
       " ('why', 28),\n",
       " ('upset', 28),\n",
       " ('upon', 28),\n",
       " ('sure', 28),\n",
       " ('chemical', 28),\n",
       " ('first', 27),\n",
       " ('high', 27),\n",
       " ('most', 27),\n",
       " ('working', 27),\n",
       " ('effect', 27),\n",
       " ('capsules', 27),\n",
       " ('horse', 27),\n",
       " ('disappointing', 27),\n",
       " ('worst', 27),\n",
       " ('over', 27),\n",
       " ('ever', 27),\n",
       " ('expected', 27),\n",
       " ('its', 27),\n",
       " ('value', 27),\n",
       " ('recommend', 26),\n",
       " ('back', 26),\n",
       " ('food', 26),\n",
       " ('still', 26),\n",
       " ('vanilla', 26),\n",
       " ('even', 26),\n",
       " ('fine', 26),\n",
       " ('useless', 25),\n",
       " ('fan', 25),\n",
       " ('before', 25),\n",
       " ('long', 25),\n",
       " ('control', 25),\n",
       " ('ineffective', 25),\n",
       " ('shipped', 25),\n",
       " ('buying', 25),\n",
       " ('worse', 25),\n",
       " ('careful', 25),\n",
       " ('feel', 25),\n",
       " ('contain', 25),\n",
       " ('may', 24),\n",
       " ('bottles', 24),\n",
       " ('empty', 24),\n",
       " ('maybe', 24),\n",
       " ('sour', 24),\n",
       " ('gritty', 24),\n",
       " ('about', 24),\n",
       " ('buyer', 24),\n",
       " ('flavors', 24),\n",
       " ('any', 23),\n",
       " ('change', 23),\n",
       " ('less', 23),\n",
       " ('cheap', 23),\n",
       " ('stevia', 23),\n",
       " ('months', 23),\n",
       " ('anything', 23),\n",
       " ('results', 22),\n",
       " ('super', 22),\n",
       " ('away', 22),\n",
       " ('hot', 22),\n",
       " ('many', 21),\n",
       " ('where', 21),\n",
       " ('energy', 21),\n",
       " ('stuff', 21),\n",
       " ('bitter', 21),\n",
       " ('extremely', 21),\n",
       " ('decent', 21),\n",
       " ('arrival', 21),\n",
       " ('mix', 21),\n",
       " ('cap', 21),\n",
       " ('tasty', 21),\n",
       " ('iron', 21),\n",
       " ('chew', 21),\n",
       " ('container', 20),\n",
       " ('issues', 20),\n",
       " ('dissolve', 20),\n",
       " ('pretty', 20),\n",
       " ('same', 20),\n",
       " ('full', 20),\n",
       " ('read', 20),\n",
       " ('glass', 20),\n",
       " ('organic', 20),\n",
       " ('impressed', 19),\n",
       " ('look', 19),\n",
       " ('bag', 19),\n",
       " ('instead', 19),\n",
       " ('cause', 19),\n",
       " ('cookies', 19),\n",
       " ('natural', 19),\n",
       " ('enough', 19),\n",
       " ('headache', 19),\n",
       " ('fast', 19),\n",
       " ('twice', 19),\n",
       " ('mess', 19),\n",
       " ('brands', 19),\n",
       " ('odor', 19),\n",
       " ('need', 18),\n",
       " ('dry', 18),\n",
       " ('thick', 18),\n",
       " ('rash', 18),\n",
       " ('almost', 18),\n",
       " ('hate', 18),\n",
       " ('happy', 18),\n",
       " ('description', 18),\n",
       " ('please', 18),\n",
       " ('diarrhea', 18),\n",
       " ('want', 18),\n",
       " ('hair', 18),\n",
       " ('vegan', 18),\n",
       " ('weight', 18),\n",
       " ('items', 18),\n",
       " ('unsealed', 17),\n",
       " ('leaked', 17),\n",
       " ('lid', 17),\n",
       " ('sleep', 17),\n",
       " ('pain', 17),\n",
       " ('past', 17),\n",
       " ('drink', 17),\n",
       " ('absolutely', 17),\n",
       " ('try', 17),\n",
       " ('tablets', 17),\n",
       " ('odd', 17),\n",
       " ('scent', 17),\n",
       " ('check', 16),\n",
       " ('send', 16),\n",
       " ('easy', 16),\n",
       " ('been', 16),\n",
       " ('easily', 16),\n",
       " ('might', 16),\n",
       " ('nauseous', 16),\n",
       " ('acid', 16),\n",
       " ('burns', 16),\n",
       " ('trash', 15),\n",
       " ('lost', 15),\n",
       " ('nice', 15),\n",
       " ('else', 15),\n",
       " ('consistency', 15),\n",
       " ('month', 15),\n",
       " ('last', 15),\n",
       " ('safety', 15),\n",
       " ('constipation', 15),\n",
       " ('water', 15),\n",
       " ('stick', 15),\n",
       " ('how', 15),\n",
       " ('save', 15),\n",
       " ('artificial', 15),\n",
       " ('bought', 15),\n",
       " ('stars', 15),\n",
       " ('strawberry', 15),\n",
       " ('toxic', 15),\n",
       " ('expire', 15),\n",
       " ('lot', 15),\n",
       " ('here', 15),\n",
       " ('eyes', 15),\n",
       " ('smelled', 14),\n",
       " ('yucky', 14),\n",
       " ('shipment', 14),\n",
       " ('gas', 14),\n",
       " ('thin', 14),\n",
       " ('alcohol', 14),\n",
       " ('milk', 14),\n",
       " ('three', 14),\n",
       " ('right', 14),\n",
       " ('there', 14),\n",
       " ('sensitive', 14),\n",
       " ('candy', 14),\n",
       " ('store', 14),\n",
       " ('soon', 14),\n",
       " ('using', 14),\n",
       " ('improvement', 14),\n",
       " ('severe', 14),\n",
       " ('everyone', 14),\n",
       " ('warm', 14),\n",
       " ('mouth', 14),\n",
       " ('rotten', 14),\n",
       " ('others', 14),\n",
       " ('flavored', 14),\n",
       " ('nausea', 13),\n",
       " ('pack', 13),\n",
       " ('serving', 13),\n",
       " ('difficult', 13),\n",
       " ('sealed', 13),\n",
       " ('deal', 13),\n",
       " ('see', 13),\n",
       " ('allergy', 13),\n",
       " ('every', 13),\n",
       " ('noticeable', 13),\n",
       " ('cetaphil', 13),\n",
       " ('taking', 13),\n",
       " ('should', 13),\n",
       " ('reviews', 13),\n",
       " ('wipes', 13),\n",
       " ('star', 13),\n",
       " ('tablet', 13),\n",
       " ('coated', 13),\n",
       " ('favorite', 13),\n",
       " ('correct', 13),\n",
       " ('condition', 13),\n",
       " ('mold', 13),\n",
       " ('days', 13),\n",
       " ('inside', 13),\n",
       " ('wet', 13),\n",
       " ('teeth', 13),\n",
       " ('started', 13),\n",
       " ('care', 13),\n",
       " ('stinky', 13),\n",
       " ('life', 13),\n",
       " ('half', 13),\n",
       " ('per', 12),\n",
       " ('cost', 12),\n",
       " ('hit', 12),\n",
       " ('tasted', 12),\n",
       " ('chemicals', 12),\n",
       " ('compared', 12),\n",
       " ('chewy', 12),\n",
       " ('experience', 12),\n",
       " ('sugary', 12),\n",
       " ('headaches', 12),\n",
       " ('soft', 12),\n",
       " ('packaged', 12),\n",
       " ('coffee', 12),\n",
       " ('sucralose', 12),\n",
       " ('face', 12),\n",
       " ('wish', 12),\n",
       " ('real', 12),\n",
       " ('week', 12),\n",
       " ('know', 12),\n",
       " ('sweetener', 12),\n",
       " ('false', 12),\n",
       " ('thing', 12),\n",
       " ('scoop', 12),\n",
       " ('cinnamon', 12),\n",
       " ('completely', 12),\n",
       " ('pure', 12),\n",
       " ('synthetic', 12),\n",
       " ('warning', 12),\n",
       " ('corn', 12),\n",
       " ('loss', 12),\n",
       " ('soy', 12),\n",
       " ('took', 12),\n",
       " ('fresh', 12),\n",
       " ('available', 12),\n",
       " ('review', 12),\n",
       " ('centrum', 12),\n",
       " ('done', 12),\n",
       " ('bags', 11),\n",
       " ('increase', 11),\n",
       " ('always', 11),\n",
       " ('dha', 11),\n",
       " ('cold', 11),\n",
       " ('free', 11),\n",
       " ('yet', 11),\n",
       " ('looks', 11),\n",
       " ('clumpy', 11),\n",
       " ('zinc', 11),\n",
       " ('causes', 11),\n",
       " ('sucks', 11),\n",
       " ('delicious', 11),\n",
       " ('everywhere', 11),\n",
       " ('option', 11),\n",
       " ('yikes', 11),\n",
       " ('defective', 11),\n",
       " ('who', 11),\n",
       " ('make', 11),\n",
       " ('nestle', 11),\n",
       " ('rather', 11),\n",
       " ('mushy', 11),\n",
       " ('leaky', 11),\n",
       " ('during', 11),\n",
       " ('problems', 11),\n",
       " ('pre', 11),\n",
       " ('lemon', 11),\n",
       " ('their', 11),\n",
       " ('garden', 11),\n",
       " ('switch', 11),\n",
       " ('aware', 11),\n",
       " ('watch', 11),\n",
       " ('folic', 11),\n",
       " ('stay', 11),\n",
       " ('spots', 11),\n",
       " ('people', 11),\n",
       " ('name', 11),\n",
       " ('totally', 10),\n",
       " ('trust', 10),\n",
       " ('notice', 10),\n",
       " ('priced', 10),\n",
       " ('fruit', 10),\n",
       " ('broke', 10),\n",
       " ('red', 10),\n",
       " ('leaks', 10),\n",
       " ('company', 10),\n",
       " ('needs', 10),\n",
       " ('otherwise', 10),\n",
       " ('our', 10),\n",
       " ('dreams', 10),\n",
       " ('heartburn', 10),\n",
       " ('advertising', 10),\n",
       " ('down', 10),\n",
       " ('went', 10),\n",
       " ('smelling', 10),\n",
       " ('sold', 10),\n",
       " ('poison', 10),\n",
       " ('ingredient', 10),\n",
       " ('list', 10),\n",
       " ('gentle', 10),\n",
       " ('flavorless', 10),\n",
       " ('bland', 10),\n",
       " ('expires', 10),\n",
       " ('non', 10),\n",
       " ('choice', 10),\n",
       " ('overpriced', 10),\n",
       " ('transit', 10),\n",
       " ('cramps', 10),\n",
       " ('cherry', 10),\n",
       " ('things', 10),\n",
       " ('horrid', 9),\n",
       " ('slow', 9),\n",
       " ('symptoms', 9),\n",
       " ('thought', 9),\n",
       " ('also', 9),\n",
       " ('hives', 9),\n",
       " ('miss', 9),\n",
       " ('color', 9),\n",
       " ('formulation', 9),\n",
       " ('contact', 9),\n",
       " ('regular', 9),\n",
       " ('capsule', 9),\n",
       " ('version', 9),\n",
       " ('hype', 9),\n",
       " ('rip', 9),\n",
       " ('kind', 9),\n",
       " ('nutrition', 9),\n",
       " ('caution', 9),\n",
       " ('average', 9),\n",
       " ('deliver', 9),\n",
       " ('weeks', 9),\n",
       " ('because', 9),\n",
       " ('cake', 9),\n",
       " ('felt', 9),\n",
       " ('sticks', 9),\n",
       " ('health', 9),\n",
       " ('arrive', 9),\n",
       " ('small', 9),\n",
       " ('advertised', 9),\n",
       " ('slightly', 9),\n",
       " ('job', 9),\n",
       " ('ruined', 9),\n",
       " ('diapers', 9),\n",
       " ('opening', 9),\n",
       " ('greasy', 9),\n",
       " ('ugh', 9),\n",
       " ('melatonin', 9),\n",
       " ('unbearable', 9),\n",
       " ('cream', 9),\n",
       " ('bring', 8),\n",
       " ('servings', 8),\n",
       " ('almond', 8),\n",
       " ('discolored', 8),\n",
       " ('nearly', 8),\n",
       " ('skip', 8),\n",
       " ('vega', 8),\n",
       " ('unable', 8),\n",
       " ('without', 8),\n",
       " ('probably', 8),\n",
       " ('allergies', 8),\n",
       " ('syrup', 8),\n",
       " ('unpleasant', 8),\n",
       " ('takes', 8),\n",
       " ('dangerous', 8),\n",
       " ('mint', 8),\n",
       " ('hoping', 8),\n",
       " ('itchy', 8),\n",
       " ('paid', 8),\n",
       " ('ice', 8),\n",
       " ('stink', 8),\n",
       " ('close', 8),\n",
       " ('hated', 8),\n",
       " ('absorb', 8),\n",
       " ('costco', 8),\n",
       " ('returned', 8),\n",
       " ('lotion', 8),\n",
       " ('strange', 8),\n",
       " ('online', 8),\n",
       " ('multivitamin', 8),\n",
       " ('ripped', 8),\n",
       " ('guess', 8),\n",
       " ('spoiled', 8),\n",
       " ('quite', 8),\n",
       " ('tea', 8),\n",
       " ('true', 8),\n",
       " ('didnt', 8),\n",
       " ('gives', 8),\n",
       " ('purple', 8),\n",
       " ('bother', 8),\n",
       " ('multivitamins', 8),\n",
       " ('seem', 8),\n",
       " ('wanted', 8),\n",
       " ('inedible', 8),\n",
       " ('actually', 8),\n",
       " ('rubbery', 8),\n",
       " ('pass', 8),\n",
       " ('green', 8),\n",
       " ('getting', 8),\n",
       " ('put', 8),\n",
       " ('weak', 8),\n",
       " ('itself', 8),\n",
       " ('scam', 8),\n",
       " ('messy', 8),\n",
       " ('dose', 7),\n",
       " ('information', 7),\n",
       " ('tummy', 7),\n",
       " ('prenatals', 7),\n",
       " ('bit', 7),\n",
       " ('deceptive', 7),\n",
       " ('late', 7),\n",
       " ('blend', 7),\n",
       " ('tampered', 7),\n",
       " ('tell', 7),\n",
       " ('adhesive', 7),\n",
       " ('break', 7),\n",
       " ('acne', 7),\n",
       " ('creatine', 7),\n",
       " ('pictured', 7),\n",
       " ('left', 7),\n",
       " ('helps', 7),\n",
       " ('chalk', 7),\n",
       " ('gooey', 7),\n",
       " ('folate', 7),\n",
       " ('address', 7),\n",
       " ('short', 7),\n",
       " ('seals', 7),\n",
       " ('garbage', 7),\n",
       " ('workout', 7),\n",
       " ('found', 7),\n",
       " ('sticking', 7),\n",
       " ('quantity', 7),\n",
       " ('original', 7),\n",
       " ('dog', 7),\n",
       " ('avoid', 7),\n",
       " ('find', 7),\n",
       " ('form', 7),\n",
       " ('night', 7),\n",
       " ('lead', 7),\n",
       " ('healthy', 7),\n",
       " ('yes', 7),\n",
       " ('seriously', 7),\n",
       " ('once', 7),\n",
       " ('pregnant', 7),\n",
       " ('replacement', 7),\n",
       " ('solid', 7),\n",
       " ('seems', 7),\n",
       " ('leak', 7),\n",
       " ('expiry', 7),\n",
       " ('whole', 7),\n",
       " ('believe', 7),\n",
       " ('nutritional', 7),\n",
       " ('berry', 7),\n",
       " ('poorly', 7),\n",
       " ('gone', 7),\n",
       " ('probiotics', 7),\n",
       " ('room', 7),\n",
       " ('watermelon', 7),\n",
       " ('recipe', 7),\n",
       " ('aweful', 7),\n",
       " ('prenatal', 7),\n",
       " ('spilled', 7),\n",
       " ('within', 7),\n",
       " ('ache', 7),\n",
       " ('filling', 7),\n",
       " ('potent', 7),\n",
       " ('batches', 7),\n",
       " ('research', 7),\n",
       " ('turbo', 7),\n",
       " ('finish', 7),\n",
       " ('apple', 7),\n",
       " ('intended', 7),\n",
       " ('standard', 7),\n",
       " ('prime', 7),\n",
       " ('arms', 7),\n",
       " ('residue', 7),\n",
       " ('messes', 6),\n",
       " ('bottom', 6),\n",
       " ('increased', 6),\n",
       " ('amazing', 6),\n",
       " ('returns', 6),\n",
       " ('give', 6),\n",
       " ('coconut', 6),\n",
       " ('ick', 6),\n",
       " ('liquid', 6),\n",
       " ('baby', 6),\n",
       " ('gel', 6),\n",
       " ('caps', 6),\n",
       " ('pleasant', 6),\n",
       " ('blue', 6),\n",
       " ('say', 6),\n",
       " ('body', 6),\n",
       " ('both', 6),\n",
       " ('sand', 6),\n",
       " ('exactly', 6),\n",
       " ('mixes', 6),\n",
       " ('doesnt', 6),\n",
       " ('due', 6),\n",
       " ('response', 6),\n",
       " ('literal', 6),\n",
       " ('years', 6),\n",
       " ('noticed', 6),\n",
       " ('mango', 6),\n",
       " ('daily', 6),\n",
       " ('pains', 6),\n",
       " ('far', 6),\n",
       " ('bars', 6),\n",
       " ('cut', 6),\n",
       " ('rate', 6),\n",
       " ('pecans', 6),\n",
       " ('supplements', 6),\n",
       " ('think', 6),\n",
       " ('shakes', 6),\n",
       " ('morning', 6),\n",
       " ('sickness', 6),\n",
       " ('threw', 6),\n",
       " ('possible', 6),\n",
       " ('moldy', 6),\n",
       " ('mixed', 6),\n",
       " ('major', 6),\n",
       " ('breakouts', 6),\n",
       " ('hurt', 6),\n",
       " ('into', 6),\n",
       " ('concerned', 6),\n",
       " ('envelope', 6),\n",
       " ('opinion', 6),\n",
       " ('contaminated', 6),\n",
       " ('tasteless', 6),\n",
       " ('spend', 6),\n",
       " ('constipated', 6),\n",
       " ('label', 6),\n",
       " ('packing', 6),\n",
       " ('nauseating', 6),\n",
       " ('biotin', 6),\n",
       " ('problem', 6),\n",
       " ('wash', 6),\n",
       " ('tampering', 6),\n",
       " ('levels', 6),\n",
       " ('multi', 6),\n",
       " ('painful', 6),\n",
       " ('poisoning', 6),\n",
       " ('comparable', 6),\n",
       " ('gain', 6),\n",
       " ('counterfeit', 6),\n",
       " ('then', 6),\n",
       " ('near', 6),\n",
       " ('gag', 6),\n",
       " ('gold', 6),\n",
       " ('vegetarian', 6),\n",
       " ('ships', 6),\n",
       " ('unbearably', 6),\n",
       " ('unflavored', 6),\n",
       " ('blends', 6),\n",
       " ('unexplained', 6),\n",
       " ('multiple', 5),\n",
       " ('lots', 5),\n",
       " ('entire', 5),\n",
       " ('until', 5),\n",
       " ('hopes', 5),\n",
       " ('packages', 5),\n",
       " ('fragrance', 5),\n",
       " ('digestion', 5),\n",
       " ('maple', 5),\n",
       " ('impossible', 5),\n",
       " ('indigestion', 5),\n",
       " ('boost', 5),\n",
       " ('dosage', 5),\n",
       " ('throw', 5),\n",
       " ('trip', 5),\n",
       " ('cans', 5),\n",
       " ('pick', 5),\n",
       " ('china', 5),\n",
       " ('grape', 5),\n",
       " ('diaper', 5),\n",
       " ('tuna', 5),\n",
       " ('row', 5),\n",
       " ('muscle', 5),\n",
       " ('changes', 5),\n",
       " ('cup', 5),\n",
       " ('unusable', 5),\n",
       " ('recieved', 5),\n",
       " ('salted', 5),\n",
       " ('digestive', 5),\n",
       " ('matters', 5),\n",
       " ('update', 5),\n",
       " ('satisfied', 5),\n",
       " ('son', 5),\n",
       " ('honest', 5),\n",
       " ('nails', 5),\n",
       " ('looking', 5),\n",
       " ('runs', 5),\n",
       " ('peach', 5),\n",
       " ('bloated', 5),\n",
       " ('created', 5),\n",
       " ('times', 5),\n",
       " ('said', 5),\n",
       " ('french', 5),\n",
       " ('turns', 5),\n",
       " ('labeling', 5),\n",
       " ('honey', 5),\n",
       " ('drying', 5),\n",
       " ('stomachache', 5),\n",
       " ('omega', 5),\n",
       " ('itching', 5),\n",
       " ('while', 5),\n",
       " ('acidic', 5),\n",
       " ('acceptable', 5),\n",
       " ('apart', 5),\n",
       " ('pregnancy', 5),\n",
       " ('blech', 5),\n",
       " ('amount', 5),\n",
       " ('significant', 5),\n",
       " ('having', 5),\n",
       " ('under', 5),\n",
       " ('chewable', 5),\n",
       " ('hope', 5),\n",
       " ('march', 5),\n",
       " ('trying', 5),\n",
       " ('policy', 5),\n",
       " ('oyster', 5),\n",
       " ('shellfish', 5),\n",
       " ('ship', 5),\n",
       " ('returnable', 5),\n",
       " ('amounts', 5),\n",
       " ('tons', 5),\n",
       " ('excessive', 5),\n",
       " ('going', 5),\n",
       " ('smaller', 5),\n",
       " ('rough', 5),\n",
       " ('mild', 5),\n",
       " ('blended', 5),\n",
       " ('truly', 5),\n",
       " ('egg', 5),\n",
       " ('already', 5),\n",
       " ('nature', 5),\n",
       " ('powdery', 5),\n",
       " ('magnesium', 5),\n",
       " ('trimester', 5),\n",
       " ('sometimes', 5),\n",
       " ('lacks', 5),\n",
       " ('black', 5),\n",
       " ('brain', 5),\n",
       " ('swallowing', 5),\n",
       " ('cant', 5),\n",
       " ('medicine', 5),\n",
       " ('separates', 5),\n",
       " ('costs', 5),\n",
       " ('wonderful', 5),\n",
       " ('anymore', 5),\n",
       " ('canister', 5),\n",
       " ('smelly', 5),\n",
       " ('blob', 5),\n",
       " ('coating', 5),\n",
       " ('through', 5),\n",
       " ('pissed', 5),\n",
       " ('gluten', 5),\n",
       " ('mask', 5),\n",
       " ('general', 5),\n",
       " ('gummie', 5),\n",
       " ('knockoff', 5),\n",
       " ('dairy', 5),\n",
       " ('questionable', 5),\n",
       " ('harsher', 5),\n",
       " ('taffy', 5),\n",
       " ('edible', 5),\n",
       " ('heavy', 5),\n",
       " ('pieces', 5),\n",
       " ('ammonia', 4),\n",
       " ('calories', 4),\n",
       " ('ibs', 4),\n",
       " ('another', 4),\n",
       " ('second', 4),\n",
       " ('clump', 4),\n",
       " ('urine', 4),\n",
       " ('bloating', 4),\n",
       " ('unscented', 4),\n",
       " ('mean', 4),\n",
       " ('promised', 4),\n",
       " ('maltodextrin', 4),\n",
       " ('heat', 4),\n",
       " ('stung', 4),\n",
       " ('quickly', 4),\n",
       " ('vendor', 4),\n",
       " ('natures', 4),\n",
       " ('bounty', 4),\n",
       " ('optimum', 4),\n",
       " ('infused', 4),\n",
       " ('blah', 4),\n",
       " ('shattered', 4),\n",
       " ('managing', 4),\n",
       " ('flavour', 4),\n",
       " ('calcium', 4),\n",
       " ('vivid', 4),\n",
       " ('according', 4),\n",
       " ('quick', 4),\n",
       " ('deals', 4),\n",
       " ('nail', 4),\n",
       " ('selling', 4),\n",
       " ('highly', 4),\n",
       " ('important', 4),\n",
       " ('site', 4),\n",
       " ('feeling', 4),\n",
       " ('described', 4),\n",
       " ('nose', 4),\n",
       " ('caramel', 4),\n",
       " ('solimo', 4),\n",
       " ('future', 4),\n",
       " ('reason', 4),\n",
       " ('daughter', 4),\n",
       " ('storage', 4),\n",
       " ('fully', 4),\n",
       " ('options', 4),\n",
       " ('cheaper', 4),\n",
       " ('directly', 4),\n",
       " ('settles', 4),\n",
       " ('tiny', 4),\n",
       " ('properly', 4),\n",
       " ('aggressive', 4),\n",
       " ('leaking', 4),\n",
       " ('shit', 4),\n",
       " ('lol', 4),\n",
       " ('burning', 4),\n",
       " ('power', 4),\n",
       " ('own', 4),\n",
       " ('anxiety', 4),\n",
       " ('stolen', 4),\n",
       " ('must', 4),\n",
       " ('kept', 4),\n",
       " ('praline', 4),\n",
       " ('content', 4),\n",
       " ('ascorbic', 4),\n",
       " ('fall', 4),\n",
       " ('hypoallergenic', 4),\n",
       " ('picture', 4),\n",
       " ('replace', 4),\n",
       " ('kinda', 4),\n",
       " ('sell', 4),\n",
       " ('clay', 4),\n",
       " ('batter', 4),\n",
       " ('uses', 4),\n",
       " ('exp', 4),\n",
       " ('inaccurate', 4),\n",
       " ('thanks', 4),\n",
       " ('reactions', 4),\n",
       " ('overrated', 4),\n",
       " ('heart', 4),\n",
       " ('tho', 4),\n",
       " ('lacking', 4),\n",
       " ('tart', 4),\n",
       " ('supposed', 4),\n",
       " ('massive', 4),\n",
       " ('irritated', 4),\n",
       " ('changing', 4),\n",
       " ('bait', 4),\n",
       " ('part', 4),\n",
       " ('live', 4),\n",
       " ('tangerine', 4),\n",
       " ('strength', 4),\n",
       " ('naturally', 4),\n",
       " ('funny', 4),\n",
       " ('says', 4),\n",
       " ('tear', 4),\n",
       " ('stop', 4),\n",
       " ('spray', 4),\n",
       " ('concept', 4),\n",
       " ('sad', 4),\n",
       " ('brown', 4),\n",
       " ('seen', 4),\n",
       " ('pink', 4),\n",
       " ('elderberry', 4),\n",
       " ('recent', 4),\n",
       " ('horrific', 4),\n",
       " ('monster', 4),\n",
       " ('unfortunately', 4),\n",
       " ('initial', 4),\n",
       " ('extreme', 4),\n",
       " ('unlike', 4),\n",
       " ('reviewers', 4),\n",
       " ('treat', 4),\n",
       " ('almonds', 4),\n",
       " ('air', 4),\n",
       " ('ones', 4),\n",
       " ('women', 4),\n",
       " ('term', 4),\n",
       " ('upsets', 4),\n",
       " ('sweetened', 4),\n",
       " ('usps', 4),\n",
       " ('carbs', 4),\n",
       " ('toxicity', 4),\n",
       " ('cholesterol', 4),\n",
       " ('type', 4),\n",
       " ('grainy', 4),\n",
       " ('lemonade', 4),\n",
       " ('facial', 4),\n",
       " ('jar', 4),\n",
       " ('guys', 4),\n",
       " ('play', 4),\n",
       " ('few', 4),\n",
       " ('blood', 4),\n",
       " ('unhealthy', 4),\n",
       " ('happened', 4),\n",
       " ('shake', 4),\n",
       " ('case', 4),\n",
       " ('natal', 4),\n",
       " ('foods', 4),\n",
       " ('failure', 4),\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_dict.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>useless</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trash</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not</td>\n",
       "      <td>2041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>buy</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>these</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  freq\n",
       "0  useless    25\n",
       "1    trash    15\n",
       "2      not  2041\n",
       "3      buy   255\n",
       "4    these   142"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = pd.DataFrame.from_dict(word_dict, orient='index')\n",
    "\n",
    "word_freq = word_freq.reset_index()\n",
    "word_freq.columns = ['word','freq']\n",
    "word_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = train['topic'].astype('category').cat.codes\n",
    "train['target'] = train['target'].astype('int')\n",
    "\n",
    "def get_mapping(df, col_name):\n",
    "    cat_codes = df[col_name].astype('category')\n",
    "    \n",
    "    class_mapping = {}\n",
    "    i = 0\n",
    "    for col in cat_codes.cat.categories:\n",
    "        class_mapping[col] = i\n",
    "        i += 1\n",
    "    \n",
    "    class_mapping_reverse = {}\n",
    "    for key, value in class_mapping.items():\n",
    "        class_mapping_reverse[value] = key\n",
    "\n",
    "    return class_mapping, class_mapping_reverse\n",
    "\n",
    "cl_map, cl_map_inv = get_mapping(train, 'topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "\n",
    "def tfidf_feature(train, test, col_name, min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=3, stopwords='english', \n",
    "                  n_component=120, decom_flag=False, which_method='svd', \n",
    "                  max_features=None, feat_col_name='svd'):\n",
    "    \"\"\"return tfidf feature\n",
    "    Args:\n",
    "        train, test: dataframe\n",
    "        col_name: column name of text feature\n",
    "        min_df: if Int, then it represent count of the minimum words in corpus (remove very rare word)\n",
    "        analyzer: [‘word’, ‘char’]\n",
    "        ngram: max range of ngram\n",
    "        token_pattern: [using: r'\\w{1,}'] [by default: '(?u)\\b\\w\\w+\\b']\n",
    "        stopwords: ['english' or customized by remove specific words]\n",
    "        n_component: n_component of svd feature transform\n",
    "        decom_flag: Wheteher to run svd/nmf on top of that or not (by default: False)\n",
    "        which_method: which to run [svd or nmf] on top of tfidf (by default: False)\n",
    "        max_features: max no of features to keep, based on frequency. It will keep words with higher freq\n",
    "    return:\n",
    "        Transformed feature space of the text data, as well as tfidf function instance\n",
    "        if svd_flag== True : train_tf, test_tf, tfv, svd\n",
    "        else : train_tf, test_tf, tfv\n",
    "    example:\n",
    "        train_tfv, test_tfv, tfv = tfidf_feature(X_train, X_test, ['text'], min_df=3)\n",
    "        train_svd, test_svd, complete_tfv, tfv, svd = tfidf_feature(X_train, X_test, ['text'], \n",
    "            min_df=3, svd_component=3, svd_flag=True)\n",
    "\n",
    "    \"\"\"\n",
    "    tfv = TfidfVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, max_df=1.0, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                stop_words = stopwords)\n",
    "\n",
    "    complete_df = pd.concat([train[col_name], test[col_name]], axis=0)\n",
    "#         return complete_df\n",
    "#         print(complete_df.shape, complete_df.columns)\n",
    "\n",
    "    tfv.fit(list(complete_df[:].values))\n",
    "\n",
    "    if decom_flag is False:\n",
    "        train_tfv =  tfv.transform(train[col_name].values.ravel()) \n",
    "        test_tfv  = tfv.transform(test[col_name].values.ravel())\n",
    "\n",
    "        del complete_df\n",
    "        gc.collect()\n",
    "        return train_tfv, test_tfv, tfv\n",
    "    else:\n",
    "        complete_tfv = tfv.transform(complete_df[:].values.ravel())\n",
    "        \n",
    "        if which_method is 'svd':\n",
    "            svd = TruncatedSVD(n_components=n_component)\n",
    "            svd.fit(complete_tfv)\n",
    "            complete_dec = svd.transform(complete_tfv)\n",
    "        else:\n",
    "            nmf = NMF(n_components=n_component, random_state=1234, alpha=0, l1_ratio=0)\n",
    "            nmf.fit(complete_tfv)            \n",
    "            complete_dec = nmf.fit_transform(complete_tfv)            \n",
    "        \n",
    "        \n",
    "        complete_dec = pd.DataFrame(data=complete_dec)\n",
    "        complete_dec.columns = [feat_col_name+'_'+str(i) for i in range(n_component)]\n",
    "\n",
    "        train_dec = complete_dec.iloc[:train.shape[0]]\n",
    "        test_dec = complete_dec.iloc[train.shape[0]:].reset_index(drop=True)\n",
    "\n",
    "        del complete_dec, complete_df\n",
    "        gc.collect()\n",
    "        print(\"=\"*15, \" done \", \"=\"*15)\n",
    "        return train_dec, test_dec, complete_tfv, tfv\n",
    "\n",
    "def countvect_feature(train, test, col_name, min_df=3, \n",
    "                      analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                      ngram=3, stopwords='english', max_features=None):\n",
    "    \"\"\"return CountVectorizer feature\n",
    "    Args:\n",
    "        train, test: dataset\n",
    "        col_name: columns name of the text feature\n",
    "        min_df: if Int, then it represent count of the minimum words in corpus (remove very rare word)\n",
    "        analyzer: [‘word’, ‘char’]\n",
    "        ngram: max range of ngram\n",
    "        token_pattern: [using: r'\\w{1,}'] [by default: '(?u)\\b\\w\\w+\\b']\n",
    "        stopwords: ['english' or customized by remove specific words]\n",
    "        max_features: max no of features to keep, based on frequency. It will keep words with higher freq\n",
    "    return:\n",
    "        Count feature space of the text data, as well as its function instance\n",
    "    \"\"\"\n",
    "    ctv = CountVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                stop_words = stopwords)\n",
    "\n",
    "    complete_df = pd.concat([train[col_name], test[col_name]], axis=0)\n",
    "    ctv.fit(list(complete_df[:].values))\n",
    "\n",
    "    train_tf =  ctv.transform(train[col_name].values.ravel()) \n",
    "    test_tf  = ctv.transform(test[col_name].values.ravel())\n",
    "\n",
    "    del complete_df\n",
    "    gc.collect()\n",
    "    return train_tf, test_tf, ctv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_vectorizer(df, col_name, min_df=3, analyzer='word', stopwords='english', \n",
    "                     token_pattern=r'\\w{1,}', ngram=3, max_features=None):\n",
    "    ctv = CountVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                stop_words = stopwords)\n",
    "\n",
    "    ctv.fit(list(df[col_name].values))\n",
    "\n",
    "    df_new =  ctv.transform(df[col_name].values.ravel()) \n",
    "    return df_new\n",
    "\n",
    "\n",
    "def get_tfidf_feature(df, col_name, min_df=3, analyzer='word', stopwords='english',\n",
    "                  token_pattern=r'\\w{1,}', ngram=3, max_features=None):\n",
    "\n",
    "    tfv = TfidfVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, max_df=1.0, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                stop_words = stopwords)\n",
    "\n",
    "    tfv.fit(list(df[col_name].values))\n",
    "    df_new =  tfv.transform(df[col_name].values.ravel()) \n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8512, 3064)\n",
      "(8512, 2791)\n",
      "(8512, 2571)\n",
      "(8512, 2401)\n",
      "(8512, 2214)\n"
     ]
    }
   ],
   "source": [
    "count_vect_text = []\n",
    "for ngram in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]:\n",
    "#     if ngram < 11 : continue\n",
    "    cvect = get_count_vectorizer(df, 'text', ngram)\n",
    "    print(cvect.shape)\n",
    "    count_vect_text.append(cvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8512, 5102)\n",
      "(8512, 24310)\n",
      "(8512, 40712)\n",
      "(8512, 56071)\n",
      "(8512, 70887)\n",
      "(8512, 85207)\n",
      "(8512, 99043)\n",
      "(8512, 112402)\n",
      "(8512, 125290)\n",
      "(8512, 137714)\n"
     ]
    }
   ],
   "source": [
    "tfidf_text_store = []\n",
    "for ngram in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    tfidf = get_tfidf_feature(df, 'text', ngram=ngram)\n",
    "    print(tfidf.shape)\n",
    "    tfidf_text_store.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count-vect 0          acc: 0.3081\n",
      "count-vect 1          acc: 0.3094\n",
      "count-vect 2          acc: 0.3725\n",
      "count-vect 3          acc: 0.4101\n",
      "count-vect 4          acc: 0.4047\n",
      "count-vect 5          acc: 0.4235\n",
      "count-vect 6          acc: 0.4201\n",
      "count-vect 7          acc: 0.4235\n",
      "count-vect 8          acc: 0.4396\n",
      "count-vect 9          acc: 0.4215\n",
      "count-vect 10         acc: 0.4329\n",
      "count-vect 11         acc: 0.4309\n",
      "count-vect 12         acc: 0.4101\n",
      "count-vect 13         acc: 0.4201\n",
      "count-vect 14         acc: 0.4470\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(count_vect_text):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = MultinomialNB().fit(X_train, Y_train)\n",
    "    print('count-vect {:<10} acc: {:.4f}'.format(str(i), clf.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf 0          acc: 0.3557\n",
      "tfidf 1          acc: 0.3342\n",
      "tfidf 2          acc: 0.3315\n",
      "tfidf 3          acc: 0.3235\n",
      "tfidf 4          acc: 0.3094\n",
      "tfidf 5          acc: 0.3067\n",
      "tfidf 6          acc: 0.3128\n",
      "tfidf 7          acc: 0.2993\n",
      "tfidf 8          acc: 0.3181\n",
      "tfidf 9          acc: 0.3154\n",
      "==============================\n",
      "count-vect 0          acc: 0.3081\n",
      "count-vect 1          acc: 0.3295\n",
      "count-vect 2          acc: 0.3497\n",
      "count-vect 3          acc: 0.3879\n",
      "count-vect 4          acc: 0.4013\n",
      "count-vect 5          acc: 0.4188\n",
      "count-vect 6          acc: 0.4195\n",
      "count-vect 7          acc: 0.4383\n",
      "count-vect 8          acc: 0.4416\n",
      "count-vect 9          acc: 0.4436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split    \n",
    "\n",
    "for i, data in enumerate(tfidf_text_store):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = MultinomialNB().fit(X_train, Y_train)\n",
    "    print('tfidf {:<10} acc: {:.4f}'.format(str(i), clf.score(X_test, Y_test)))\n",
    "\n",
    "print(\"=\"*30)\n",
    "for i, data in enumerate(count_vect_text):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = MultinomialNB().fit(X_train, Y_train)\n",
    "    print('count-vect {:<10} acc: {:.4f}'.format(str(i), clf.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8512,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(count_vect_text[-1])\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I use chia seed in my protein shakes. These ta...</td>\n",
       "      <td>Bad tast</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I use chia seed in my protein shakes. These ta...</td>\n",
       "      <td>Bad tast</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don’t waste your money.</td>\n",
       "      <td>No change. No results.</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I use the book 'Fortify Your Life' by Tieraona...</td>\n",
       "      <td>Good Vegan Choice, Poor Non Vegan Choice</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I use the book 'Fortify Your Life' by Tieraona...</td>\n",
       "      <td>Good Vegan Choice, Poor Non Vegan Choice</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review Text  \\\n",
       "0  I use chia seed in my protein shakes. These ta...   \n",
       "1  I use chia seed in my protein shakes. These ta...   \n",
       "2                            Don’t waste your money.   \n",
       "3  I use the book 'Fortify Your Life' by Tieraona...   \n",
       "4  I use the book 'Fortify Your Life' by Tieraona...   \n",
       "\n",
       "                               Review Title             topic  \n",
       "0                                  Bad tast  Bad Taste/Flavor  \n",
       "1                                  Bad tast  Bad Taste/Flavor  \n",
       "2                    No change. No results.     Not Effective  \n",
       "3  Good Vegan Choice, Poor Non Vegan Choice       Ingredients  \n",
       "4  Good Vegan Choice, Poor Non Vegan Choice       Ingredients  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pd.DataFrame(data=pred[-test.shape[0]:], columns=['target'])\n",
    "sub1 = pd.concat([test, pred], axis=1)\n",
    "sub1['topic'] = sub1['target'].apply(lambda x: cl_map_inv[x])\n",
    "\n",
    "sub1.drop('target', axis=1, inplace=True)\n",
    "sub1.columns = sub.columns\n",
    "sub1.to_csv('submission/sub3_countvect_text_15.csv', index=None)\n",
    "sub1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8512, 14131)\n",
      "(8512, 6430)\n",
      "(8512, 2958)\n",
      "(8512, 1650)\n",
      "(8512, 1155)\n",
      "(8512, 914)\n",
      "(8512, 654)\n"
     ]
    }
   ],
   "source": [
    "count_vect = []\n",
    "for ngram in [1,2,3,4,5,6,8]:\n",
    "    cvect = get_count_vectorizer(df, 'title', ngram)\n",
    "    print(cvect.shape)\n",
    "    count_vect.append(cvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8512, 1183)\n",
      "(8512, 2369)\n",
      "(8512, 2958)\n",
      "(8512, 3240)\n",
      "(8512, 3382)\n",
      "(8512, 3452)\n",
      "(8512, 3484)\n",
      "(8512, 3496)\n"
     ]
    }
   ],
   "source": [
    "tfidf_store = []\n",
    "for ngram in [1,2,3,4,5,6,7,8]:\n",
    "    tfidf = get_tfidf_feature(df, 'title', ngram=ngram)\n",
    "    print(tfidf.shape)\n",
    "    tfidf_store.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5959, 3), (2553, 3))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_len = train.shape[0]\n",
    "ts_len = test.shape[0]\n",
    "# train.shape[0] + test.shape[0], df.shape[0]\n",
    "train1 = df.iloc[:tr_len]\n",
    "test1  = df.iloc[tr_len:]\n",
    "train1.shape, test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf 0 0.39731543624161075\n",
      "tfidf 1 0.3718120805369127\n",
      "tfidf 2 0.3818791946308725\n",
      "tfidf 3 0.3718120805369127\n",
      "tfidf 4 0.36711409395973155\n",
      "tfidf 5 0.374496644295302\n",
      "tfidf 6 0.38456375838926177\n",
      "tfidf 7 0.3838926174496644\n",
      "==============================\n",
      "count-vect 0 0.37785234899328857\n",
      "count-vect 1 0.348993288590604\n",
      "count-vect 2 0.3812080536912752\n",
      "count-vect 3 0.4120805369127517\n",
      "count-vect 4 0.4161073825503356\n",
      "count-vect 5 0.4120805369127517\n",
      "count-vect 6 0.4046979865771812\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split    \n",
    "\n",
    "for i, data in enumerate(tfidf_store):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = MultinomialNB().fit(X_train, Y_train)\n",
    "#     print(\"tfidf \"+str(i), clf.score(X_test, Y_test))\n",
    "    print('tfidf {:<10} acc: {:.4f}'.format(str(i), clf.score(X_test, Y_test)))\n",
    "\n",
    "    \n",
    "print(\"=\"*30)\n",
    "for i, data in enumerate(count_vect):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = MultinomialNB().fit(X_train, Y_train)\n",
    "#     print(\"count-vect \"+str(i), clf.score(X_test, Y_test))\n",
    "    print('count-vect {:<10} acc: {:.4f}'.format(str(i), clf.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf 0          acc: 0.5013\n",
      "tfidf 1          acc: 0.4946\n",
      "tfidf 2          acc: 0.4785\n",
      "tfidf 3          acc: 0.4698\n",
      "tfidf 4          acc: 0.4698\n",
      "tfidf 5          acc: 0.4718\n",
      "tfidf 6          acc: 0.4664\n",
      "tfidf 7          acc: 0.4564\n",
      "tfidf 8          acc: 0.4631\n",
      "tfidf 9          acc: 0.4483\n",
      "==============================\n",
      "count-vect 0          acc: 0.4477\n",
      "count-vect 1          acc: 0.4215\n",
      "count-vect 2          acc: 0.4523\n",
      "count-vect 3          acc: 0.4651\n",
      "count-vect 4          acc: 0.4678\n",
      "count-vect 5          acc: 0.4758\n",
      "count-vect 6          acc: 0.4705\n",
      "count-vect 7          acc: 0.4718\n",
      "count-vect 8          acc: 0.4758\n",
      "count-vect 9          acc: 0.4752\n",
      "count-vect 10         acc: 0.4973\n",
      "count-vect 11         acc: 0.4678\n",
      "count-vect 12         acc: 0.4725\n",
      "count-vect 13         acc: 0.4933\n",
      "count-vect 14         acc: 0.4792\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split    \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "    C=0.1, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "    random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "\n",
    "for i, data in enumerate(tfidf_text_store):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = logistic_reg.fit(X_train, Y_train)\n",
    "#     print(\"log-reg \"+str(i), clf.score(X_test, Y_test))\n",
    "    print('tfidf {:<10} acc: {:.4f}'.format(str(i), clf.score(X_test, Y_test)))\n",
    "\n",
    "print(\"=\"*30)\n",
    "for i, data in enumerate(count_vect_text):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = logistic_reg.fit(X_train, Y_train)\n",
    "#     print(\"log-reg \"+str(i), clf.score(X_test, Y_test))\n",
    "    print('count-vect {:<10} acc: {:.4f}'.format(str(i), clf.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf 0          acc: 0.4557\n",
      "tfidf 1          acc: 0.4584\n",
      "tfidf 2          acc: 0.4376\n",
      "tfidf 3          acc: 0.4362\n",
      "tfidf 4          acc: 0.4430\n",
      "tfidf 5          acc: 0.4544\n",
      "tfidf 6          acc: 0.4396\n",
      "tfidf 7          acc: 0.4349\n",
      "tfidf 8          acc: 0.4248\n",
      "tfidf 9          acc: 0.4450\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "stdc = StandardScaler()\n",
    "\n",
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, solver='lbfgs',\n",
    "    C=0.1, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "    random_state=1234, max_iter=100, multi_class='multinomial', verbose=0, n_jobs=4)\n",
    "\n",
    "for i, data in enumerate(tfidf_text_store):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "#     X_train = X_train.todense()\n",
    "#     X_test  = X_test.todense()\n",
    "#     stdc.fit(X_train)\n",
    "    clf = logistic_reg.fit(X_train, Y_train)\n",
    "#     print(\"log-reg \"+str(i), clf.score(X_test, Y_test))\n",
    "    print('tfidf {:<10} acc: {:.4f}'.format(str(i), clf.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf 0          acc: 0.4859\n",
      "tfidf 1          acc: 0.4893\n",
      "tfidf 2          acc: 0.4550\n",
      "tfidf 3          acc: 0.4779\n",
      "tfidf 4          acc: 0.4624\n",
      "tfidf 5          acc: 0.4839\n",
      "tfidf 6          acc: 0.4758\n",
      "tfidf 7          acc: 0.4812\n",
      "tfidf 8          acc: 0.4530\n",
      "tfidf 9          acc: 0.4544\n"
     ]
    }
   ],
   "source": [
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "    C=0.1, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "    random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "\n",
    "for i, data in enumerate(tfidf_text_store):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = logistic_reg.fit(X_train, Y_train)\n",
    "#     print(\"log-reg \"+str(i), clf.score(X_test, Y_test))\n",
    "    print('tfidf {:<10} acc: {:.4f}'.format(str(i), clf.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49261744966442955"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(loss='log', penalty='elasticnet', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, \n",
    "                        max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=4, \n",
    "                        random_state=1337, learning_rate='optimal', eta0=0.0, power_t=0.5,\n",
    "                        early_stopping=True, validation_fraction=0.25, n_iter_no_change=5, \n",
    "                        class_weight='balanced', warm_start=False, average=False, n_iter=1000)\n",
    "sgd_clf.fit(X_train, Y_train)\n",
    "sgd_clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1490,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = sgd_clf.predict(X_test)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fdd2ea81208>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(18, 4))\n",
    "sns.countplot(pred, color='k', ax=ax)\n",
    "sns.countplot(Y_test, color='r', ax=ax, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf 0          acc: 0.4866\n",
      "tfidf 1          acc: 0.4960\n",
      "tfidf 2          acc: 0.4826\n",
      "tfidf 3          acc: 0.5148\n",
      "tfidf 4          acc: 0.5034\n",
      "tfidf 5          acc: 0.4913\n",
      "tfidf 6          acc: 0.5020\n",
      "tfidf 7          acc: 0.4960\n",
      "tfidf 8          acc: 0.4886\n",
      "tfidf 9          acc: 0.5087\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(tfidf_text_store):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    \n",
    "    sgd_clf = SGDClassifier(loss='log', penalty='elasticnet', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, \n",
    "                        max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=4, \n",
    "                        random_state=1337, learning_rate='optimal', eta0=0.0, power_t=0.5,\n",
    "                        early_stopping=True, validation_fraction=0.25, n_iter_no_change=5, \n",
    "                        class_weight='balanced', warm_start=False, average=False, n_iter=1000)\n",
    "    sgd_clf.fit(X_train, Y_train)\n",
    "#     sgd_clf.score(X_test, Y_test)\n",
    "\n",
    "#     clf = logistic_reg.fit(X_train, Y_train)\n",
    "#     print(\"log-reg \"+str(i), clf.score(X_test, Y_test))\n",
    "    print('tfidf {:<10} acc: {:.4f}'.format(str(i), sgd_clf.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2553,), (2553, 2))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ = tfidf_text_store[4][:train.shape[0]]\n",
    "test_  = tfidf_text_store[4][train.shape[0]:]\n",
    "\n",
    "sgd_clf = SGDClassifier(loss='log', penalty='elasticnet', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, \n",
    "                    max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=4, \n",
    "                    random_state=1337, learning_rate='optimal', eta0=0.0, power_t=0.5,\n",
    "                    early_stopping=True, validation_fraction=0.25, n_iter_no_change=5, \n",
    "                    class_weight='balanced', warm_start=False, average=False, n_iter=1000)\n",
    "sgd_clf.fit(train_, train['target'])\n",
    "pred = sgd_clf.predict(test_)\n",
    "pred.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I use chia seed in my protein shakes. These ta...</td>\n",
       "      <td>Bad tast</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I use chia seed in my protein shakes. These ta...</td>\n",
       "      <td>Bad tast</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don’t waste your money.</td>\n",
       "      <td>No change. No results.</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I use the book 'Fortify Your Life' by Tieraona...</td>\n",
       "      <td>Good Vegan Choice, Poor Non Vegan Choice</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I use the book 'Fortify Your Life' by Tieraona...</td>\n",
       "      <td>Good Vegan Choice, Poor Non Vegan Choice</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review Text  \\\n",
       "0  I use chia seed in my protein shakes. These ta...   \n",
       "1  I use chia seed in my protein shakes. These ta...   \n",
       "2                            Don’t waste your money.   \n",
       "3  I use the book 'Fortify Your Life' by Tieraona...   \n",
       "4  I use the book 'Fortify Your Life' by Tieraona...   \n",
       "\n",
       "                               Review Title             topic  \n",
       "0                                  Bad tast  Bad Taste/Flavor  \n",
       "1                                  Bad tast  Bad Taste/Flavor  \n",
       "2                    No change. No results.     Not Effective  \n",
       "3  Good Vegan Choice, Poor Non Vegan Choice       Ingredients  \n",
       "4  Good Vegan Choice, Poor Non Vegan Choice       Ingredients  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pd.DataFrame(data=pred[-test.shape[0]:], columns=['target'])\n",
    "sub1 = pd.concat([test, pred], axis=1)\n",
    "sub1['topic'] = sub1['target'].apply(lambda x: cl_map_inv[x])\n",
    "\n",
    "sub1.drop('target', axis=1, inplace=True)\n",
    "sub1.columns = sub.columns\n",
    "sub1.to_csv('submission/sub4_tfidf4_sgd.csv', index=None)\n",
    "sub1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I use chia seed in my protein shakes. These ta...</td>\n",
       "      <td>Bad tast</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don’t waste your money.</td>\n",
       "      <td>No change. No results.</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I use the book 'Fortify Your Life' by Tieraona...</td>\n",
       "      <td>Good Vegan Choice, Poor Non Vegan Choice</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I used to be loyal customer to this brand. I h...</td>\n",
       "      <td>SMELL HORRIBLE!</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I haven’t received it yet.</td>\n",
       "      <td>Shipping</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I bought these for my girlfriend, who I couldn...</td>\n",
       "      <td>These suppliments don't work - No suppliment s...</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The almonds were sealed, but not dusted separa...</td>\n",
       "      <td>Heated damaged contents...Not dusted almonds, ...</td>\n",
       "      <td>Wrong Product received</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I really wanted to give these a try and notice...</td>\n",
       "      <td>Vitex</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I ordered Cherry Vanilla,  got BlackBerry. Cus...</td>\n",
       "      <td>Wrong description !!!  Or wrong product?</td>\n",
       "      <td>Wrong Product received</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Buyer beware: these vitamins are not technical...</td>\n",
       "      <td>These are not vegan, but \"X Brand\"'s Customer ...</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>My children weren't a fan of the flavors. They...</td>\n",
       "      <td>Lack in Flavor Department</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Horrible, gritty, disgusting.  I have tried it...</td>\n",
       "      <td>Disgusting</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>maybe good for you but I suggest they rework t...</td>\n",
       "      <td>sticky, thick and a ugly taste</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Not absorbent, not long.  Poise 6 is much,much...</td>\n",
       "      <td>Don't buy this for heavy duty use</td>\n",
       "      <td>Pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Very disappointed!  I made a mistake and order...</td>\n",
       "      <td>This is not the Organic all in one coconut alm...</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Was broken Upon delivery. The top broke off in...</td>\n",
       "      <td>Delivered broken</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>This product is essentially worthless. Does no...</td>\n",
       "      <td>Cheap ineffective knock off</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Note that I currently take Nature's Way Alive,...</td>\n",
       "      <td>Organic, vegan, gluten-free, and good price</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>I love that these pack so much into a vitamin....</td>\n",
       "      <td>Some things I love, others I really don't</td>\n",
       "      <td>Too big to swallow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>The top of the bottle was broken when I receiv...</td>\n",
       "      <td>Broken</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>This was my second time ordering these gummies...</td>\n",
       "      <td>Not as fresh</td>\n",
       "      <td>Expiry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Whoever made these massive things has never be...</td>\n",
       "      <td>Horse pills!</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Paid more to have shipped sooner and STILL don...</td>\n",
       "      <td>Terrible shipping!!!</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>I wanted to like this product because it's got...</td>\n",
       "      <td>Not so great for me</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>I didn’t receive coconut oil</td>\n",
       "      <td>Didn’t receive</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Unfortunately, I can't rate this item because ...</td>\n",
       "      <td>Did not receive item</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>I bought this to see if it would aid with my p...</td>\n",
       "      <td>Glad it has helped others.</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NASTY. Bought this mix as a less expensive alt...</td>\n",
       "      <td>Do not buy this yucky stuff.</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>I ordered Nature Made Magnesium.  Imagine my s...</td>\n",
       "      <td>Sent wrong product...</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>I RECEIVED THIS AND THE BOTTLE WAS BROKEN...  ...</td>\n",
       "      <td>I RECEIVED THIS AND THE BOTTLE WAS BROKEN&gt;&gt;&gt;</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>I was hoping for the best with this supplement...</td>\n",
       "      <td>Gave it three attempts, each time felt horribl...</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>We've bought these in the past and they work l...</td>\n",
       "      <td>Works BUT bought in June 2019 and they expire ...</td>\n",
       "      <td>Pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Pretty chalky, even after shaking.  Flavor is ...</td>\n",
       "      <td>Perfect for dairy intolerance</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Nature's Bounty GlowOn Beauty Multivitamins&lt;br...</td>\n",
       "      <td>No noticeable positive effects</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Not bad BAD but definitely not good. Weird tas...</td>\n",
       "      <td>Taste is weird</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>This makes both myself and my wife's face and/...</td>\n",
       "      <td>Makes face and arms itchy</td>\n",
       "      <td>Customer Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>I do not want to say the product may not work....</td>\n",
       "      <td>After intensive research...</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>This was ok just didn’t like the taste therefo...</td>\n",
       "      <td>Didn’t like the taste</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>These regularly leaked around the sides and th...</td>\n",
       "      <td>Not good!</td>\n",
       "      <td>Customer Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Taste nasty.</td>\n",
       "      <td>Don't buy</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Pros: This is a pretty good protein powder in ...</td>\n",
       "      <td>Birthday Cake tastes terrible.</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Couldn't tell a difference.  Overall I'd say d...</td>\n",
       "      <td>Doesn't work.</td>\n",
       "      <td>Customer Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Protein cakes were 9 months expired when it ar...</td>\n",
       "      <td>9 Months expired</td>\n",
       "      <td>Expiry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Totally gross. I used this product for many ye...</td>\n",
       "      <td>Bring back the old formula</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Flavor was way too sweet and had a weird caram...</td>\n",
       "      <td>Weird flavor</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Lost in transit.  \"X Brand\" sent me an email t...</td>\n",
       "      <td>Lost in transit</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>These are good for the price but it has a fish...</td>\n",
       "      <td>Smells Fishy</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Way too strong even taking one tablet too stro...</td>\n",
       "      <td>Get something else</td>\n",
       "      <td>Texture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Honestly sucked when I tried using for my body...</td>\n",
       "      <td>NOT FAINT AND DEFINITELY NOT STRONG</td>\n",
       "      <td>Smells Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Bad stomach pain. Do not take them. The heartb...</td>\n",
       "      <td>Don’t Purchase</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Let me start by saying I purchased this exact ...</td>\n",
       "      <td>THIS IS A FAKE PRODUCT. DO NOT BUY.</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Seller did not deliver on time.</td>\n",
       "      <td>Hard to rate when my two day delivery takes a ...</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>I took the first bottle even tho it had a chem...</td>\n",
       "      <td>Chemical smell and taste!</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Broken lid. Frustrating.</td>\n",
       "      <td>BROKEN LID</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>I have not had unflavored whey in some time. W...</td>\n",
       "      <td>It's a good price for isolate but the taste is...</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>This does not get a three star rating from me ...</td>\n",
       "      <td>I Think 3 Stars Is What All Protein Powders Are</td>\n",
       "      <td>Too Sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>I ordered the chocolate one but they sent me t...</td>\n",
       "      <td>Wrong flavor</td>\n",
       "      <td>Wrong Product received</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Did not do much for me. I found no difference ...</td>\n",
       "      <td>Did not work for me</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>I don't like this brand.  Cayenne is WEAK.  Th...</td>\n",
       "      <td>Trash</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>I never received my order. You kept sending me...</td>\n",
       "      <td>Never got my order right</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>I love this brand of protein. It is generally ...</td>\n",
       "      <td>Mixes well. But, the maple flavor was not for me.</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>I have been happily using the unscented wipes ...</td>\n",
       "      <td>New wipes cause irritation</td>\n",
       "      <td>Color and texture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>As of this review this bottle of adult vitamin...</td>\n",
       "      <td>Does Not Contain D3</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Maybe worth it if regular vitamins really affe...</td>\n",
       "      <td>yuck</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>I had a very bad experience with this company....</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>So I've been using these diapers for about a m...</td>\n",
       "      <td>Soft &amp; Cute</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>It’s my 4th day taking these. So far I’m not s...</td>\n",
       "      <td>Not working for me</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>This female nutrient sugar is not the same as ...</td>\n",
       "      <td>taste bad</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>My mom LOVES the chocolate shake, and it has a...</td>\n",
       "      <td>WRONG FLAVOR OVER AND OVER AGAIN!!!</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>I was excited to get this product for my daugh...</td>\n",
       "      <td>Item did not arrive cold</td>\n",
       "      <td>Customer Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Taste not good</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>I did not receive this item.</td>\n",
       "      <td>Did not receive</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Did not work.</td>\n",
       "      <td>Poor sleep</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Does not dissolve at all. Even in a NutriBulle...</td>\n",
       "      <td>Hard to dissolve and flavor is bad</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>I haven't even received my product and I order...</td>\n",
       "      <td>This is BS</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>This is the weirdest taste gummy I have ever  ...</td>\n",
       "      <td>So artificial fruit flavor</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Wanted to take the Fish oil for plantar facili...</td>\n",
       "      <td>It’s too BIG to swallow</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Gross</td>\n",
       "      <td>Gross</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Disgusting flavor, waste of my money. Love the...</td>\n",
       "      <td>Flavor</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>I haven't noticed anything different in a posi...</td>\n",
       "      <td>Harsh on my digestive system</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>I just got these in today and was very excited...</td>\n",
       "      <td>Seal broken</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>500 mg is 142 times the amount an adult female...</td>\n",
       "      <td>Ended up in the ER after one pill!</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>I noticed no change or improvement or reason t...</td>\n",
       "      <td>Does Nothing</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>I bought this product back in December and I h...</td>\n",
       "      <td>Horrible taste</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>I'm sure it does what it's supposed to... but ...</td>\n",
       "      <td>Good vitamin, bad pill...</td>\n",
       "      <td>Texture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Used these before and during my first pregnanc...</td>\n",
       "      <td>Fishy</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>It is very hard to swallow. Kept getting stuck...</td>\n",
       "      <td>Really hard to swallow</td>\n",
       "      <td>Texture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>This stuff is so clumpy and it gets gross real...</td>\n",
       "      <td>Does not dissolve</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>The bear design is cute. And I like that it's ...</td>\n",
       "      <td>My Kids Don't Like the Taste</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Ordered two, package arrived not sealed/taped ...</td>\n",
       "      <td>Didn't receive</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>One of the two jars had a broken seal and ther...</td>\n",
       "      <td>One of the jars is unusable</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>This stuff is gross!! open the bottle and it s...</td>\n",
       "      <td>Don’t buy!!!</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>This gets 2.4 stars from me.  This is very moi...</td>\n",
       "      <td>Moisturizing but greasy, greasy, greasy  &amp; tak...</td>\n",
       "      <td>Smells Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Has no discernable difference from lower price...</td>\n",
       "      <td>It is ok. Not competitively priced</td>\n",
       "      <td>Pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>I do not like the taste of this powder at all....</td>\n",
       "      <td>hate the taste</td>\n",
       "      <td>Color and texture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Gave my wife constipation after taking.  This ...</td>\n",
       "      <td>Caused Constipation despite clear promise othe...</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>I ordered 4 bottles, but only received 3bottles.</td>\n",
       "      <td>Missing product</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Took it at 7 am now at 1130 am I am so flushed...</td>\n",
       "      <td>Caused Delayed aggressive flush</td>\n",
       "      <td>Texture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>this does not help me at all as far as energy ...</td>\n",
       "      <td>quality</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>For more than 15 years I've been buying this f...</td>\n",
       "      <td>They changed the formula!!!</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2411</th>\n",
       "      <td>I'm not sure if I got a bad batch or what, but...</td>\n",
       "      <td>Weird, melty texture</td>\n",
       "      <td>Color and texture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2414</th>\n",
       "      <td>I did notice any difference in my skin.</td>\n",
       "      <td>No difference in skin</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2415</th>\n",
       "      <td>Never received the product. Shipping container...</td>\n",
       "      <td>Never received order</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2417</th>\n",
       "      <td>I am very disappointed with the product as the...</td>\n",
       "      <td>bottle came unsealed</td>\n",
       "      <td>Texture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>it mixed well but it was very hard removing th...</td>\n",
       "      <td>mixes well but hard to remove from bottle</td>\n",
       "      <td>Color and texture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2420</th>\n",
       "      <td>economic health supplement&lt;br /&gt;the package wr...</td>\n",
       "      <td>economic health supplement</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2421</th>\n",
       "      <td>Did not receive order. Waiting to hear back fr...</td>\n",
       "      <td>Order Not Received</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2422</th>\n",
       "      <td>These seem to be working well for a lot of peo...</td>\n",
       "      <td>Texture is awful, but otherwise ok</td>\n",
       "      <td>Color and texture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2424</th>\n",
       "      <td>I am determined to finish this due to the pric...</td>\n",
       "      <td>If you can get past the taste and texture...gr...</td>\n",
       "      <td>Color and texture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>This contact lens solution consistently dries ...</td>\n",
       "      <td>Not good for long term contact lens storage</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>Didn't work.  Way too low probiotics count.  S...</td>\n",
       "      <td>Nope</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2427</th>\n",
       "      <td>I know elderberry can be sour, but I could har...</td>\n",
       "      <td>Comparable to war head candy</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2428</th>\n",
       "      <td>This item never arrived and \"X Brand\" informed...</td>\n",
       "      <td>Never arrived</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2429</th>\n",
       "      <td>This is my 2nd bottle but it was broken when I...</td>\n",
       "      <td>Pills are good but not the bottle</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2430</th>\n",
       "      <td>I ordered the wrong thing and thought I would ...</td>\n",
       "      <td>Smells like rancid rotting fruit</td>\n",
       "      <td>Smells Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>I had been taking the Kirkland general daily m...</td>\n",
       "      <td>Oddly Low Calcium</td>\n",
       "      <td>Too big to swallow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>Very gross flavor! I couldnt make more than on...</td>\n",
       "      <td>Taste gross</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>I received a different looking product, does n...</td>\n",
       "      <td>Not getting what's shown on my order</td>\n",
       "      <td>Wrong Product received</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>I use these for dog wraps and they don’t work....</td>\n",
       "      <td>Smell goes through. Yech!</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>Arrived COMPLETELY all melted together. Useless</td>\n",
       "      <td>Melted and useless</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>The first time buying this brand and first tim...</td>\n",
       "      <td>Not good. Makes me sick</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>Made me sick to my tummy.  Start slow and buil...</td>\n",
       "      <td>You need 7 pills to take as prescribed!! Yikes...</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>I had been using the Vega Essentials drink and...</td>\n",
       "      <td>Like the quality and nutrition of the shake. N...</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>Have used this B12 spray for a little over a m...</td>\n",
       "      <td>No change whatsoever</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>I love using grape seed oil and I love this br...</td>\n",
       "      <td>Arrived broken and made a mess</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>I like this product but the last one I receive...</td>\n",
       "      <td>Bad Batch</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445</th>\n",
       "      <td>I ordered these to send it to my grandma and t...</td>\n",
       "      <td>Glass bottles without any bubble wrap.</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>While they're ok for occasional use for sleep,...</td>\n",
       "      <td>Just ok.</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>Please don't use these it cause my throat to s...</td>\n",
       "      <td>DON'T DO IT!!! Causes throat swelling!</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>I mix one scoop of this with 8oz of water, 5-6...</td>\n",
       "      <td>Weird aftertaste makes this a NO for me</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450</th>\n",
       "      <td>Only tried one scoop, but will throw the whole...</td>\n",
       "      <td>The taste is so bad I had to spit it out...</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>Do not buy this product unless you love the fa...</td>\n",
       "      <td>horrible flavor and texture</td>\n",
       "      <td>Too Sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>Lacks flavor, but mixes well.</td>\n",
       "      <td>Not as good as others</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>Tracking says item delivered yesterday, but no...</td>\n",
       "      <td>Ordered and paid.  Not received</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>Value is good because they ate terrible to swa...</td>\n",
       "      <td>Don't buy they get stuck in your throat</td>\n",
       "      <td>Too big to swallow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2459</th>\n",
       "      <td>The pills have a horrible taste and smell whic...</td>\n",
       "      <td>Horrible packaging</td>\n",
       "      <td>Smells Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>These things are huge. I will not be ordering ...</td>\n",
       "      <td>Really big capsules</td>\n",
       "      <td>Too big to swallow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>The packaging doesn’t pair well with the produ...</td>\n",
       "      <td>Good product, but weird packaging issue</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>Would not buy again.  They leak all the time !!</td>\n",
       "      <td>Leak all the time !!</td>\n",
       "      <td>Customer Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>but way too high priced</td>\n",
       "      <td>same as costco</td>\n",
       "      <td>Pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>This flavor tastes like lightly salted dirt. G...</td>\n",
       "      <td>Don’t buy the salted caramel</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>The wrong item was delivered and I am being to...</td>\n",
       "      <td>Wrong item sent! Do not order from seller</td>\n",
       "      <td>Wrong Product received</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>Definitely not as good as the big brands unfor...</td>\n",
       "      <td>Sticks to my teeth</td>\n",
       "      <td>Too big to swallow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>The product was what I ordered and expected.  ...</td>\n",
       "      <td>Shipping Misinformation</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>I was so excited to find these, I bought 2 jar...</td>\n",
       "      <td>1 out of 10 is extremely bad!</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>This was the most disgusting kids vitamin! I c...</td>\n",
       "      <td>Gross taste</td>\n",
       "      <td>Customer Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>I did NOT receive 2 bottles for one. I texted ...</td>\n",
       "      <td>Be careful of deals that are too good to be true.</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>I just received my bottle yesterday and they e...</td>\n",
       "      <td>Expiration dates!</td>\n",
       "      <td>Expiry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>Didn't really notice any difference with the p...</td>\n",
       "      <td>I'd try something else,  didn't do anything fo...</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>I love all that is in this, but the texture is...</td>\n",
       "      <td>Great nutrients but terrible texture</td>\n",
       "      <td>Color and texture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>I am so pissed off! This thing was sent in the...</td>\n",
       "      <td>Are you serious!!!</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2482</th>\n",
       "      <td>Product arrived with cap broken and seal not i...</td>\n",
       "      <td>Seal and cap broken</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>Received the wrong item and currently still ha...</td>\n",
       "      <td>Wrong Product</td>\n",
       "      <td>Wrong Product received</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>I guess I am not deficient in dopamine. This s...</td>\n",
       "      <td>Gave me a headache</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>I am very disappointed.  I did not receive my ...</td>\n",
       "      <td>I did not receive my order!</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>The glycerin might be good, but the packaging ...</td>\n",
       "      <td>Bad packaging.</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2489</th>\n",
       "      <td>I have used this product before and it was fin...</td>\n",
       "      <td>lumpy texture</td>\n",
       "      <td>Hard to Chew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490</th>\n",
       "      <td>I'm not impressed by this, it had no effect on...</td>\n",
       "      <td>No effect, not impressed</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2491</th>\n",
       "      <td>In the pictures and bullet points they provide...</td>\n",
       "      <td>BEWARE: Misleading Label and Description</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2492</th>\n",
       "      <td>I have a pretty iron stomach, and my stomach n...</td>\n",
       "      <td>Makes you SUPER nauseated</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>I got these for my 73 year old mother who live...</td>\n",
       "      <td>These are not similar to Depends!</td>\n",
       "      <td>Too big to swallow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>Maybe I've got counterfeit or maybe they've ch...</td>\n",
       "      <td>Maybe I've got counterfeit or maybe they've ch...</td>\n",
       "      <td>Customer Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>These are nice and thick, but not very wet. I’...</td>\n",
       "      <td>Not wet enough</td>\n",
       "      <td>Pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>I tried to change all supplements my family ta...</td>\n",
       "      <td>Bad smell!</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>Bad allergic reaction</td>\n",
       "      <td>Bad  allergic reaction</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>Bad indigestion. Could not use and could not r...</td>\n",
       "      <td>Bad indigestion with product.</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2501</th>\n",
       "      <td>I didn’t like the fact that I received an empt...</td>\n",
       "      <td>Package was empty!</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>It worked for muscle growth..... but even thou...</td>\n",
       "      <td>It's okay.... I guess.</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>I've never had trouble swallowing pills -- I c...</td>\n",
       "      <td>Impossible to swallow!</td>\n",
       "      <td>Too big to swallow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>There is risk of cross contamination with milk...</td>\n",
       "      <td>Not allergy friendly</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>Even for protein shakes I just couldn’t drink ...</td>\n",
       "      <td>Gross</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>SIZE! TOO BIG!</td>\n",
       "      <td>HORSE PILLS</td>\n",
       "      <td>Too big to swallow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>Packaging was horrible! This should’ve been se...</td>\n",
       "      <td>Terrible packaging</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>Chocolate Creme flavor was absolutely not pala...</td>\n",
       "      <td>Undrinkable</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>This is ascorbic acid!! It’s a synthetic, most...</td>\n",
       "      <td>Synthetic ascorbic acid! BAD!!</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>Product was not delivered.</td>\n",
       "      <td>Did not receive product.</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>The product never arrived and I have yet to ge...</td>\n",
       "      <td>Don’t order from this company</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>The pills always go bad</td>\n",
       "      <td>Pills go bad</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>Too sweet</td>\n",
       "      <td>Ok product</td>\n",
       "      <td>Too Sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>The tablets are huge and coating peeled, makin...</td>\n",
       "      <td>Tried them, went back to Centrum.</td>\n",
       "      <td>Too big to swallow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>Some reviews said it smelled bad, and I think ...</td>\n",
       "      <td>Smells so bad</td>\n",
       "      <td>Smells Bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>I bought this August 2018. I barely used it wh...</td>\n",
       "      <td>Don’t waste your money!</td>\n",
       "      <td>Expiry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2526</th>\n",
       "      <td>Bought these for my daughter thinking it would...</td>\n",
       "      <td>Does not work</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2528</th>\n",
       "      <td>I bought these because I'm anemic late in my p...</td>\n",
       "      <td>Does NOT contain iron</td>\n",
       "      <td>Ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2529</th>\n",
       "      <td>Would give it a zero if I could. Never even re...</td>\n",
       "      <td>Don't waste your time</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>I’ve purchased Kirkland Olive Oil in the past:...</td>\n",
       "      <td>Product was Spoiled</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>The wild cherry flavor is enjoyable, albeit is...</td>\n",
       "      <td>Tasty but VERY Sweet - Not Sugar Coated</td>\n",
       "      <td>Too Sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>After 2nd use, I woke in middle of the night w...</td>\n",
       "      <td>Beware of stomach cramps</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>Big time overrated</td>\n",
       "      <td>Not effective</td>\n",
       "      <td>Too big to swallow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>This collagen is unflavored so the taste is pa...</td>\n",
       "      <td>Doesn't blend well.</td>\n",
       "      <td>Texture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>I wish I had read the reviews for the product ...</td>\n",
       "      <td>Tastes bad.</td>\n",
       "      <td>Too Sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>Rubbish</td>\n",
       "      <td>Doesn’t work</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>I tried half a scoop mixed in water and within...</td>\n",
       "      <td>Got sick within an hour after consuming it</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>This does not stir in easily or dissolve. It’s...</td>\n",
       "      <td>Look for another brand!!!!</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>All the bags in one of the rolls was miss cut ...</td>\n",
       "      <td>Poorly cut bags.</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2546</th>\n",
       "      <td>Product burst opened in the box and created a ...</td>\n",
       "      <td>Product packaging/ sealing needs improvement.</td>\n",
       "      <td>Packaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>This is the worse protein I've ever tried! The...</td>\n",
       "      <td>Taste is horrible</td>\n",
       "      <td>Too Sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2549</th>\n",
       "      <td>Very small and easy to swallow- no flavor at a...</td>\n",
       "      <td>Horrible stomach side effects</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>Good but it increases the bad cholesterol- LDL...</td>\n",
       "      <td>Increase LDL - bad cholesterol</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552</th>\n",
       "      <td>Will not buy the powder again.</td>\n",
       "      <td>Thick-like paste consistency</td>\n",
       "      <td>Customer Service</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1776 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Review Text  \\\n",
       "0     I use chia seed in my protein shakes. These ta...   \n",
       "2                               Don’t waste your money.   \n",
       "3     I use the book 'Fortify Your Life' by Tieraona...   \n",
       "5     I used to be loyal customer to this brand. I h...   \n",
       "9                            I haven’t received it yet.   \n",
       "10    I bought these for my girlfriend, who I couldn...   \n",
       "11    The almonds were sealed, but not dusted separa...   \n",
       "12    I really wanted to give these a try and notice...   \n",
       "14    I ordered Cherry Vanilla,  got BlackBerry. Cus...   \n",
       "16    Buyer beware: these vitamins are not technical...   \n",
       "18    My children weren't a fan of the flavors. They...   \n",
       "19    Horrible, gritty, disgusting.  I have tried it...   \n",
       "20    maybe good for you but I suggest they rework t...   \n",
       "22    Not absorbent, not long.  Poise 6 is much,much...   \n",
       "23    Very disappointed!  I made a mistake and order...   \n",
       "26    Was broken Upon delivery. The top broke off in...   \n",
       "27    This product is essentially worthless. Does no...   \n",
       "29    Note that I currently take Nature's Way Alive,...   \n",
       "31    I love that these pack so much into a vitamin....   \n",
       "35    The top of the bottle was broken when I receiv...   \n",
       "36    This was my second time ordering these gummies...   \n",
       "37    Whoever made these massive things has never be...   \n",
       "39    Paid more to have shipped sooner and STILL don...   \n",
       "40    I wanted to like this product because it's got...   \n",
       "42                         I didn’t receive coconut oil   \n",
       "43    Unfortunately, I can't rate this item because ...   \n",
       "44    I bought this to see if it would aid with my p...   \n",
       "45    NASTY. Bought this mix as a less expensive alt...   \n",
       "48    I ordered Nature Made Magnesium.  Imagine my s...   \n",
       "50    I RECEIVED THIS AND THE BOTTLE WAS BROKEN...  ...   \n",
       "51    I was hoping for the best with this supplement...   \n",
       "53    We've bought these in the past and they work l...   \n",
       "55    Pretty chalky, even after shaking.  Flavor is ...   \n",
       "57    Nature's Bounty GlowOn Beauty Multivitamins<br...   \n",
       "58    Not bad BAD but definitely not good. Weird tas...   \n",
       "61    This makes both myself and my wife's face and/...   \n",
       "62    I do not want to say the product may not work....   \n",
       "65    This was ok just didn’t like the taste therefo...   \n",
       "66    These regularly leaked around the sides and th...   \n",
       "68                                         Taste nasty.   \n",
       "69    Pros: This is a pretty good protein powder in ...   \n",
       "70    Couldn't tell a difference.  Overall I'd say d...   \n",
       "72    Protein cakes were 9 months expired when it ar...   \n",
       "73    Totally gross. I used this product for many ye...   \n",
       "75    Flavor was way too sweet and had a weird caram...   \n",
       "78    Lost in transit.  \"X Brand\" sent me an email t...   \n",
       "79    These are good for the price but it has a fish...   \n",
       "81    Way too strong even taking one tablet too stro...   \n",
       "83    Honestly sucked when I tried using for my body...   \n",
       "84    Bad stomach pain. Do not take them. The heartb...   \n",
       "86    Let me start by saying I purchased this exact ...   \n",
       "88                      Seller did not deliver on time.   \n",
       "89    I took the first bottle even tho it had a chem...   \n",
       "90                             Broken lid. Frustrating.   \n",
       "91    I have not had unflavored whey in some time. W...   \n",
       "93    This does not get a three star rating from me ...   \n",
       "94    I ordered the chocolate one but they sent me t...   \n",
       "95    Did not do much for me. I found no difference ...   \n",
       "96    I don't like this brand.  Cayenne is WEAK.  Th...   \n",
       "98    I never received my order. You kept sending me...   \n",
       "99    I love this brand of protein. It is generally ...   \n",
       "101   I have been happily using the unscented wipes ...   \n",
       "103   As of this review this bottle of adult vitamin...   \n",
       "105   Maybe worth it if regular vitamins really affe...   \n",
       "108   I had a very bad experience with this company....   \n",
       "111   So I've been using these diapers for about a m...   \n",
       "112   It’s my 4th day taking these. So far I’m not s...   \n",
       "114   This female nutrient sugar is not the same as ...   \n",
       "119   My mom LOVES the chocolate shake, and it has a...   \n",
       "121   I was excited to get this product for my daugh...   \n",
       "122                                      Taste not good   \n",
       "123                        I did not receive this item.   \n",
       "124                                       Did not work.   \n",
       "125   Does not dissolve at all. Even in a NutriBulle...   \n",
       "127   I haven't even received my product and I order...   \n",
       "128   This is the weirdest taste gummy I have ever  ...   \n",
       "130   Wanted to take the Fish oil for plantar facili...   \n",
       "134                                               Gross   \n",
       "135   Disgusting flavor, waste of my money. Love the...   \n",
       "136   I haven't noticed anything different in a posi...   \n",
       "138   I just got these in today and was very excited...   \n",
       "139   500 mg is 142 times the amount an adult female...   \n",
       "140   I noticed no change or improvement or reason t...   \n",
       "141   I bought this product back in December and I h...   \n",
       "142   I'm sure it does what it's supposed to... but ...   \n",
       "145   Used these before and during my first pregnanc...   \n",
       "147   It is very hard to swallow. Kept getting stuck...   \n",
       "149   This stuff is so clumpy and it gets gross real...   \n",
       "150   The bear design is cute. And I like that it's ...   \n",
       "151   Ordered two, package arrived not sealed/taped ...   \n",
       "152   One of the two jars had a broken seal and ther...   \n",
       "153   This stuff is gross!! open the bottle and it s...   \n",
       "155   This gets 2.4 stars from me.  This is very moi...   \n",
       "156   Has no discernable difference from lower price...   \n",
       "157   I do not like the taste of this powder at all....   \n",
       "160   Gave my wife constipation after taking.  This ...   \n",
       "161    I ordered 4 bottles, but only received 3bottles.   \n",
       "162   Took it at 7 am now at 1130 am I am so flushed...   \n",
       "164   this does not help me at all as far as energy ...   \n",
       "165   For more than 15 years I've been buying this f...   \n",
       "...                                                 ...   \n",
       "2411  I'm not sure if I got a bad batch or what, but...   \n",
       "2414            I did notice any difference in my skin.   \n",
       "2415  Never received the product. Shipping container...   \n",
       "2417  I am very disappointed with the product as the...   \n",
       "2419  it mixed well but it was very hard removing th...   \n",
       "2420  economic health supplement<br />the package wr...   \n",
       "2421  Did not receive order. Waiting to hear back fr...   \n",
       "2422  These seem to be working well for a lot of peo...   \n",
       "2424  I am determined to finish this due to the pric...   \n",
       "2425  This contact lens solution consistently dries ...   \n",
       "2426  Didn't work.  Way too low probiotics count.  S...   \n",
       "2427  I know elderberry can be sour, but I could har...   \n",
       "2428  This item never arrived and \"X Brand\" informed...   \n",
       "2429  This is my 2nd bottle but it was broken when I...   \n",
       "2430  I ordered the wrong thing and thought I would ...   \n",
       "2432  I had been taking the Kirkland general daily m...   \n",
       "2433  Very gross flavor! I couldnt make more than on...   \n",
       "2434  I received a different looking product, does n...   \n",
       "2435  I use these for dog wraps and they don’t work....   \n",
       "2436    Arrived COMPLETELY all melted together. Useless   \n",
       "2437  The first time buying this brand and first tim...   \n",
       "2439  Made me sick to my tummy.  Start slow and buil...   \n",
       "2440  I had been using the Vega Essentials drink and...   \n",
       "2441  Have used this B12 spray for a little over a m...   \n",
       "2442  I love using grape seed oil and I love this br...   \n",
       "2444  I like this product but the last one I receive...   \n",
       "2445  I ordered these to send it to my grandma and t...   \n",
       "2446  While they're ok for occasional use for sleep,...   \n",
       "2447  Please don't use these it cause my throat to s...   \n",
       "2448  I mix one scoop of this with 8oz of water, 5-6...   \n",
       "2450  Only tried one scoop, but will throw the whole...   \n",
       "2452  Do not buy this product unless you love the fa...   \n",
       "2455                      Lacks flavor, but mixes well.   \n",
       "2457  Tracking says item delivered yesterday, but no...   \n",
       "2458  Value is good because they ate terrible to swa...   \n",
       "2459  The pills have a horrible taste and smell whic...   \n",
       "2460  These things are huge. I will not be ordering ...   \n",
       "2461  The packaging doesn’t pair well with the produ...   \n",
       "2463    Would not buy again.  They leak all the time !!   \n",
       "2464                            but way too high priced   \n",
       "2465  This flavor tastes like lightly salted dirt. G...   \n",
       "2466  The wrong item was delivered and I am being to...   \n",
       "2468  Definitely not as good as the big brands unfor...   \n",
       "2470  The product was what I ordered and expected.  ...   \n",
       "2471  I was so excited to find these, I bought 2 jar...   \n",
       "2474  This was the most disgusting kids vitamin! I c...   \n",
       "2476  I did NOT receive 2 bottles for one. I texted ...   \n",
       "2478  I just received my bottle yesterday and they e...   \n",
       "2479  Didn't really notice any difference with the p...   \n",
       "2480  I love all that is in this, but the texture is...   \n",
       "2481  I am so pissed off! This thing was sent in the...   \n",
       "2482  Product arrived with cap broken and seal not i...   \n",
       "2483  Received the wrong item and currently still ha...   \n",
       "2484  I guess I am not deficient in dopamine. This s...   \n",
       "2485  I am very disappointed.  I did not receive my ...   \n",
       "2487  The glycerin might be good, but the packaging ...   \n",
       "2489  I have used this product before and it was fin...   \n",
       "2490  I'm not impressed by this, it had no effect on...   \n",
       "2491  In the pictures and bullet points they provide...   \n",
       "2492  I have a pretty iron stomach, and my stomach n...   \n",
       "2493  I got these for my 73 year old mother who live...   \n",
       "2494  Maybe I've got counterfeit or maybe they've ch...   \n",
       "2495  These are nice and thick, but not very wet. I’...   \n",
       "2496  I tried to change all supplements my family ta...   \n",
       "2497                              Bad allergic reaction   \n",
       "2499  Bad indigestion. Could not use and could not r...   \n",
       "2501  I didn’t like the fact that I received an empt...   \n",
       "2502  It worked for muscle growth..... but even thou...   \n",
       "2503  I've never had trouble swallowing pills -- I c...   \n",
       "2505  There is risk of cross contamination with milk...   \n",
       "2507  Even for protein shakes I just couldn’t drink ...   \n",
       "2508                                     SIZE! TOO BIG!   \n",
       "2509  Packaging was horrible! This should’ve been se...   \n",
       "2512  Chocolate Creme flavor was absolutely not pala...   \n",
       "2513  This is ascorbic acid!! It’s a synthetic, most...   \n",
       "2514                         Product was not delivered.   \n",
       "2515  The product never arrived and I have yet to ge...   \n",
       "2516                            The pills always go bad   \n",
       "2517                                          Too sweet   \n",
       "2518  The tablets are huge and coating peeled, makin...   \n",
       "2522  Some reviews said it smelled bad, and I think ...   \n",
       "2525  I bought this August 2018. I barely used it wh...   \n",
       "2526  Bought these for my daughter thinking it would...   \n",
       "2528  I bought these because I'm anemic late in my p...   \n",
       "2529  Would give it a zero if I could. Never even re...   \n",
       "2531  I’ve purchased Kirkland Olive Oil in the past:...   \n",
       "2532  The wild cherry flavor is enjoyable, albeit is...   \n",
       "2536  After 2nd use, I woke in middle of the night w...   \n",
       "2538                                 Big time overrated   \n",
       "2539  This collagen is unflavored so the taste is pa...   \n",
       "2540  I wish I had read the reviews for the product ...   \n",
       "2542                                            Rubbish   \n",
       "2543  I tried half a scoop mixed in water and within...   \n",
       "2544  This does not stir in easily or dissolve. It’s...   \n",
       "2545  All the bags in one of the rolls was miss cut ...   \n",
       "2546  Product burst opened in the box and created a ...   \n",
       "2547  This is the worse protein I've ever tried! The...   \n",
       "2549  Very small and easy to swallow- no flavor at a...   \n",
       "2551  Good but it increases the bad cholesterol- LDL...   \n",
       "2552                     Will not buy the powder again.   \n",
       "\n",
       "                                           Review Title  \\\n",
       "0                                              Bad tast   \n",
       "2                                No change. No results.   \n",
       "3              Good Vegan Choice, Poor Non Vegan Choice   \n",
       "5                                       SMELL HORRIBLE!   \n",
       "9                                              Shipping   \n",
       "10    These suppliments don't work - No suppliment s...   \n",
       "11    Heated damaged contents...Not dusted almonds, ...   \n",
       "12                                                Vitex   \n",
       "14             Wrong description !!!  Or wrong product?   \n",
       "16    These are not vegan, but \"X Brand\"'s Customer ...   \n",
       "18                            Lack in Flavor Department   \n",
       "19                                           Disgusting   \n",
       "20                       sticky, thick and a ugly taste   \n",
       "22                    Don't buy this for heavy duty use   \n",
       "23    This is not the Organic all in one coconut alm...   \n",
       "26                                     Delivered broken   \n",
       "27                          Cheap ineffective knock off   \n",
       "29          Organic, vegan, gluten-free, and good price   \n",
       "31            Some things I love, others I really don't   \n",
       "35                                               Broken   \n",
       "36                                         Not as fresh   \n",
       "37                                         Horse pills!   \n",
       "39                                 Terrible shipping!!!   \n",
       "40                                  Not so great for me   \n",
       "42                                       Didn’t receive   \n",
       "43                                 Did not receive item   \n",
       "44                           Glad it has helped others.   \n",
       "45                         Do not buy this yucky stuff.   \n",
       "48                                Sent wrong product...   \n",
       "50         I RECEIVED THIS AND THE BOTTLE WAS BROKEN>>>   \n",
       "51    Gave it three attempts, each time felt horribl...   \n",
       "53    Works BUT bought in June 2019 and they expire ...   \n",
       "55                        Perfect for dairy intolerance   \n",
       "57                       No noticeable positive effects   \n",
       "58                                       Taste is weird   \n",
       "61                            Makes face and arms itchy   \n",
       "62                          After intensive research...   \n",
       "65                                Didn’t like the taste   \n",
       "66                                            Not good!   \n",
       "68                                            Don't buy   \n",
       "69                       Birthday Cake tastes terrible.   \n",
       "70                                        Doesn't work.   \n",
       "72                                     9 Months expired   \n",
       "73                           Bring back the old formula   \n",
       "75                                         Weird flavor   \n",
       "78                                      Lost in transit   \n",
       "79                                         Smells Fishy   \n",
       "81                                   Get something else   \n",
       "83                  NOT FAINT AND DEFINITELY NOT STRONG   \n",
       "84                                       Don’t Purchase   \n",
       "86                  THIS IS A FAKE PRODUCT. DO NOT BUY.   \n",
       "88    Hard to rate when my two day delivery takes a ...   \n",
       "89                            Chemical smell and taste!   \n",
       "90                                           BROKEN LID   \n",
       "91    It's a good price for isolate but the taste is...   \n",
       "93      I Think 3 Stars Is What All Protein Powders Are   \n",
       "94                                         Wrong flavor   \n",
       "95                                  Did not work for me   \n",
       "96                                                Trash   \n",
       "98                             Never got my order right   \n",
       "99    Mixes well. But, the maple flavor was not for me.   \n",
       "101                          New wipes cause irritation   \n",
       "103                                 Does Not Contain D3   \n",
       "105                                                yuck   \n",
       "108                                             Nothing   \n",
       "111                                         Soft & Cute   \n",
       "112                                  Not working for me   \n",
       "114                                           taste bad   \n",
       "119                 WRONG FLAVOR OVER AND OVER AGAIN!!!   \n",
       "121                            Item did not arrive cold   \n",
       "122                                                 Bad   \n",
       "123                                     Did not receive   \n",
       "124                                          Poor sleep   \n",
       "125                  Hard to dissolve and flavor is bad   \n",
       "127                                          This is BS   \n",
       "128                          So artificial fruit flavor   \n",
       "130                             It’s too BIG to swallow   \n",
       "134                                               Gross   \n",
       "135                                              Flavor   \n",
       "136                        Harsh on my digestive system   \n",
       "138                                         Seal broken   \n",
       "139                  Ended up in the ER after one pill!   \n",
       "140                                        Does Nothing   \n",
       "141                                      Horrible taste   \n",
       "142                           Good vitamin, bad pill...   \n",
       "145                                               Fishy   \n",
       "147                              Really hard to swallow   \n",
       "149                                   Does not dissolve   \n",
       "150                        My Kids Don't Like the Taste   \n",
       "151                                      Didn't receive   \n",
       "152                         One of the jars is unusable   \n",
       "153                                        Don’t buy!!!   \n",
       "155   Moisturizing but greasy, greasy, greasy  & tak...   \n",
       "156                  It is ok. Not competitively priced   \n",
       "157                                      hate the taste   \n",
       "160   Caused Constipation despite clear promise othe...   \n",
       "161                                     Missing product   \n",
       "162                     Caused Delayed aggressive flush   \n",
       "164                                             quality   \n",
       "165                         They changed the formula!!!   \n",
       "...                                                 ...   \n",
       "2411                               Weird, melty texture   \n",
       "2414                              No difference in skin   \n",
       "2415                               Never received order   \n",
       "2417                               bottle came unsealed   \n",
       "2419          mixes well but hard to remove from bottle   \n",
       "2420                         economic health supplement   \n",
       "2421                                 Order Not Received   \n",
       "2422                 Texture is awful, but otherwise ok   \n",
       "2424  If you can get past the taste and texture...gr...   \n",
       "2425        Not good for long term contact lens storage   \n",
       "2426                                               Nope   \n",
       "2427                       Comparable to war head candy   \n",
       "2428                                      Never arrived   \n",
       "2429                  Pills are good but not the bottle   \n",
       "2430                   Smells like rancid rotting fruit   \n",
       "2432                                  Oddly Low Calcium   \n",
       "2433                                        Taste gross   \n",
       "2434               Not getting what's shown on my order   \n",
       "2435                          Smell goes through. Yech!   \n",
       "2436                                 Melted and useless   \n",
       "2437                            Not good. Makes me sick   \n",
       "2439  You need 7 pills to take as prescribed!! Yikes...   \n",
       "2440  Like the quality and nutrition of the shake. N...   \n",
       "2441                               No change whatsoever   \n",
       "2442                     Arrived broken and made a mess   \n",
       "2444                                          Bad Batch   \n",
       "2445             Glass bottles without any bubble wrap.   \n",
       "2446                                           Just ok.   \n",
       "2447             DON'T DO IT!!! Causes throat swelling!   \n",
       "2448            Weird aftertaste makes this a NO for me   \n",
       "2450        The taste is so bad I had to spit it out...   \n",
       "2452                        horrible flavor and texture   \n",
       "2455                              Not as good as others   \n",
       "2457                    Ordered and paid.  Not received   \n",
       "2458            Don't buy they get stuck in your throat   \n",
       "2459                                 Horrible packaging   \n",
       "2460                                Really big capsules   \n",
       "2461            Good product, but weird packaging issue   \n",
       "2463                               Leak all the time !!   \n",
       "2464                                     same as costco   \n",
       "2465                       Don’t buy the salted caramel   \n",
       "2466          Wrong item sent! Do not order from seller   \n",
       "2468                                 Sticks to my teeth   \n",
       "2470                            Shipping Misinformation   \n",
       "2471                      1 out of 10 is extremely bad!   \n",
       "2474                                        Gross taste   \n",
       "2476  Be careful of deals that are too good to be true.   \n",
       "2478                                  Expiration dates!   \n",
       "2479  I'd try something else,  didn't do anything fo...   \n",
       "2480               Great nutrients but terrible texture   \n",
       "2481                                 Are you serious!!!   \n",
       "2482                                Seal and cap broken   \n",
       "2483                                      Wrong Product   \n",
       "2484                                 Gave me a headache   \n",
       "2485                        I did not receive my order!   \n",
       "2487                                     Bad packaging.   \n",
       "2489                                      lumpy texture   \n",
       "2490                           No effect, not impressed   \n",
       "2491           BEWARE: Misleading Label and Description   \n",
       "2492                          Makes you SUPER nauseated   \n",
       "2493                  These are not similar to Depends!   \n",
       "2494  Maybe I've got counterfeit or maybe they've ch...   \n",
       "2495                                     Not wet enough   \n",
       "2496                                         Bad smell!   \n",
       "2497                             Bad  allergic reaction   \n",
       "2499                      Bad indigestion with product.   \n",
       "2501                                 Package was empty!   \n",
       "2502                             It's okay.... I guess.   \n",
       "2503                             Impossible to swallow!   \n",
       "2505                               Not allergy friendly   \n",
       "2507                                              Gross   \n",
       "2508                                        HORSE PILLS   \n",
       "2509                                 Terrible packaging   \n",
       "2512                                        Undrinkable   \n",
       "2513                     Synthetic ascorbic acid! BAD!!   \n",
       "2514                           Did not receive product.   \n",
       "2515                      Don’t order from this company   \n",
       "2516                                       Pills go bad   \n",
       "2517                                         Ok product   \n",
       "2518                  Tried them, went back to Centrum.   \n",
       "2522                                      Smells so bad   \n",
       "2525                            Don’t waste your money!   \n",
       "2526                                      Does not work   \n",
       "2528                              Does NOT contain iron   \n",
       "2529                              Don't waste your time   \n",
       "2531                                Product was Spoiled   \n",
       "2532            Tasty but VERY Sweet - Not Sugar Coated   \n",
       "2536                           Beware of stomach cramps   \n",
       "2538                                      Not effective   \n",
       "2539                                Doesn't blend well.   \n",
       "2540                                        Tastes bad.   \n",
       "2542                                       Doesn’t work   \n",
       "2543         Got sick within an hour after consuming it   \n",
       "2544                         Look for another brand!!!!   \n",
       "2545                                   Poorly cut bags.   \n",
       "2546      Product packaging/ sealing needs improvement.   \n",
       "2547                                  Taste is horrible   \n",
       "2549                      Horrible stomach side effects   \n",
       "2551                     Increase LDL - bad cholesterol   \n",
       "2552                       Thick-like paste consistency   \n",
       "\n",
       "                       topic  \n",
       "0           Bad Taste/Flavor  \n",
       "2              Not Effective  \n",
       "3                Ingredients  \n",
       "5       Quality/Contaminated  \n",
       "9      Shipment and delivery  \n",
       "10             Not Effective  \n",
       "11    Wrong Product received  \n",
       "12                  Allergic  \n",
       "14    Wrong Product received  \n",
       "16               Ingredients  \n",
       "18          Bad Taste/Flavor  \n",
       "19          Bad Taste/Flavor  \n",
       "20      Quality/Contaminated  \n",
       "22                   Pricing  \n",
       "23      Quality/Contaminated  \n",
       "26                 Packaging  \n",
       "27             Not Effective  \n",
       "29               Ingredients  \n",
       "31        Too big to swallow  \n",
       "35                 Packaging  \n",
       "36                    Expiry  \n",
       "37                  Allergic  \n",
       "39     Shipment and delivery  \n",
       "40                  Allergic  \n",
       "42     Shipment and delivery  \n",
       "43     Shipment and delivery  \n",
       "44             Not Effective  \n",
       "45          Bad Taste/Flavor  \n",
       "48               Ingredients  \n",
       "50                 Packaging  \n",
       "51                  Allergic  \n",
       "53                   Pricing  \n",
       "55          Bad Taste/Flavor  \n",
       "57                  Allergic  \n",
       "58      Quality/Contaminated  \n",
       "61          Customer Service  \n",
       "62             Not Effective  \n",
       "65          Bad Taste/Flavor  \n",
       "66          Customer Service  \n",
       "68          Bad Taste/Flavor  \n",
       "69          Bad Taste/Flavor  \n",
       "70          Customer Service  \n",
       "72                    Expiry  \n",
       "73          Bad Taste/Flavor  \n",
       "75          Bad Taste/Flavor  \n",
       "78     Shipment and delivery  \n",
       "79             Not Effective  \n",
       "81                   Texture  \n",
       "83                Smells Bad  \n",
       "84                  Allergic  \n",
       "86                  Allergic  \n",
       "88     Shipment and delivery  \n",
       "89                 Packaging  \n",
       "90                 Packaging  \n",
       "91      Quality/Contaminated  \n",
       "93                 Too Sweet  \n",
       "94    Wrong Product received  \n",
       "95             Not Effective  \n",
       "96               Ingredients  \n",
       "98     Shipment and delivery  \n",
       "99          Bad Taste/Flavor  \n",
       "101        Color and texture  \n",
       "103              Ingredients  \n",
       "105     Quality/Contaminated  \n",
       "108                Packaging  \n",
       "111            Not Effective  \n",
       "112                 Allergic  \n",
       "114     Quality/Contaminated  \n",
       "119    Shipment and delivery  \n",
       "121         Customer Service  \n",
       "122     Quality/Contaminated  \n",
       "123    Shipment and delivery  \n",
       "124            Not Effective  \n",
       "125         Bad Taste/Flavor  \n",
       "127    Shipment and delivery  \n",
       "128         Bad Taste/Flavor  \n",
       "130              Ingredients  \n",
       "134         Bad Taste/Flavor  \n",
       "135         Bad Taste/Flavor  \n",
       "136                 Allergic  \n",
       "138                Packaging  \n",
       "139                 Allergic  \n",
       "140            Not Effective  \n",
       "141         Bad Taste/Flavor  \n",
       "142                  Texture  \n",
       "145              Ingredients  \n",
       "147                  Texture  \n",
       "149         Bad Taste/Flavor  \n",
       "150         Bad Taste/Flavor  \n",
       "151    Shipment and delivery  \n",
       "152                Packaging  \n",
       "153              Ingredients  \n",
       "155               Smells Bad  \n",
       "156                  Pricing  \n",
       "157        Color and texture  \n",
       "160                 Allergic  \n",
       "161                Packaging  \n",
       "162                  Texture  \n",
       "164            Not Effective  \n",
       "165         Bad Taste/Flavor  \n",
       "...                      ...  \n",
       "2411       Color and texture  \n",
       "2414           Not Effective  \n",
       "2415   Shipment and delivery  \n",
       "2417                 Texture  \n",
       "2419       Color and texture  \n",
       "2420               Packaging  \n",
       "2421   Shipment and delivery  \n",
       "2422       Color and texture  \n",
       "2424       Color and texture  \n",
       "2425    Quality/Contaminated  \n",
       "2426           Not Effective  \n",
       "2427        Bad Taste/Flavor  \n",
       "2428   Shipment and delivery  \n",
       "2429               Packaging  \n",
       "2430              Smells Bad  \n",
       "2432      Too big to swallow  \n",
       "2433        Bad Taste/Flavor  \n",
       "2434  Wrong Product received  \n",
       "2435           Not Effective  \n",
       "2436   Shipment and delivery  \n",
       "2437                Allergic  \n",
       "2439                Allergic  \n",
       "2440        Bad Taste/Flavor  \n",
       "2441           Not Effective  \n",
       "2442               Packaging  \n",
       "2444   Shipment and delivery  \n",
       "2445               Packaging  \n",
       "2446           Not Effective  \n",
       "2447                Allergic  \n",
       "2448        Bad Taste/Flavor  \n",
       "2450        Bad Taste/Flavor  \n",
       "2452               Too Sweet  \n",
       "2455        Bad Taste/Flavor  \n",
       "2457   Shipment and delivery  \n",
       "2458      Too big to swallow  \n",
       "2459              Smells Bad  \n",
       "2460      Too big to swallow  \n",
       "2461               Packaging  \n",
       "2463        Customer Service  \n",
       "2464                 Pricing  \n",
       "2465        Bad Taste/Flavor  \n",
       "2466  Wrong Product received  \n",
       "2468      Too big to swallow  \n",
       "2470   Shipment and delivery  \n",
       "2471        Bad Taste/Flavor  \n",
       "2474        Customer Service  \n",
       "2476   Shipment and delivery  \n",
       "2478                  Expiry  \n",
       "2479           Not Effective  \n",
       "2480       Color and texture  \n",
       "2481             Ingredients  \n",
       "2482               Packaging  \n",
       "2483  Wrong Product received  \n",
       "2484                Allergic  \n",
       "2485   Shipment and delivery  \n",
       "2487               Packaging  \n",
       "2489            Hard to Chew  \n",
       "2490           Not Effective  \n",
       "2491             Ingredients  \n",
       "2492                Allergic  \n",
       "2493      Too big to swallow  \n",
       "2494        Customer Service  \n",
       "2495                 Pricing  \n",
       "2496    Quality/Contaminated  \n",
       "2497                Allergic  \n",
       "2499    Quality/Contaminated  \n",
       "2501   Shipment and delivery  \n",
       "2502        Bad Taste/Flavor  \n",
       "2503      Too big to swallow  \n",
       "2505             Ingredients  \n",
       "2507        Bad Taste/Flavor  \n",
       "2508      Too big to swallow  \n",
       "2509               Packaging  \n",
       "2512        Bad Taste/Flavor  \n",
       "2513             Ingredients  \n",
       "2514   Shipment and delivery  \n",
       "2515   Shipment and delivery  \n",
       "2516    Quality/Contaminated  \n",
       "2517               Too Sweet  \n",
       "2518      Too big to swallow  \n",
       "2522              Smells Bad  \n",
       "2525                  Expiry  \n",
       "2526           Not Effective  \n",
       "2528             Ingredients  \n",
       "2529               Packaging  \n",
       "2531    Quality/Contaminated  \n",
       "2532               Too Sweet  \n",
       "2536                Allergic  \n",
       "2538      Too big to swallow  \n",
       "2539                 Texture  \n",
       "2540               Too Sweet  \n",
       "2542        Bad Taste/Flavor  \n",
       "2543                Allergic  \n",
       "2544        Bad Taste/Flavor  \n",
       "2545               Packaging  \n",
       "2546               Packaging  \n",
       "2547               Too Sweet  \n",
       "2549                Allergic  \n",
       "2551    Quality/Contaminated  \n",
       "2552        Customer Service  \n",
       "\n",
       "[1776 rows x 3 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"These gummies aren't bad. They really helped me get through a few weeks in my pregnancy where I was gagging on everything, especially my giant usual pre-natal vitamins. The orange and yellow gummies are delicious and basically taste like candy. However, I have to give them 3 stars (and didn't finish my bottle) because the red ones taste very strongly of fish. I had to hold my nose in order to get them down. Eventually I just gave up, used up the orange and yellow, and switched back to my regular pre-natal.\",\n",
       "        'Some flavors have an odd aftertaste...', 'Too Sweet', 18],\n",
       "       [\"These gummies aren't bad. They really helped me get through a few weeks in my pregnancy where I was gagging on everything, especially my giant usual pre-natal vitamins. The orange and yellow gummies are delicious and basically taste like candy. However, I have to give them 3 stars (and didn't finish my bottle) because the red ones taste very strongly of fish. I had to hold my nose in order to get them down. Eventually I just gave up, used up the orange and yellow, and switched back to my regular pre-natal.\",\n",
       "        'Some flavors have an odd aftertaste...', 'Quality/Contaminated',\n",
       "        14],\n",
       "       [\"These gummies aren't bad. They really helped me get through a few weeks in my pregnancy where I was gagging on everything, especially my giant usual pre-natal vitamins. The orange and yellow gummies are delicious and basically taste like candy. However, I have to give them 3 stars (and didn't finish my bottle) because the red ones taste very strongly of fish. I had to hold my nose in order to get them down. Eventually I just gave up, used up the orange and yellow, and switched back to my regular pre-natal.\",\n",
       "        'Some flavors have an odd aftertaste...', 'Ingredients', 10],\n",
       "       [\"These gummies aren't bad. They really helped me get through a few weeks in my pregnancy where I was gagging on everything, especially my giant usual pre-natal vitamins. The orange and yellow gummies are delicious and basically taste like candy. However, I have to give them 3 stars (and didn't finish my bottle) because the red ones taste very strongly of fish. I had to hold my nose in order to get them down. Eventually I just gave up, used up the orange and yellow, and switched back to my regular pre-natal.\",\n",
       "        'Some flavors have an odd aftertaste...', 'Bad Taste/Flavor', 1]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[5941:5945].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split    \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "    C=0.1, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "    random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "\n",
    "for i, data in enumerate(tfidf_text_store):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = logistic_reg.fit(X_train, Y_train)\n",
    "    print(\"log-reg \"+str(i), clf.score(X_test, Y_test))\n",
    "\n",
    "print(\"=\"*30)\n",
    "for i, data in enumerate(count_vect_text):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = logistic_reg.fit(X_train, Y_train)\n",
    "    print(\"log-reg \"+str(i), clf.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_accuracy(truths, preds, n):\n",
    "    best_n = np.argsort(preds, axis=1)[:,-n:]\n",
    "#     ts = np.argmax(truths, axis=1)\n",
    "    ts = truths\n",
    "    success = 0\n",
    "    for t, p in zip(ts, best_n):\n",
    "        if t in p:\n",
    "            success += 1\n",
    "    return float(success)/preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, gc\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# sklearn.metrics.accuracy_score(y_true, y_pred\n",
    "\n",
    "def train_lgb_model(X_train, y_train, X_valid, y_valid, features, param, num_round):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X_train, X_valid: training and valid data\n",
    "        y_train, y_valid: training and valid target\n",
    "        X_test: test-data\n",
    "        features: training features\n",
    "    Return:\n",
    "        oof-pred, test_preds model, model_imp\n",
    "    \"\"\"\n",
    "    _train = lgb.Dataset(X_train[features], label=y_train, feature_name=list(features))\n",
    "    _valid = lgb.Dataset(X_valid[features], label=y_valid,feature_name=list(features))\n",
    "    \n",
    "    clf = lgb.train(param, _train, num_round, \n",
    "                    valid_sets = [_train, _valid], \n",
    "                    verbose_eval=200, \n",
    "                    early_stopping_rounds = 25)                  \n",
    "    \n",
    "    oof = clf.predict(X_valid[features], num_iteration=clf.best_iteration)\n",
    "#     test_pred = clf.predict(X_test[features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    lgb_imp = pd.DataFrame(data=[clf.feature_name(), list(clf.feature_importance())]).T\n",
    "    lgb_imp.columns = ['feature','imp']\n",
    "    \n",
    "    return oof, clf, lgb_imp\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def run_cv_lgb(train_df, target, leaves=30):\n",
    "\n",
    "    param = {\n",
    "        'bagging_freq'           : 5,\n",
    "        'bagging_fraction'       : 0.8,\n",
    "        'boost_from_average'     : 'false',\n",
    "        'boost'                  : 'gbdt',\n",
    "        'feature_fraction'       : 0.8,\n",
    "        'learning_rate'          : 0.01,\n",
    "        'max_depth'              : -1,\n",
    "        'metric'                 : 'auc',\n",
    "#         'min_data_in_leaf'       : 100,\n",
    "#         'min_sum_hessian_in_leaf': 10.0,\n",
    "        'num_leaves'             : leaves,\n",
    "        'num_threads'            : 4,\n",
    "        'tree_learner'           : 'serial',\n",
    "#         'objective'              : 'binary',\n",
    "        'verbosity'              : 1,\n",
    "    #     'lambda_l1'              : 0.001,\n",
    "        'lambda_l2'              : 0.1,\n",
    "        'objective'              : 'multiclassova',\n",
    "        'is_unbalance'           : True,\n",
    "        'num_class'              : 21,\n",
    "    }   \n",
    "    if leaves is not None:\n",
    "        param['num_leaves'] = leaves\n",
    "        print(\"using leaves: \", param['num_leaves'])\n",
    "\n",
    "    random_seed = 1234\n",
    "    n_splits = 4\n",
    "    num_round = 10000\n",
    "    feature_imp = pd.DataFrame()\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "    oof_lgb = np.zeros((len(train_df), 21))\n",
    "#     predictions = np.zeros((len(test_df),n_splits))\n",
    "\n",
    "    clfs = []\n",
    "    \n",
    "    for fold_, (train_index, valid_index) in enumerate(folds.split(train_df, target)):\n",
    "        print(train_index.shape, valid_index.shape)\n",
    "        print(\"Fold {}\".format(fold_))\n",
    "    \n",
    "        y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n",
    "        X_train, X_valid = train_df.iloc[train_index,:], train_df.iloc[valid_index,:]\n",
    "        features = X_train.columns\n",
    "        \n",
    "\n",
    "        num_round = 10000\n",
    "        oof, clf, lgb_imp = train_lgb_model(X_train, y_train, \n",
    "                                            X_valid, y_valid, \n",
    "                                            features, param, \n",
    "                                            num_round)\n",
    "        lgb_imp['fold'] = fold_\n",
    "        feature_imp = pd.concat([feature_imp, lgb_imp], axis=0)\n",
    "    \n",
    "        oof_lgb[valid_index] = oof\n",
    "#         predictions[:,fold_] = test_pred\n",
    "        clfs.append(clf)\n",
    "        \n",
    "        score = accuracy_score(y_valid, np.argmax(oof, axis=1))\n",
    "#         print(classification_report(y_valid, np.argmax(oof, axis=1)))\n",
    "#         score = roc_auc_score(y_valid, oof)\n",
    "        print( \"  auc = \", score )\n",
    "        print(top_n_accuracy(y_valid, oof, 3))\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "#         break\n",
    "#     return y_valid, oof\n",
    "        \n",
    "    feature_imp.imp = feature_imp.imp.astype('float')\n",
    "    feature_imp = feature_imp.groupby(['feature'])['imp'].mean()\n",
    "    feature_imp = pd.DataFrame(data=[feature_imp.index, feature_imp.values]).T\n",
    "    feature_imp.columns=['feature','imp']\n",
    "    feature_imp = feature_imp.sort_values(by='imp')\n",
    "\n",
    "    return clfs, feature_imp, oof_lgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using leaves:  10000\n",
      "(4463,) (1496,)\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.405608\tvalid_1's auc: 0.417911\n",
      "  auc =  0.16377005347593582\n",
      "0.3235294117647059\n",
      "============================================================\n",
      "(4465,) (1494,)\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.411251\tvalid_1's auc: 0.405284\n",
      "  auc =  0.19143239625167335\n",
      "0.321954484605087\n",
      "============================================================\n",
      "(4470,) (1489,)\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.401348\tvalid_1's auc: 0.436842\n",
      "  auc =  0.1605104096709201\n",
      "0.2807253190060443\n",
      "============================================================\n",
      "(4479,) (1480,)\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\ttraining's auc: 0.395933\tvalid_1's auc: 0.392722\n",
      "  auc =  0.17094594594594595\n",
      "0.2918918918918919\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# _, _, _ = run_cv_lgb(train_sparse_matrix, train['target'], leaves=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using leaves:  10000\n",
      "(4463,) (1496,)\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.408856\tvalid_1's auc: 0.428366\n",
      "  auc =  0.30213903743315507\n",
      "0.5006684491978609\n",
      "============================================================\n",
      "(4465,) (1494,)\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.407776\tvalid_1's auc: 0.406841\n",
      "  auc =  0.30522088353413657\n",
      "0.536144578313253\n",
      "============================================================\n",
      "(4470,) (1489,)\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.413765\tvalid_1's auc: 0.449881\n",
      "  auc =  0.29012760241773\n",
      "0.5023505708529215\n",
      "============================================================\n",
      "(4479,) (1480,)\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.385803\tvalid_1's auc: 0.392989\n",
      "  auc =  0.29864864864864865\n",
      "0.5236486486486487\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "_, _, _ = run_cv_lgb(train_sparse_matrix, train['target'], leaves=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using leaves:  10000\n",
      "(4463,) (1496,)\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.406346\tvalid_1's auc: 0.42111\n",
      "  auc =  0.3054812834224599\n",
      "0.517379679144385\n",
      "============================================================\n",
      "(4465,) (1494,)\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.413985\tvalid_1's auc: 0.415912\n",
      "  auc =  0.3286479250334672\n",
      "0.5401606425702812\n",
      "============================================================\n",
      "(4470,) (1489,)\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.419028\tvalid_1's auc: 0.448459\n",
      "  auc =  0.2995298858294157\n",
      "0.5251846877098724\n",
      "============================================================\n",
      "(4479,) (1480,)\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.399299\tvalid_1's auc: 0.391379\n",
      "  auc =  0.30743243243243246\n",
      "0.5324324324324324\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "_, f, _ = run_cv_lgb(train_sparse_matrix, train['target'], leaves=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f70fa4fed68>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot('imp', data=f.sort_values('imp', ascending=False)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-reg  0.4006711409395973\n"
     ]
    }
   ],
   "source": [
    "train_ = count_vect[-2][:train.shape[0]]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    train_, train['target'], \n",
    "    stratify=train['target'], \n",
    "    test_size=0.25\n",
    ")\n",
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "    C=0.2, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "    random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "\n",
    "clf = logistic_reg.fit(X_train, Y_train)\n",
    "print(\"log-reg \", clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# model = LogisticRegression(solver='sag')\n",
    "sfm = SelectFromModel(logistic_reg, threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse_matrix = sfm.fit_transform(train_, target)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5959, 1486), (5959, 1650))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sparse_matrix.shape, train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse_matrix = train_sparse_matrix.todense()\n",
    "train_sparse_matrix = pd.DataFrame(train_sparse_matrix)\n",
    "train_sparse_matrix.columns = ['col'+str(i) for i in range(train_sparse_matrix.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5959, 3), (2553, 3))"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ = df[:train.shape[0]]\n",
    "test_  = df[train.shape[0]:]\n",
    "train_.shape, test_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============  done  ===============\n",
      "===============  done  ===============\n",
      "===============  done  ===============\n",
      "===============  done  ===============\n",
      "===============  done  ===============\n",
      "===============  done  ===============\n"
     ]
    }
   ],
   "source": [
    "kuch_title1 = tfidf_feature(train, test, 'title', min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=2, stopwords='english', \n",
    "                  n_component=75, decom_flag=True, which_method='svd', \n",
    "                  max_features=None, feat_col_name='svd_title1')\n",
    "\n",
    "kuch_title2 = tfidf_feature(train, test, 'title', min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=3, stopwords='english', \n",
    "                  n_component=75, decom_flag=True, which_method='svd', \n",
    "                  max_features=None, feat_col_name='svd_title2')\n",
    "\n",
    "kuch_text11 = tfidf_feature(train, test, 'text', min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=2, stopwords='english', \n",
    "                  n_component=100, decom_flag=True, which_method='svd', \n",
    "                  max_features=None, feat_col_name='svd_text11')\n",
    "\n",
    "kuch_text21 = tfidf_feature(train, test, 'text', min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=3, stopwords='english', \n",
    "                  n_component=100, decom_flag=True, which_method='svd', \n",
    "                  max_features=None, feat_col_name='svd_text21')\n",
    "\n",
    "\n",
    "kuch_text12 = tfidf_feature(train, test, 'text', min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=2, stopwords='english', \n",
    "                  n_component=50, decom_flag=True, which_method='nmf', \n",
    "                  max_features=None, feat_col_name='svd_text12')\n",
    "\n",
    "kuch_text22 = tfidf_feature(train, test, 'text', min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=3, stopwords='english', \n",
    "                  n_component=50, decom_flag=True, which_method='nmf', \n",
    "                  max_features=None, feat_col_name='svd_text22')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5959, 450), (2553, 450))"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "train_all = pd.concat([kuch_title1[0], kuch_title2[0], kuch_text11[0],\n",
    "                       kuch_text12[0], kuch_text21[0], kuch_text22[0]], axis=1)\n",
    "test_all  = pd.concat([kuch_title1[1], kuch_title2[1], kuch_text11[1],\n",
    "                       kuch_text12[1], kuch_text21[1], kuch_text22[1]], axis=1)\n",
    "\n",
    "train_all.shape, test_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using leaves:  70\n",
      "(4463,) (1496,)\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.147056\tvalid_1's auc: 0.236048\n",
      "  auc =  0.4391711229946524\n",
      "0.7179144385026738\n",
      "============================================================\n",
      "(4465,) (1494,)\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.171365\tvalid_1's auc: 0.218049\n",
      "  auc =  0.4243641231593039\n",
      "0.715528781793842\n",
      "============================================================\n",
      "(4470,) (1489,)\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.175862\tvalid_1's auc: 0.247368\n",
      "  auc =  0.40899932840832776\n",
      "0.6991269308260577\n",
      "============================================================\n",
      "(4479,) (1480,)\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.169342\tvalid_1's auc: 0.221444\n",
      "  auc =  0.4195945945945946\n",
      "0.6925675675675675\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "_, f, _ = run_cv_lgb(train_all, train['target'], leaves=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>svd_title1_0</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>svd_title1_1</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>svd_text11_12</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>svd_text12_39</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>svd_text12_41</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>svd_text12_46</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>svd_text12_28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>svd_text12_38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>svd_title1_5</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>svd_title1_2</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>svd_text22_25</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svd_text11_25</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>svd_text11_9</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>svd_text22_47</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>svd_title1_26</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>svd_text21_97</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>svd_title1_12</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>svd_title2_3</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>svd_text11_2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>svd_text12_17</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>svd_title1_13</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>svd_title1_11</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>svd_text22_7</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>svd_title1_69</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>svd_text12_18</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>svd_title2_0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>svd_text11_47</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>svd_title2_13</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>svd_text22_41</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>svd_title2_38</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>svd_text21_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>svd_title1_71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>svd_text11_64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>svd_text22_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>svd_title2_4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>svd_text12_35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>svd_text11_22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>svd_title1_61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>svd_text11_41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>svd_text11_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>svd_text12_14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>svd_text21_72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>svd_text12_43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>svd_text11_38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>svd_text11_90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>svd_text11_32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>svd_text11_35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>svd_text21_83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>svd_text11_13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>svd_text12_42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>svd_title2_69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>svd_title1_56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>svd_text21_63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>svd_text12_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>svd_text11_20</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>svd_text11_57</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>svd_title2_6</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>svd_text11_27</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>svd_text11_28</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>svd_title2_51</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature   imp\n",
       "300   svd_title1_0   5.5\n",
       "301   svd_title1_1  3.25\n",
       "4    svd_text11_12   2.5\n",
       "133  svd_text12_39   2.5\n",
       "136  svd_text12_41  2.25\n",
       "141  svd_text12_46  2.25\n",
       "121  svd_text12_28     2\n",
       "132  svd_text12_38     2\n",
       "345   svd_title1_5  1.75\n",
       "312   svd_title1_2  1.75\n",
       "268  svd_text22_25  1.75\n",
       "18   svd_text11_25  1.75\n",
       "89    svd_text11_9  1.75\n",
       "292  svd_text22_47  1.75\n",
       "319  svd_title1_26   1.5\n",
       "247  svd_text21_97   1.5\n",
       "304  svd_title1_12   1.5\n",
       "398   svd_title2_3   1.5\n",
       "12    svd_text11_2   1.5\n",
       "109  svd_text12_17   1.5\n",
       "305  svd_title1_13   1.5\n",
       "303  svd_title1_11   1.5\n",
       "297   svd_text22_7   1.5\n",
       "366  svd_title1_69   1.5\n",
       "110  svd_text12_18   1.5\n",
       "375   svd_title2_0   1.5\n",
       "42   svd_text11_47   1.5\n",
       "380  svd_title2_13   1.5\n",
       "286  svd_text22_41   1.5\n",
       "407  svd_title2_38   1.5\n",
       "..             ...   ...\n",
       "162   svd_text21_2     1\n",
       "369  svd_title1_71     1\n",
       "61   svd_text11_64     1\n",
       "252  svd_text22_10     1\n",
       "409   svd_title2_4     1\n",
       "129  svd_text12_35     1\n",
       "15   svd_text11_22     1\n",
       "358  svd_title1_61     1\n",
       "36   svd_text11_41     1\n",
       "1     svd_text11_1     1\n",
       "106  svd_text12_14     1\n",
       "220  svd_text21_72     1\n",
       "138  svd_text12_43     1\n",
       "32   svd_text11_38     1\n",
       "90   svd_text11_90     1\n",
       "26   svd_text11_32     1\n",
       "29   svd_text11_35     1\n",
       "232  svd_text21_83     1\n",
       "5    svd_text11_13     1\n",
       "137  svd_text12_42     1\n",
       "441  svd_title2_69     1\n",
       "352  svd_title1_56     1\n",
       "210  svd_text21_63     1\n",
       "102  svd_text12_10     1\n",
       "13   svd_text11_20  0.75\n",
       "53   svd_text11_57  0.75\n",
       "431   svd_title2_6  0.75\n",
       "20   svd_text11_27  0.75\n",
       "21   svd_text11_28  0.75\n",
       "422  svd_title2_51  0.75\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.sort_values('imp', ascending=False)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using leaves:  50\n",
      "(4463,) (1496,)\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.147056\tvalid_1's auc: 0.236048\n",
      "  auc =  0.4391711229946524\n",
      "0.7179144385026738\n",
      "============================================================\n",
      "(4465,) (1494,)\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.171365\tvalid_1's auc: 0.218049\n",
      "  auc =  0.4243641231593039\n",
      "0.715528781793842\n",
      "============================================================\n",
      "(4470,) (1489,)\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.175862\tvalid_1's auc: 0.247368\n",
      "  auc =  0.40899932840832776\n",
      "0.6991269308260577\n",
      "============================================================\n",
      "(4479,) (1480,)\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.169342\tvalid_1's auc: 0.221444\n",
      "  auc =  0.4195945945945946\n",
      "0.6925675675675675\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "_, f, _ = run_cv_lgb(train_all, train['target'], leaves=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using leaves:  200\n",
      "(4463,) (1496,)\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.0635784\tvalid_1's auc: 0.228486\n",
      "  auc =  0.33890374331550804\n",
      "0.6778074866310161\n",
      "============================================================\n",
      "(4465,) (1494,)\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.088599\tvalid_1's auc: 0.235155\n",
      "  auc =  0.35876840696117807\n",
      "0.6927710843373494\n",
      "============================================================\n",
      "(4470,) (1489,)\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.0824321\tvalid_1's auc: 0.202618\n",
      "  auc =  0.3364674278038952\n",
      "0.6830087306917394\n",
      "============================================================\n",
      "(4479,) (1480,)\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.0796541\tvalid_1's auc: 0.219037\n",
      "  auc =  0.37027027027027026\n",
      "0.7047297297297297\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "_, f, _ = run_cv_lgb(train_all, train['target'], leaves=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>svd_title1_1</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>svd_text11_9</td>\n",
       "      <td>6.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>svd_title1_0</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>svd_text12_39</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>svd_text11_12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>svd_text11_2</td>\n",
       "      <td>5.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>svd_text11_3</td>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>svd_title1_3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>svd_text21_7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>svd_text11_7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>svd_title1_2</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>svd_text11_14</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>svd_text12_41</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>svd_text21_9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>svd_text22_18</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>svd_text21_15</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>svd_text21_20</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>svd_title1_8</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>svd_text11_84</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>svd_title2_0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>svd_title1_13</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>svd_text11_33</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>svd_title1_12</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svd_text11_23</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>svd_text22_30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>svd_text11_17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>svd_title1_51</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>svd_text21_87</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svd_text11_10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>svd_text12_17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>svd_text21_66</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>svd_text12_45</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>svd_text11_22</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>svd_text21_28</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>svd_text11_94</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>svd_text12_10</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>svd_text21_29</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>svd_title2_39</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>svd_text21_19</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>svd_text21_31</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>svd_text21_32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>svd_title2_13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>svd_text21_33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>svd_text11_74</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>svd_text21_84</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>svd_text21_6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>svd_text12_35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>svd_text11_72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>svd_text21_23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>svd_title1_60</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>svd_text21_70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>svd_text11_30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>svd_title1_65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>svd_text11_31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>svd_text11_95</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>svd_text21_45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>svd_text11_35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>svd_text21_55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>svd_text11_47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>svd_title1_23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature   imp\n",
       "301   svd_title1_1   7.5\n",
       "89    svd_text11_9  6.75\n",
       "300   svd_title1_0   6.5\n",
       "133  svd_text12_39   6.5\n",
       "4    svd_text11_12     6\n",
       "12    svd_text11_2  5.75\n",
       "23    svd_text11_3  5.25\n",
       "323   svd_title1_3     5\n",
       "217   svd_text21_7     5\n",
       "67    svd_text11_7     5\n",
       "312   svd_title1_2   4.5\n",
       "6    svd_text11_14  4.25\n",
       "136  svd_text12_41  4.25\n",
       "239   svd_text21_9     4\n",
       "260  svd_text22_18  3.75\n",
       "157  svd_text21_15  3.75\n",
       "163  svd_text21_20   3.5\n",
       "373   svd_title1_8   3.5\n",
       "83   svd_text11_84   3.5\n",
       "375   svd_title2_0   3.5\n",
       "305  svd_title1_13   3.5\n",
       "27   svd_text11_33  3.25\n",
       "304  svd_title1_12  3.25\n",
       "16   svd_text11_23  3.25\n",
       "274  svd_text22_30     3\n",
       "9    svd_text11_17     3\n",
       "347  svd_title1_51     3\n",
       "236  svd_text21_87     3\n",
       "2    svd_text11_10     3\n",
       "109  svd_text12_17     3\n",
       "..             ...   ...\n",
       "213  svd_text21_66  2.25\n",
       "140  svd_text12_45  2.25\n",
       "15   svd_text11_22  2.25\n",
       "171  svd_text21_28  2.25\n",
       "94   svd_text11_94  2.25\n",
       "102  svd_text12_10  2.25\n",
       "172  svd_text21_29  2.25\n",
       "408  svd_title2_39  2.25\n",
       "161  svd_text21_19  2.25\n",
       "175  svd_text21_31  2.25\n",
       "176  svd_text21_32     2\n",
       "380  svd_title2_13     2\n",
       "177  svd_text21_33     2\n",
       "72   svd_text11_74     2\n",
       "233  svd_text21_84     2\n",
       "206   svd_text21_6     2\n",
       "129  svd_text12_35     2\n",
       "70   svd_text11_72     2\n",
       "166  svd_text21_23     2\n",
       "357  svd_title1_60     2\n",
       "218  svd_text21_70     2\n",
       "24   svd_text11_30     2\n",
       "362  svd_title1_65     2\n",
       "25   svd_text11_31     2\n",
       "95   svd_text11_95     2\n",
       "190  svd_text21_45     2\n",
       "29   svd_text11_35     2\n",
       "201  svd_text21_55     2\n",
       "42   svd_text11_47     2\n",
       "316  svd_title1_23     2\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.sort_values('imp', ascending=False)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_xgb_model(X_train, y_train, X_valid, y_valid, features, param, \n",
    "                    num_round):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X_train, X_valid: training and valid data\n",
    "        y_train, y_valid: training and valid target\n",
    "        X_test: test-data\n",
    "        features: training features\n",
    "    Return:\n",
    "        oof-pred, test_preds, model, model_imp\n",
    "    \"\"\"\n",
    "    _train = xgb.DMatrix(X_train[features], label=y_train, feature_names=list(features))\n",
    "    _valid = xgb.DMatrix(X_valid[features], label=y_valid,feature_names=list(features))\n",
    "    \n",
    "    watchlist = [(_valid, 'valid')]\n",
    "    clf = xgb.train(dtrain=_train, \n",
    "                    num_boost_round=num_round, \n",
    "                    evals=watchlist,\n",
    "                    early_stopping_rounds=25, \n",
    "                    verbose_eval=200, \n",
    "                    params=param)\n",
    "    \n",
    "    valid_frame = xgb.DMatrix(X_valid[features],feature_names=list(features))\n",
    "    oof  = clf.predict(valid_frame, ntree_limit=clf.best_ntree_limit)\n",
    "\n",
    "\n",
    "#     test_frame = xgb.DMatrix(X_test[features],feature_names=list(features))\n",
    "#     test_pred = clf.predict(test_frame, ntree_limit=clf.best_ntree_limit)\n",
    "\n",
    "    \n",
    "    xgb_imp = pd.DataFrame(data=[list(clf.get_fscore().keys()), \n",
    "                                 list(clf.get_fscore().values())]).T\n",
    "    xgb_imp.columns = ['feature','imp']\n",
    "    xgb_imp.imp = xgb_imp.imp.astype('float')\n",
    "    \n",
    "#     return oof, test_pred, clf, xgb_imp\n",
    "    return oof, clf, xgb_imp\n",
    "\n",
    "\n",
    "# def run_cv_xgb(train_df, target, test_df, depth):\n",
    "def run_cv_xgb(train_df, target, depth):\n",
    "\n",
    "    features = train_df.columns\n",
    "    params = {\n",
    "        'eval_metric'     : 'auc',\n",
    "        'seed'            : 1337,\n",
    "        'eta'             : 0.05,\n",
    "        'subsample'       : 0.7,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'silent'          : 1,\n",
    "        'nthread'         : 4,\n",
    "        'Scale_pos_weight': 3.607,\n",
    "        'objective'       : 'multi:softmax',\n",
    "        'num_class'       : 21,\n",
    "        'max_depth'       : depth,\n",
    "        'alpha'           : 0.05\n",
    "    }\n",
    "    \n",
    "    n_splits = 3\n",
    "    random_seed = 1234\n",
    "    feature_imp = pd.DataFrame()\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "    oof_xgb = np.zeros((len(train_df), 21))\n",
    "#     predictions = np.zeros((len(test_df),n_splits))\n",
    "    clfs = []\n",
    "##########################\n",
    "    for fold_, (train_index, valid_index) in enumerate(folds.split(train_df, target)):\n",
    "        print(train_index.shape, valid_index.shape)\n",
    "        print(\"Fold {}\".format(fold_))\n",
    "    \n",
    "        y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n",
    "        X_train, X_valid = train_df.iloc[train_index,:], train_df.iloc[valid_index,:]\n",
    "        features = X_train.columns\n",
    "        \n",
    "\n",
    "        num_rounds = 10000\n",
    "        oof, test_pred, clf, xgb_imp = train_xgb_model(X_train, y_train, \n",
    "                                                       X_valid, y_valid, \n",
    "                                                       features, params, \n",
    "                                                       num_rounds)\n",
    "        \n",
    "        xgb_imp['fold'] = fold_\n",
    "        feature_imp = pd.concat([feature_imp, xgb_imp], axis=0)\n",
    "    \n",
    "        oof_xgb[valid_index] = oof\n",
    "#         predictions[:,fold_] = test_pred\n",
    "        clfs.append(clf)\n",
    "        \n",
    "        score = accuracy_score(y_valid, np.argmax(oof_xgb, axis=1))\n",
    "#         print(classification_report(y_valid, np.argmax(oof, axis=1)))\n",
    "#         score = roc_auc_score(y_valid, oof)\n",
    "        print( \"  auc = \", score )\n",
    "        print(top_n_accuracy(y_valid, oof_xgb, 3))\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    feature_imp.imp = feature_imp.imp.astype('float')\n",
    "    feature_imp = feature_imp.groupby(['feature'])['imp'].mean()\n",
    "    feature_imp = pd.DataFrame(data=[feature_imp.index, feature_imp.values]).T\n",
    "    feature_imp.columns=['feature','imp']\n",
    "    feature_imp = feature_imp.sort_values(by='imp')\n",
    "\n",
    "\n",
    "    return clfs, feature_imp, oof_xgb#, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_cv_xgb(train_all, target, depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.522 \n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(max_depth=4, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(train_all, target)\n",
    "predictions = clf.predict_proba(train_all)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(target, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4469, 450), (4469,))"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    train_all, train['target'], \n",
    "    stratify=train['target'], \n",
    "    test_size=0.25\n",
    ")\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 2.122 \n",
      "  auc =  0.42348993288590603\n",
      "0.825503355704698\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(max_depth=4, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict_proba(X_test)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "\n",
    "score = accuracy_score(Y_test, np.argmax(predictions, axis=1))\n",
    "print( \"  auc = \", score )\n",
    "print(top_n_accuracy(Y_test, predictions, 3))\n",
    "print(\"=\"*60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.849 \n",
      "  auc =  0.4268456375838926\n",
      "0.8315436241610739\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(max_depth=6, n_estimators=50, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict_proba(X_test)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "\n",
    "score = accuracy_score(Y_test, np.argmax(predictions, axis=1))\n",
    "print( \"  auc = \", score )\n",
    "print(top_n_accuracy(Y_test, predictions, 3))\n",
    "print(\"=\"*60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scl = StandardScaler()\n",
    "scl.fit(train_all)\n",
    "train_all = scl.transform(train_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.881 \n",
      "  auc =  0.4161073825503356\n",
      "0.8422818791946308\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(max_depth=6, n_estimators=50, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict_proba(X_test)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "\n",
    "score = accuracy_score(Y_test, np.argmax(predictions, axis=1))\n",
    "print( \"  auc = \", score )\n",
    "print(top_n_accuracy(Y_test, predictions, 3))\n",
    "print(\"=\"*60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=-1, penalty='l2', random_state=1234,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = MultinomialNB().fit(X_train, Y_train)\n",
    "# predictions = clf.predictions\n",
    "# print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "    C=0.01, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "    random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "# predictions = logistic_reg.pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.968 \n",
      "  auc =  0.5140939597315436\n",
      "0.8100671140939597\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "predictions = logistic_reg.predict_proba(X_test)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "score = accuracy_score(Y_test, np.argmax(predictions, axis=1))\n",
    "print( \"  auc = \", score )\n",
    "print(top_n_accuracy(Y_test, predictions, 3))\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.643 \n",
      "  auc =  0.4791946308724832\n",
      "0.812751677852349\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict_proba(X_test)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "score = accuracy_score(Y_test, np.argmax(predictions, axis=1))\n",
    "print( \"  auc = \", score )\n",
    "print(top_n_accuracy(Y_test, predictions, 3))\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:27, 14394.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('../../ml-toolbox-testing/dataset/glove.6B/glove.6B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4469,), (4469,))"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    df[:train.shape[0]]['title'], train['target'], \n",
    "    stratify=train['target'], \n",
    "    test_size=0.25\n",
    ")\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/4469 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|█▋        | 730/4469 [00:00<00:00, 7298.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 1463/4469 [00:00<00:00, 7306.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 2251/4469 [00:00<00:00, 7467.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████▊   | 3027/4469 [00:00<00:00, 7552.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 3840/4469 [00:00<00:00, 7714.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 4469/4469 [00:00<00:00, 7675.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1490 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 786/1490 [00:00<00:00, 7856.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1490/1490 [00:00<00:00, 7561.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()#.decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(X_train)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(X_test)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ankish/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4469, 300), (1490, 300))"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)\n",
    "xtrain_glove.shape, xvalid_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 2.309 \n",
      "  auc =  0.33758389261744964\n",
      "0.6161073825503356\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "# clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf = xgb.XGBClassifier(max_depth=6, n_estimators=50, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_glove, Y_train)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "score = accuracy_score(Y_test, np.argmax(predictions, axis=1))\n",
    "print( \"  auc = \", score )\n",
    "print(top_n_accuracy(Y_test, predictions, 3))\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 3.201 \n",
      "  auc =  0.3308724832214765\n",
      "0.614765100671141\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_glove, Y_train)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "score = accuracy_score(Y_test, np.argmax(predictions, axis=1))\n",
    "print( \"  auc = \", score )\n",
    "print(top_n_accuracy(Y_test, predictions, 3))\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35126"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = StandardScaler()\n",
    "xtrain_glove = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove = scl.transform(xvalid_glove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 21)                6321      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 189,321\n",
      "Trainable params: 188,121\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"no model previously\")\n",
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "yvalid_enc = np_utils.to_categorical(Y_test)\n",
    "\n",
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 1490 samples\n",
      "Epoch 1/3\n",
      "4469/4469 [==============================] - 1s 283us/step - loss: 1.2083 - acc: 0.5963 - val_loss: 2.5464 - val_acc: 0.3530\n",
      "Epoch 2/3\n",
      "4469/4469 [==============================] - 0s 87us/step - loss: 1.1535 - acc: 0.5979 - val_loss: 2.5796 - val_acc: 0.3477\n",
      "Epoch 3/3\n",
      "4469/4469 [==============================] - 0s 88us/step - loss: 1.1485 - acc: 0.6124 - val_loss: 2.5863 - val_acc: 0.3409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f70f44fc4e0>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(xtrain_glove, y=ytrain_enc, batch_size=64, \n",
    "          epochs=3, verbose=1, \n",
    "          validation_data=(xvalid_glove, yvalid_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2367 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 2367/2367 [00:00<00:00, 301247.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 17\n",
    "\n",
    "token.fit_on_texts(list(X_train) + list(X_test))\n",
    "xtrain_seq = token.texts_to_sequences(X_train)\n",
    "xvalid_seq = token.texts_to_sequences(X_test)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index\n",
    "\n",
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 17, 300)           710400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_16 (Spatia (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1024)              103424    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 21)                21525     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 2,045,349\n",
      "Trainable params: 1,334,949\n",
      "Non-trainable params: 710,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except:\n",
    "    print(\"no model previously\")\n",
    "    \n",
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 1490 samples\n",
      "Epoch 1/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.6959 - acc: 0.1821 - val_loss: 2.4985 - val_acc: 0.2322\n",
      "Epoch 2/50\n",
      "4469/4469 [==============================] - 3s 671us/step - loss: 2.5046 - acc: 0.2399 - val_loss: 2.3612 - val_acc: 0.3416\n",
      "Epoch 3/50\n",
      "4469/4469 [==============================] - 3s 600us/step - loss: 2.3438 - acc: 0.2987 - val_loss: 2.2129 - val_acc: 0.3497\n",
      "Epoch 4/50\n",
      "4469/4469 [==============================] - 3s 631us/step - loss: 2.2656 - acc: 0.3269 - val_loss: 2.1535 - val_acc: 0.3658\n",
      "Epoch 5/50\n",
      "4469/4469 [==============================] - 3s 711us/step - loss: 2.2204 - acc: 0.3450 - val_loss: 2.1467 - val_acc: 0.3812\n",
      "Epoch 6/50\n",
      "4469/4469 [==============================] - 3s 599us/step - loss: 2.1601 - acc: 0.3571 - val_loss: 2.1154 - val_acc: 0.3953\n",
      "Epoch 7/50\n",
      "4469/4469 [==============================] - 3s 596us/step - loss: 2.1292 - acc: 0.3808 - val_loss: 2.0727 - val_acc: 0.4020\n",
      "Epoch 8/50\n",
      "4469/4469 [==============================] - 3s 697us/step - loss: 2.1043 - acc: 0.3766 - val_loss: 2.0646 - val_acc: 0.4074\n",
      "Epoch 9/50\n",
      "4469/4469 [==============================] - 3s 737us/step - loss: 2.0765 - acc: 0.3967 - val_loss: 2.0482 - val_acc: 0.4174\n",
      "Epoch 10/50\n",
      "4469/4469 [==============================] - 3s 617us/step - loss: 2.0363 - acc: 0.3972 - val_loss: 2.0292 - val_acc: 0.4235\n",
      "Epoch 11/50\n",
      "4469/4469 [==============================] - 3s 716us/step - loss: 2.0118 - acc: 0.4030 - val_loss: 2.0183 - val_acc: 0.4322\n",
      "Epoch 12/50\n",
      "4469/4469 [==============================] - 3s 776us/step - loss: 2.0032 - acc: 0.4182 - val_loss: 2.0042 - val_acc: 0.4322\n",
      "Epoch 13/50\n",
      "4469/4469 [==============================] - 3s 653us/step - loss: 1.9769 - acc: 0.4276 - val_loss: 2.0034 - val_acc: 0.4396\n",
      "Epoch 14/50\n",
      "4469/4469 [==============================] - 3s 612us/step - loss: 1.9594 - acc: 0.4229 - val_loss: 2.0039 - val_acc: 0.4409\n",
      "Epoch 15/50\n",
      "4469/4469 [==============================] - 3s 674us/step - loss: 1.9272 - acc: 0.4301 - val_loss: 2.0003 - val_acc: 0.4436\n",
      "Epoch 16/50\n",
      "4469/4469 [==============================] - 3s 649us/step - loss: 1.9356 - acc: 0.4332 - val_loss: 1.9934 - val_acc: 0.4329\n",
      "Epoch 17/50\n",
      "4469/4469 [==============================] - 3s 754us/step - loss: 1.9009 - acc: 0.4444 - val_loss: 1.9968 - val_acc: 0.4450\n",
      "Epoch 18/50\n",
      "4469/4469 [==============================] - 3s 657us/step - loss: 1.9000 - acc: 0.4392 - val_loss: 1.9985 - val_acc: 0.4396\n",
      "Epoch 19/50\n",
      "4469/4469 [==============================] - 3s 609us/step - loss: 1.8770 - acc: 0.4410 - val_loss: 1.9915 - val_acc: 0.4450\n",
      "Epoch 20/50\n",
      "4469/4469 [==============================] - 3s 613us/step - loss: 1.8704 - acc: 0.4489 - val_loss: 1.9910 - val_acc: 0.4483\n",
      "Epoch 21/50\n",
      "4469/4469 [==============================] - 3s 602us/step - loss: 1.8421 - acc: 0.4536 - val_loss: 1.9884 - val_acc: 0.4503\n",
      "Epoch 22/50\n",
      "4469/4469 [==============================] - 3s 618us/step - loss: 1.8455 - acc: 0.4558 - val_loss: 1.9858 - val_acc: 0.4450\n",
      "Epoch 23/50\n",
      "4469/4469 [==============================] - 3s 611us/step - loss: 1.8136 - acc: 0.4589 - val_loss: 1.9868 - val_acc: 0.4443\n",
      "Epoch 24/50\n",
      "4469/4469 [==============================] - 3s 612us/step - loss: 1.8146 - acc: 0.4589 - val_loss: 1.9864 - val_acc: 0.4423\n",
      "Epoch 25/50\n",
      "4469/4469 [==============================] - 3s 634us/step - loss: 1.8002 - acc: 0.4654 - val_loss: 1.9973 - val_acc: 0.4477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f70725cf518>"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=batch_size, epochs=epochs, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n",
    "\n",
    "\n",
    "# model.fit(xtrain_pad, y=ytrain_enc, batch_size=64, \n",
    "#           epochs=10, verbose=1, \n",
    "#           validation_data=(xvalid_pad, yvalid_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 17, 300)           710400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_17 (Spatia (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 21)                21525     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 2,810,949\n",
      "Trainable params: 2,100,549\n",
      "Non-trainable params: 710,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except:\n",
    "    print(\"no model previously\")\n",
    "\n",
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 1490 samples\n",
      "Epoch 1/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.6953 - acc: 0.1804 - val_loss: 2.5068 - val_acc: 0.2698\n",
      "Epoch 2/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.4504 - acc: 0.2584 - val_loss: 2.3125 - val_acc: 0.3477\n",
      "Epoch 3/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.3225 - acc: 0.3142 - val_loss: 2.2211 - val_acc: 0.3611\n",
      "Epoch 4/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.2541 - acc: 0.3256 - val_loss: 2.1455 - val_acc: 0.3732\n",
      "Epoch 5/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.2084 - acc: 0.3533 - val_loss: 2.1323 - val_acc: 0.3832\n",
      "Epoch 6/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.1647 - acc: 0.3654 - val_loss: 2.0669 - val_acc: 0.3946\n",
      "Epoch 7/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.1048 - acc: 0.3853 - val_loss: 2.0603 - val_acc: 0.4060\n",
      "Epoch 8/50\n",
      "4469/4469 [==============================] - 8s 2ms/step - loss: 2.0791 - acc: 0.3909 - val_loss: 2.0370 - val_acc: 0.4215\n",
      "Epoch 9/50\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 2.0670 - acc: 0.3905 - val_loss: 2.0429 - val_acc: 0.4195\n",
      "Epoch 10/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.0308 - acc: 0.4146 - val_loss: 2.0278 - val_acc: 0.4255\n",
      "Epoch 11/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.0241 - acc: 0.4122 - val_loss: 2.0176 - val_acc: 0.4282\n",
      "Epoch 12/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 1.9877 - acc: 0.4213 - val_loss: 2.0183 - val_acc: 0.4221\n",
      "Epoch 13/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 1.9760 - acc: 0.4316 - val_loss: 2.0176 - val_acc: 0.4369\n",
      "Epoch 14/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 1.9485 - acc: 0.4267 - val_loss: 1.9954 - val_acc: 0.4295\n",
      "Epoch 15/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 1.9440 - acc: 0.4290 - val_loss: 2.0110 - val_acc: 0.4463\n",
      "Epoch 16/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 1.9357 - acc: 0.4301 - val_loss: 1.9959 - val_acc: 0.4383\n",
      "Epoch 17/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 1.9074 - acc: 0.4314 - val_loss: 2.0144 - val_acc: 0.4409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f70664002b0>"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=batch_size, epochs=epochs, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 17, 300)           710400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_18 (Spatia (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 600)               1442400   \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1024)              615424    \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 21)                21525     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 3,839,349\n",
      "Trainable params: 3,128,949\n",
      "Non-trainable params: 710,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except:\n",
    "    print(\"no model previously\")\n",
    "\n",
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 1490 samples\n",
      "Epoch 1/50\n",
      "4469/4469 [==============================] - 14s 3ms/step - loss: 2.6930 - val_loss: 2.4781\n",
      "Epoch 2/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.4610 - val_loss: 2.2849\n",
      "Epoch 3/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.3076 - val_loss: 2.1916\n",
      "Epoch 4/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.2434 - val_loss: 2.1729\n",
      "Epoch 5/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.1994 - val_loss: 2.1053\n",
      "Epoch 6/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.1592 - val_loss: 2.0853\n",
      "Epoch 7/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.1100 - val_loss: 2.0549\n",
      "Epoch 8/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.0700 - val_loss: 2.0334\n",
      "Epoch 9/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.0545 - val_loss: 2.0186\n",
      "Epoch 10/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.0127 - val_loss: 2.0102\n",
      "Epoch 11/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.0043 - val_loss: 2.0223\n",
      "Epoch 12/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 1.9885 - val_loss: 2.0118\n",
      "Epoch 13/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 1.9564 - val_loss: 2.0085\n",
      "Epoch 14/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 1.9439 - val_loss: 1.9895\n",
      "Epoch 15/50\n",
      "4469/4469 [==============================] - 11s 2ms/step - loss: 1.9229 - val_loss: 2.0053\n",
      "Epoch 16/50\n",
      "4469/4469 [==============================] - 11s 2ms/step - loss: 1.9026 - val_loss: 2.0005\n",
      "Epoch 17/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 1.8849 - val_loss: 1.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7072636a90>"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=batch_size, epochs=epochs, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 17, 300)           710400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_19 (Spatia (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 17, 300)           540900    \n",
      "_________________________________________________________________\n",
      "gru_16 (GRU)                 (None, 300)               540900    \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 21)                21525     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 3,171,549\n",
      "Trainable params: 2,461,149\n",
      "Non-trainable params: 710,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except:\n",
    "    print(\"no model previously\")\n",
    "    \n",
    "    \n",
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 1490 samples\n",
      "Epoch 1/50\n",
      "4469/4469 [==============================] - 13s 3ms/step - loss: 2.7085 - acc: 0.1716 - val_loss: 2.5526 - val_acc: 0.2732\n",
      "Epoch 2/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.5028 - acc: 0.2352 - val_loss: 2.3511 - val_acc: 0.3315\n",
      "Epoch 3/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.3682 - acc: 0.2998 - val_loss: 2.2801 - val_acc: 0.3423\n",
      "Epoch 4/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.3073 - acc: 0.3242 - val_loss: 2.2014 - val_acc: 0.3617\n",
      "Epoch 5/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.2583 - acc: 0.3381 - val_loss: 2.1720 - val_acc: 0.3819\n",
      "Epoch 6/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.2265 - acc: 0.3394 - val_loss: 2.1587 - val_acc: 0.4067\n",
      "Epoch 7/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1944 - acc: 0.3522 - val_loss: 2.1368 - val_acc: 0.4181\n",
      "Epoch 8/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1634 - acc: 0.3591 - val_loss: 2.0892 - val_acc: 0.4081\n",
      "Epoch 9/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1406 - acc: 0.3737 - val_loss: 2.1075 - val_acc: 0.4181\n",
      "Epoch 10/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1027 - acc: 0.3844 - val_loss: 2.0645 - val_acc: 0.4201\n",
      "Epoch 11/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1001 - acc: 0.3878 - val_loss: 2.0654 - val_acc: 0.4329\n",
      "Epoch 12/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0650 - acc: 0.3929 - val_loss: 2.0559 - val_acc: 0.4295\n",
      "Epoch 13/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0579 - acc: 0.4019 - val_loss: 2.0667 - val_acc: 0.4349\n",
      "Epoch 14/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0564 - acc: 0.3992 - val_loss: 2.0220 - val_acc: 0.4302\n",
      "Epoch 15/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0122 - acc: 0.4086 - val_loss: 2.0187 - val_acc: 0.4396\n",
      "Epoch 16/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0023 - acc: 0.4135 - val_loss: 2.0412 - val_acc: 0.4403\n",
      "Epoch 17/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9985 - acc: 0.4162 - val_loss: 2.0127 - val_acc: 0.4436\n",
      "Epoch 18/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9882 - acc: 0.4236 - val_loss: 2.0198 - val_acc: 0.4409\n",
      "Epoch 19/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 1.9406 - acc: 0.4319 - val_loss: 2.0356 - val_acc: 0.4356\n",
      "Epoch 20/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9362 - acc: 0.4406 - val_loss: 2.0268 - val_acc: 0.4436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f705af79240>"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=batch_size, epochs=epochs, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias: eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True)+K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import Model\n",
    "\n",
    "from keras.layers import Bidirectional, CuDNNLSTM, LSTM, CuDNNGRU, GRU, Embedding\n",
    "from keras.layers import Dense, Input, Dropout, Activation, Conv1D, Flatten, Concatenate\n",
    "from keras.layers import SpatialDropout1D, Dropout, GlobalMaxPooling1D, MaxPooling1D\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping,ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "embedding_21 (Embedding)     (None, 17, 300)           710400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_20 (Spatia (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "gru_17 (GRU)                 (None, 17, 300)           540900    \n",
      "_________________________________________________________________\n",
      "gru_18 (GRU)                 (None, 17, 300)           540900    \n",
      "_________________________________________________________________\n",
      "attention_5 (Attention)      (None, 300)               317       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 21)                21525     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 3,171,866\n",
      "Trainable params: 2,461,466\n",
      "Non-trainable params: 710,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except:\n",
    "    print(\"no model previously\")\n",
    "    \n",
    "\n",
    "# GRU with glove embeddings and two dense layers\n",
    "\n",
    "inp = Input(shape=(max_len,))\n",
    "x = Embedding(  len(word_index) + 1,\n",
    "                300,\n",
    "                weights=[embedding_matrix],\n",
    "                input_length=max_len,\n",
    "                trainable=False)(inp)\n",
    "\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "x = GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)(x)\n",
    "x = GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)(x)\n",
    "\n",
    "x = Attention(max_len)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.8)(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.8)(x)\n",
    "x = Dense(21)(x)\n",
    "x = Activation('softmax')(x)\n",
    "\n",
    "model = Model(inp, x)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 17, 300)           710400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_21 (Spatia (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "gru_19 (GRU)                 (None, 17, 300)           540900    \n",
      "_________________________________________________________________\n",
      "gru_20 (GRU)                 (None, 17, 300)           540900    \n",
      "_________________________________________________________________\n",
      "attention_6 (Attention)      (None, 300)               317       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 21)                21525     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 3,171,866\n",
      "Trainable params: 2,461,466\n",
      "Non-trainable params: 710,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "\n",
    "model.add(Attention(max_len))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 1490 samples\n",
      "Epoch 1/50\n",
      "4469/4469 [==============================] - 14s 3ms/step - loss: 2.7430 - acc: 0.1707 - val_loss: 2.5905 - val_acc: 0.2020\n",
      "Epoch 2/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.5748 - acc: 0.1987 - val_loss: 2.4684 - val_acc: 0.2322\n",
      "Epoch 3/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.4698 - acc: 0.2493 - val_loss: 2.3137 - val_acc: 0.3188\n",
      "Epoch 4/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.3434 - acc: 0.3048 - val_loss: 2.2061 - val_acc: 0.3570\n",
      "Epoch 5/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.2650 - acc: 0.3327 - val_loss: 2.1630 - val_acc: 0.3711\n",
      "Epoch 6/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.2222 - acc: 0.3457 - val_loss: 2.1120 - val_acc: 0.3846\n",
      "Epoch 7/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1889 - acc: 0.3576 - val_loss: 2.0916 - val_acc: 0.3960\n",
      "Epoch 8/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1338 - acc: 0.3775 - val_loss: 2.0642 - val_acc: 0.3933\n",
      "Epoch 9/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1125 - acc: 0.3815 - val_loss: 2.0648 - val_acc: 0.4027\n",
      "Epoch 10/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0925 - acc: 0.3851 - val_loss: 2.0560 - val_acc: 0.4034\n",
      "Epoch 11/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0833 - acc: 0.3871 - val_loss: 2.0249 - val_acc: 0.4128\n",
      "Epoch 12/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0542 - acc: 0.4008 - val_loss: 2.0197 - val_acc: 0.4215\n",
      "Epoch 13/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0311 - acc: 0.4093 - val_loss: 2.0106 - val_acc: 0.4188\n",
      "Epoch 14/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0160 - acc: 0.4059 - val_loss: 2.0014 - val_acc: 0.4242\n",
      "Epoch 15/50\n",
      "4469/4469 [==============================] - 11s 2ms/step - loss: 2.0009 - acc: 0.4144 - val_loss: 1.9773 - val_acc: 0.4315\n",
      "Epoch 16/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 1.9918 - acc: 0.4236 - val_loss: 1.9840 - val_acc: 0.4282\n",
      "Epoch 17/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9445 - acc: 0.4296 - val_loss: 1.9710 - val_acc: 0.4396\n",
      "Epoch 18/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9563 - acc: 0.4281 - val_loss: 1.9645 - val_acc: 0.4416\n",
      "Epoch 19/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9301 - acc: 0.4319 - val_loss: 1.9638 - val_acc: 0.4456\n",
      "Epoch 20/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9048 - acc: 0.4408 - val_loss: 1.9783 - val_acc: 0.4369\n",
      "Epoch 21/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9121 - acc: 0.4375 - val_loss: 1.9695 - val_acc: 0.4349\n",
      "Epoch 22/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.8974 - acc: 0.4473 - val_loss: 1.9835 - val_acc: 0.4409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f704f20c7f0>"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=batch_size, epochs=epochs, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 17, 300)           710400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_22 (Spatia (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 17, 600)           1442400   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 17, 600)           2162400   \n",
      "_________________________________________________________________\n",
      "attention_7 (Attention)      (None, 600)               617       \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 1024)              615424    \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 21)                21525     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 6,002,366\n",
      "Trainable params: 5,291,966\n",
      "Non-trainable params: 710,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except:\n",
    "    print(\"no model previously\")\n",
    "    \n",
    "\n",
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))\n",
    "\n",
    "model.add(Attention(max_len))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 1490 samples\n",
      "Epoch 1/50\n",
      "4469/4469 [==============================] - 34s 8ms/step - loss: 2.7301 - acc: 0.1774 - val_loss: 2.5207 - val_acc: 0.2161\n",
      "Epoch 2/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 2.5041 - acc: 0.2298 - val_loss: 2.3269 - val_acc: 0.2993\n",
      "Epoch 3/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 2.3609 - acc: 0.2873 - val_loss: 2.2491 - val_acc: 0.3403\n",
      "Epoch 4/50\n",
      "4469/4469 [==============================] - 26s 6ms/step - loss: 2.2912 - acc: 0.3262 - val_loss: 2.1848 - val_acc: 0.3611\n",
      "Epoch 5/50\n",
      "4469/4469 [==============================] - 26s 6ms/step - loss: 2.2233 - acc: 0.3468 - val_loss: 2.1001 - val_acc: 0.3846\n",
      "Epoch 6/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 2.1735 - acc: 0.3580 - val_loss: 2.0771 - val_acc: 0.3913\n",
      "Epoch 7/50\n",
      "4469/4469 [==============================] - 29s 6ms/step - loss: 2.1306 - acc: 0.3806 - val_loss: 2.0955 - val_acc: 0.4054\n",
      "Epoch 8/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 2.1003 - acc: 0.3846 - val_loss: 2.0544 - val_acc: 0.4040\n",
      "Epoch 9/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 2.0689 - acc: 0.3963 - val_loss: 2.0332 - val_acc: 0.4248\n",
      "Epoch 10/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 2.0659 - acc: 0.4028 - val_loss: 2.0264 - val_acc: 0.4248\n",
      "Epoch 11/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 2.0173 - acc: 0.4155 - val_loss: 2.0041 - val_acc: 0.4289\n",
      "Epoch 12/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 1.9870 - acc: 0.4184 - val_loss: 1.9961 - val_acc: 0.4289\n",
      "Epoch 13/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 1.9608 - acc: 0.4265 - val_loss: 1.9892 - val_acc: 0.4295\n",
      "Epoch 14/50\n",
      "4469/4469 [==============================] - 29s 7ms/step - loss: 1.9517 - acc: 0.4319 - val_loss: 1.9951 - val_acc: 0.4336\n",
      "Epoch 15/50\n",
      "4469/4469 [==============================] - 28s 6ms/step - loss: 1.9205 - acc: 0.4350 - val_loss: 1.9890 - val_acc: 0.4362\n",
      "Epoch 16/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 1.9214 - acc: 0.4457 - val_loss: 1.9853 - val_acc: 0.4403\n",
      "Epoch 17/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 1.9020 - acc: 0.4457 - val_loss: 1.9827 - val_acc: 0.4409\n",
      "Epoch 18/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 1.8673 - acc: 0.4478 - val_loss: 1.9795 - val_acc: 0.4456\n",
      "Epoch 19/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 1.8476 - acc: 0.4547 - val_loss: 1.9906 - val_acc: 0.4436\n",
      "Epoch 20/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 1.8533 - acc: 0.4567 - val_loss: 1.9873 - val_acc: 0.4510\n",
      "Epoch 21/50\n",
      "4469/4469 [==============================] - 28s 6ms/step - loss: 1.8190 - acc: 0.4659 - val_loss: 2.0006 - val_acc: 0.4483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f704f20c908>"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=batch_size, epochs=epochs, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1490, 17)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xvalid_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2553, 17)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest_seq = token.texts_to_sequences(df[train.shape[0] : ]['title'])\n",
    "xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n",
    "xtest_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21610738233031843,\n",
       " 0.2993288591004058,\n",
       " 0.3402684564958483,\n",
       " 0.36107382562336504,\n",
       " 0.38456375814924304,\n",
       " 0.39127516790524425,\n",
       " 0.4053691279968159,\n",
       " 0.4040268461176213,\n",
       " 0.4248322142450601,\n",
       " 0.4248322146050882,\n",
       " 0.42885906088272197,\n",
       " 0.42885906088272197,\n",
       " 0.4295302011822694,\n",
       " 0.4335570474599032,\n",
       " 0.43624160993819266,\n",
       " 0.44026845685587634,\n",
       " 0.4409395974354456,\n",
       " 0.4456375843726549,\n",
       " 0.443624160913813,\n",
       " 0.45100671088935546,\n",
       " 0.44832214805103787]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2553,)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict_classes(xtest_pad, batch_size=500)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 567, 1194,  234,    8,  239,   31,  136,   37,    6,   44,  216,\n",
       "        611,  467,  107,  715,  390,  123,  410,   97,  228,   99])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([207, 862,  43,   0,  92,   0,  67,   0,   0,   0,  56, 279, 238,\n",
       "        26, 225, 151,  38,  84,  45,  88,  52])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(data=pred, columns=['target'])\n",
    "sub1 = pd.concat([test, pred], axis=1)\n",
    "sub1['topic'] = sub1['target'].apply(lambda x: cl_map_inv[x])\n",
    "\n",
    "sub1.drop('target', axis=1, inplace=True)\n",
    "sub1.to_csv('submission/sub2.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I use chia seed in my protein shakes. These ta...</td>\n",
       "      <td>Bad tast</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I use chia seed in my protein shakes. These ta...</td>\n",
       "      <td>Bad tast</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don’t waste your money.</td>\n",
       "      <td>No change. No results.</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I use the book 'Fortify Your Life' by Tieraona...</td>\n",
       "      <td>Good Vegan Choice, Poor Non Vegan Choice</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I use the book 'Fortify Your Life' by Tieraona...</td>\n",
       "      <td>Good Vegan Choice, Poor Non Vegan Choice</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review Text  \\\n",
       "0  I use chia seed in my protein shakes. These ta...   \n",
       "1  I use chia seed in my protein shakes. These ta...   \n",
       "2                            Don’t waste your money.   \n",
       "3  I use the book 'Fortify Your Life' by Tieraona...   \n",
       "4  I use the book 'Fortify Your Life' by Tieraona...   \n",
       "\n",
       "                               Review Title                 topic  \n",
       "0                                  Bad tast  Quality/Contaminated  \n",
       "1                                  Bad tast  Quality/Contaminated  \n",
       "2                    No change. No results.         Not Effective  \n",
       "3  Good Vegan Choice, Poor Non Vegan Choice      Bad Taste/Flavor  \n",
       "4  Good Vegan Choice, Poor Non Vegan Choice      Bad Taste/Flavor  "
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1.columns = sub_df.columns\n",
    "sub1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not terrible, but not good. Tastes burnt and a...</td>\n",
       "      <td>Not my cup o’ joe</td>\n",
       "      <td>Burnt/ Over -roast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am so disappointed, it has no flavor, doesn'...</td>\n",
       "      <td>I am so disappointed, it has no flavor</td>\n",
       "      <td>Bad Flavor/Taste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I recently gave up my daily 6 cups of coffee, ...</td>\n",
       "      <td>Flavor was dissapointing</td>\n",
       "      <td>Bitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meh. I've adored Peruvian coffee for 20 years....</td>\n",
       "      <td>Smooth but majorly bland. Won't repurchase.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meh. I've adored Peruvian coffee for 20 years....</td>\n",
       "      <td>Smooth but majorly bland. Won't repurchase.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review Text  \\\n",
       "0  Not terrible, but not good. Tastes burnt and a...   \n",
       "1  I am so disappointed, it has no flavor, doesn'...   \n",
       "2  I recently gave up my daily 6 cups of coffee, ...   \n",
       "3  Meh. I've adored Peruvian coffee for 20 years....   \n",
       "4  Meh. I've adored Peruvian coffee for 20 years....   \n",
       "\n",
       "                                  Review Title               topic  \n",
       "0                            Not my cup o’ joe  Burnt/ Over -roast  \n",
       "1       I am so disappointed, it has no flavor    Bad Flavor/Taste  \n",
       "2                     Flavor was dissapointing              Bitter  \n",
       "3  Smooth but majorly bland. Won't repurchase.                 NaN  \n",
       "4  Smooth but majorly bland. Won't repurchase.                 NaN  "
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.read_csv('Dataset/Sample_Submission.csv')\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1.to_csv('submission/sub2.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f69ebb84c88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIcAAAD8CAYAAADt28SKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsvX+YFNd55/s93fSgBiMNIw1EjBiNNCFoLY8Aa2JGmXvzIDkKXmHHY0mIEMg6iVfk3nWyUXBmAxFrpCx6wEuM5H2yN88jEm/iK0IkbKWjNVpj9iJtbgjgjDSgiWJzZSQENLLAgtEvBhhmzv2ju1rV1edUnaqu6qru/n6eh4fp6qpz3jrnPafrvPWe9xVSShBCCCGEEEIIIYSQ5iQVtwCEEEIIIYQQQgghJD5oHCKEEEIIIYQQQghpYmgcIoQQQgghhBBCCGliaBwihBBCCCGEEEIIaWJoHCKEEEIIIYQQQghpYmgcIoQQQgghhBBCCGliaBwihBBCCCGEEEIIaWJoHCKEEEIIIYQQQghpYmgcIoQQQgghhBBCCGlipsQtAABcd911squrK24xCCGEEBIRL7300k+llO1xy0HK4TMYIYQQ0tiYPoMlwjjU1dWFoaGhuMUghBBCSEQIId6MWwZSCZ/BCCGEkMbG9BmM28oIIYQQQgghhBBCmhgahwghhBBCCCGEEEKaGBqHCCGEEEIIIYQQQpoYGocIIYQQQgghhBBCmhgahwghhBBCCCGEEEKamERkK4uL3HAeW/ccxenRMcxpzWJw6XwMLOqIWyxCCCGEEEIIITFy97YX8dqZD0uf582ajr1rl8QnECERY+w5JIRICyGGhRDfLX6+SQhxSAjxmhDiaSFES/H41OLnHxe/74pG9OrIDeex/tkR5EfHIAHkR8ew/tkR5IbzcYtGCCGEEEIIISQmnIYhAHjtzIe4e9uL8QhESA3ws63s9wD80Pb5awAel1LOA3AewJeKx78E4LyU8mcBPF48L3Fs3XMUY+MTZcfGxiewdc/RmCQihBBCCCGEEBI3TsOQ13FCGgEj45AQ4gYAywD8efGzAHAXgG8XT/krAAPFvz9f/Izi958unp8oTo+O+TpOCCGEEEIIIYQQ0oiYeg49AeA/AJgsfr4WwKiU8krx8ykAVrCeDgAnAaD4/bvF8xPFnNasr+OEEEIIIYQQQgghjYincUgI8VkAZ6SUL9kPK06VBt/Zy10jhBgSQgydPXvWSNgwGVw6H9lMuuxYNpPG4NL5NZeFEEIIIYQQQkgymDdruq/jhDQCJp5D/QB+RQhxHMDfoLCd7AkArUIIK9vZDQBOF/8+BWAuABS/vwbAOWehUsonpZS9Usre9vb2qm4iCAOLOrD53h50tGYhAHS0ZrH53h5mKyOEEGJEbjiP/i37cNO63ejfso8JDQghhJAGYe/aJRWGIGYrI42OZyp7KeV6AOsBQAixBMAfSClXCSF2AbgfBYPRFwH8XfGS54qfDxS/3yelrPAcSgIDizpoDCKEEOIbK+OlldjAyngJgL8rhBBCSANAQxBpNvxkK3PyhwDWCiF+jEJMob8oHv8LANcWj68FsK46EQkhhJBkwYyXhBBCCCGkkfD0HLIjpXwRwIvFv18H8CnFORcBLA9BNkIIISSRMOMlIYQQQghpJKrxHCKEEEKaEma8JIQQQgghjQSNQ4QQQohPmPGSEEIIIYQ0EjQOEUIIIT5hxksSJ0KIbwohzggh/tlx/HeFEEeFEK8KIf5zXPIRQgghpP7wFXOIEEIIIQWY8ZLEyF8C+FMA37IOCCHuBPB5ALdJKS8JIWbFJBshhBCiJTecx9Y9R3F6dAxzWrMYXDrf83lqQ24EOw+dxISUSAuBlYvnYtNAT40kbh5oHCKEEEIIqSOklH8vhOhyHP4/AWyRUl4qnnOm1nIRQgghbuSG81j/7Egp42t+dAzrnx0BAK2BaENuBE8dPFH6PCFl6TMNROHCbWWEEEIIIfXPzwH434UQh4QQ/0sI8fNxC0QIIYTY2brnaMkwZDE2PoGte45qr9l56KSv4yQ49BwihBBCCKl/pgCYCaAPwM8DeEYIcbOUUjpPFEKsAbAGADo7O2sqJCGEkObl9OiYr+NAwVPIz3ESHHoOEUIIIYTUP6cAPCsL/ADAJIDrVCdKKZ+UUvZKKXvb29trKiQhhJDmZU5r1tdxAEgL4es4CQ6NQ4QQQggh9U8OwF0AIIT4OQAtAH4aq0SEEEKIjcGl85HNpMuOZTNpDC6dr71m5eK5vo6T4HBbGSGEEEJIHSGE2AlgCYDrhBCnAGwE8E0A3yymt78M4IuqLWWEEEJIXFhBp/1kK7OCTjNbWfSIJDw39Pb2yqGhobjFIIQQQuqKekrtKoR4SUrZG7ccpBw+gxFCCCGNjekzGD2HCCGE+GbxY3vx9vuXS59TAtj2wELXNz8kXJjalRBCCCGEhAVjDhFCCPGF0zAEAJMSeOjpw8gN52OSqvlgaldCCCGEEBIWNA4RQgjxhdMwZGfrnqM1lKS5YWpXQgghhBASFjQOEUIICY3To2Nxi9A0MLUrIYQQQggJC8YcIoQQEhpzWrNxi9A0rFw8tyzmkP04IYTUktxw3lf2oVrL03VtFgdfP18XwfstVm0/gP3HzpU+93e3YceDd0RW393bXsRrZz4sfZ43azr2rl3iq4zccB6PPPcqRsfGAQAzp2Ww8XO3NkQ8wqTpOCFR4Ok5JIS4SgjxAyHEESHEq0KIR4vH/1II8YYQ4nDx38LicSGE+C9CiB8LIV4RQnwy6psghBBSO2bPaNF+N7h0fg0laW42DfRgdV9nyVMoLQRW93UmfsFDCGkscsN5rH92BPnRMUgA+dExrH92JLYYdCp59h87V9pyawXv35AbiUU+E5yGIQDYf+wcVm0/EEl9TsMQALx25kPcve1F4zJyw3kM7jpSMgwBwPkL4xj89pG6j0eYNB0nJCpMtpVdAnCXlHIBgIUAPiOE6Ct+NyilXFj8d7h47F8DmFf8twbAn4UtNCGEkPg49PDdFQailACeWMFsZbVm00APjm2+B8e3LMOxzffQMEQIqTlb9xzF2PhE2bGx8YnYYtCp5FGR5OD9TsOQ1/FqcRqGvI6r2LrnKMYnK2PejU/Iuo9HmDQdJyQqPLeVSSklgA+KHzPFf27RLj8P4FvF6w4KIVqFENdLKd+qWlpCCCGJ4NDDd8ctAiGEkASgizUXVww603oZvD9c3Nq93uMRJk3HCYkKo4DUQoi0EOIwgDMA9kopDxW/eqy4dexxIcTU4rEOAHZT/KniMWeZa4QQQ0KIobNnz1ZxC4QQQgghhJA40MWaiysGnWm9DN4fLm7tXu/xCJOm44REhZFxSEo5IaVcCOAGAJ8SQnwCwHoAtwD4eQBtAP6weLpqpq0wzUspn5RS9kope9vb2wMJTwghhBBCCImPwaXzkc2ky45lM+nYYtCp5FGR5OD9/d1tvo5Xy7xZ030dVzG4dD4yqcplYCYt6j4eYdJ0nJCo8JXKXko5CuBFAJ+RUr4lC1wC8N8AfKp42ikA9tn2BgCnQ5CVEOLChtwIutc/j651u9G9/vlEB1okhBBCSGMwsKgDm+/tQUdrFgJAR2sWm+/tiS0GnUqe/u62ugrev+PBOyoMQVFmK9u7dkmFIchvtrKBRR3YunwBWrOZ0rGZ0zLYev+Cuo9HmDQdJyQqhPTYbyuEaAcwLqUcFUJkAXwfwNcAvCSlfEsIIQA8DuCilHKdEGIZgN8BcA+AxQD+i5TyU7ryAaC3t1cODQ2FcDuENCcbciPKlNZJf/ghhDQPQoiXpJS9cctByuEzGCGEENLYmD6DmXgOXQ/gBSHEKwD+CYWYQ98FsEMIMQJgBMB1ADYVz38ewOsAfgxgO4B/F0B+QogPdBk3kpyJgxBCCCGEEEJIMjDJVvYKgEWK43dpzpcAvly9aIQQU3QZN5iJgxBCCCGEEEKIF75iDhFCkoku4wYzcRBCCCGEEEII8YLGIUIaAF3GjSRn4iCEEEIIIYQQkgw8t5URQpKPFXR656GTmJASaSGwcvFcZTDqVdsPYP+xc6XPUWa/IIQQEj5CiG8C+CyAM1LKTzi++wMAWwG0Syl/God8YaFKthDVb1ZuOI+te47i9OgY5rRmMbh0fqIzEdWbvKQx2JAbMXrWjINmGBOq9u+9sQ2PPPcqRsfGARQyxG383K11fe8ma5Vm6O848MxWVguYKYOQ2uCcbC1oICKERA2zlYWHEOIXAXwA4Ft245AQYi6APwdwC4DbTYxDSX0G02XhBML/zcoN57H+2RGMjU+UjmUz6cSmqq43eUljkOTMuM0wJnTtLwA4V/OZtMDW+xfU5b2brFWaob/DJsxsZYSQBkE12bodJ4QQkjyklH8PQDVxPw7gP6ByrVB3uGXbDPs3a+ueo2WLDAAYG5/A1j1HQ60nLOpNXtIYJDkzbjOMCV07qyb78QlZt/duslZphv6OCxqHCCGEEELqHCHErwDISymPGJy7RggxJIQYOnv2bA2k808ts22eHh3zdTxu6k1e0hgkOTNuM4wJv+3cSPfupBn6Oy4Yc6hJYJwZQgghpDERQkwD8DCAXzY5X0r5JIAngcK2sghFC0xaiJotOue0ZpFXLCrmtGZrUr9f6k1e0hjoxmQSMuM2w5jwOyc20r07aYb+jgt6DjUBqr2b+4+dw6rtB2KSiMRFf3ebr+OEEELqgm4ANwE4IoQ4DuAGAC8LIX4mVqmqwC3bZti/WYNL5yObSZcdy2bSGFw6P9R6wqLe5CWNQZIz4zbDmNC1s8o0l0mLur13k7VKM/R3XNA41AQwzgyx2PHgHRWTLr3ICCGkvpFSjkgpZ0kpu6SUXQBOAfiklPInMYsWmE0DPVjd11lxPIrfrIFFHdh8bw86WrMQADpas4kObFpv8pLGwBqTlqdQWohEBKMGmmNM6Nr/8RUL0ZrNlM6bOS1Tt8GoAbO1SjP0d1wwW1kT0LVut/a741uW1VASQgghzQqzlYWHEGIngCUArgPwNoCNUsq/sH1/HEBvPWcrI4QQQkg4mD6DMeYQIYQQQkgdIaVc6fF9V41EIYQQQkiDwG1lTQDjzBBCCCGEEEIIIUQHjUNNAOPMEEIIIYQQQgghRAe3lTUJNAQRQgghhBBCCCFEBT2HCCGEEEIIIYQQQpoYT88hIcRVAP4ewNTi+d+WUm4UQtwE4G8AtAF4GcCvSykvCyGmAvgWgNsBvANghZTyeETyE0IIIYSQBmZDbgQ7D53EhJRIC4GVi+cmIn22G7nhPLbuOYrTo2OY05rF4NL5iUyz7Kdto+yH3HAejzz3KkbHxgEU0nFv/NytiWwzP9RCd526duct7XjhR2dddS83nMf6Z1/B2PgkACAlgF9b7D8tfdL13Kv9kzC3+GnDpLd3EFZtP4D9x86VPict9EkSdKSWmHgOXQJwl5RyAYCFAD4jhOgD8DUAj0sp5wE4D+BLxfO/BOC8lPJnATxePI8QQgghhBBfbMiN4KmDJzAhJQBgQko8dfAENuRGYpZMT2HhPYL86BgkgPzoGNY/O4LccD5u0crw07ZR9kNuOI/BXUdKhiEAOH9hHIPfPpK4NvNDLXRXpWtPHTzhqnu54TzWPn24ZBgCgEkJ37IlXc+92j8Jc4ufNkx6ewfBaRgCgP3HzmHV9gMxSVROEnSk1ngah2SBD4ofM8V/EsBdAL5dPP5XAAaKf3+++BnF7z8thBChSUwIIYQQQpqCnYdO+jqeBLbuOYqx8YmyY2PjE9i652hMEqnx07ZR9sPWPUcxPikrjo9PyMS1mR9qobsqXXPi1L2te45iUnOuH9mSrude7Z+EucVPGya9vYPgNAx5Ha81SdCRWmMUc0gIkRZCHAZwBsBeAMcAjEoprxRPOQXA8mnrAHASAIrfvwvg2jCFJoQQQgghjY/1xtb0eBI4PTrm63hc+GnbKPvBrV2S1mZ+qIXumraP/Ty3a/zIlnQ992r/JMwtftow6e3diCRBR2qNkXFISjkhpVwI4AYAnwLwr1SnFf9XeQlVtKAQYo0QYkgIMXT27FlTeQkhhBBCSJOQ1jif644ngTmtWV/H48JP20bZD27tkrQ280MtdNe0feznuV3jR7ak67lX+ydhbvHThklv70YkCTpSa3xlK5NSjgJ4EUAfgFYhhBXQ+gYAp4t/nwIwFwCK318DoMI3TEr5pJSyV0rZ297eHkx6QgghhBDSsKxcPNfX8SQwuHQ+spl02bFsJo3BpfNjkkiNn7aNsh8Gl85HJlW52MqkReLazA+10F2Vrjlx6t7g0vnaBaAf2ZKu517tn4S5xU8bJr29g9Df3ebreK1Jgo7UGk/jkBCiXQjRWvw7C+CXAPwQwAsA7i+e9kUAf1f8+7niZxS/3ydlA/texUBuOI/+Lftw07rd6N+yr64DkRFCCCGE6Ng00IPVfZ1lb/tX9/nPqlRLBhZ1YPO9PehozUIA6GjNYvO9PYnLKuSnbaPsh4FFHdi6fAFas5nSsZnTMth6/4LEtZkfaqG7Kl1b3dfpqnsDizqwbcVCZDMfLQNTAr5lS7qee7V/EuYWP22Y9PYOwo4H76gwBCUpW1kSdKTWCC+7jRDiNhQCTKdRMCY9I6X8YyHEzfgolf0wgNVSyktCiKsA/N8AFqHgMfSrUsrX3ero7e2VQ0NDVd9MM2BFqrcHJMtm0nU/ORBCmi9dJmkuhBAvSSl745aDlMNnMEIIIaSxMX0Gm+J1gpTyFRQMPc7jr6MQf8h5/CKA5YZyEp+4RaqncYiQ+sVKl2lhpcsEQAMRIYQQQgghJFJ8xRwi8cNI9YQ0Js2YLpMQQgghhBCSDDw9h0iymNOaRV5hCGKketIsNOrWq2ZMl0kIIYQQQghJBvQcqjMaMVI9IaZYW68sg4m19WpDbiRmyaqnGdNlEkKCIYT4phDijBDin23HtgohfiSEeEUI8bdWMhFCCCGEEBPoOVRnWHGFtu45itOjY5jTmsXg0vmMN0SaAretV/XuPbRy8dyymEP244QQ4uAvAfwpgG/Zju0FsF5KeUUI8TUA6wH8YQyyAagvL8/ccD7U56qwywujPpNz4uyzMOuuB92LSkdyw3k88tyrGB0bLx2bOS2DjZ+7VVt+WO1Va72PQgb79ddkMxACGL0wXtX9rNp+APuPnSt9rmU2Ll3fhjVnRI1Tn710OUqCjJMktKFfaByqQwYWdSResUiyqcfJCmjsrVfWD0zSH2gJIfEjpfx7IUSX49j3bR8PAri/ljLZqacA+84ssPnRMax/tuCNGuR3MezywqjP5Jw4+yzMuutB96LSkdxwHoO7jmB8svyZ6PyFcQx++4iy/LDaq9Z6H4UMzuvtBrag9+M0DAHA/mPnsGr7gcgNRLq+fePsB3j5xLtVzxlRo9JnN12OkiDjJAltGARuKyOkybAmq/zoGCQ+mqxyw/m4RfOk0bdebRrowbHN9+D4lmU4tvmexDzIEkLqjt8C8D/iqryeAuy7ZYFNQnlh1GdyTpx9Fmbd9aB7UenI1j1HKwxDFuMTUll+WO1Va72PQgbV9UHLsnAahryOh4muD/cfOxfKnBE1On3W6XKUBBknSWjDINA4REiTUa+TFaDfYsWtV4QQUkAI8TCAKwB2uJyzRggxJIQYOnv2bOgy1JOXZ9hZYGudVdakPpNz4uyzMOuuB92LSke8rld9H1Z7JSGbcrUymJxXT9mhq+nDJPdnreUAgo2TJLRhEGgcIqTJqNfJCih41qzu6yx5CqWFwOq+TnrYEEIIACHEFwF8FsAqKfVPrVLKJ6WUvVLK3vb29tDlqCcvT12216BZYMMuL4z6TM6Js8/CrLsedC8qHfG6XvV9WO1Va72PQgaT8+opO3Q1fZjk/qy1HECwcZKENgwCjUOkYbh724voWre77N+q7QfiFitx1OtkZcGtV4QQUokQ4jMoBKD+FSnlhThlqScvz7CzwNY6q6xJfSbnxNlnYdZdD7oXlY4MLp2PTEq9WM2khbL8sNorCdmUq5VBdX3Qsiz6u9t8HQ8TXR/2d7eFMmdEjU6fdbocJUHGSRLaMAgNHZA6zujwpLbcve1FvHbmw4rjtQr6Vk8MLp1fFiANqI/JyhSnLsybNR171y6JTyBCCAkZIcROAEsAXCeEOAVgIwrZyaYC2CsKbzMPSin/jzjkq6cA+2Fnga11VlmT+kzOibPPwqy7HnQvKh2xrveTrSys9kpCNuVqZXBeH0a2sh0P3hHbetStb70S0ySpP5OQrSzIOElCGwZBuHgd14ze3l45NDQUapmq6PAADUSNSte63a7fH9+yrEaS1Af1mq3MC52RkAYiQuJHCPGSlLI3bjlIOVE8gxFCCCEkOZg+gzWs51Cc0eFVNOpinNQnA4s6GlL/VIYht+OEEEIIIYQQQhrYOJQkrNTh1jYeK3U4gIZcoBNCCCGEEEIIIaR+oHGoBrilDqdxKBzmzZqu9Q6pRdA3QpLKhtxIomMvEEIIIYQQQuKnYbOVxRkd3kk9pw6vF/auXYJ5s6ZXHGeMqeZCpQNuxxudDbkRPHXwBCaKseUmpMRTB09gQ24kZskIIYQQQgghSaJhPYfijA7vZE5rFnmFIaheUofXCww4TPauXcJsZTZ2HjqpPU7vIUJIHNi9Ge0IAKv6OkObm3LDeW2Wm1r+TtTKezNobEvddSq5e29sc63D2eZOrHZ2k9XrPkzvM2gdYcQIbbY4o9X2iXXcuVYyXbeF3d5ecnrpjZXl7PyF8nEwvSWNx77Qo5UtCr3xKtN0fjKRLYj8urnYrSxV0qkgc6uzjjtvaccLPzprPPfceUs7vnvkLV+Z1OphbvDMViaEmAvgWwB+BsAkgCellN8QQjwC4EEAZ4un/pGU8vniNesBfAnABIB/L6Xc41ZHo2fKcMYcAgqpwzffq58gCCGkWtyy+NVzBj/Vg8HqEBeVJBqYrSyZ1PIZzPJmdCOMsZwbzmNw1xGMT5Y/42bSAm3TMnj7/csV10RhINLdb9jzVdDnTN11n+y8RpnAJZ0SmLC1qb0OXZs7mT2jBe9dnFDKCsD1Pkzv0+08tzq86jeh2Z75q+2T+27vwHdeyleE37DwMhCF3d5+5HTTGx3plMDXly9QGlbC1huvMk3nJxPZgsivyzDsNkfsGjrhmlzKdG5VyevEa+5RkUkLbL2/sn91ddZybjB9BjPZVnYFwFeklP8KQB+ALwshPl787nEp5cLiP8sw9HEAvwrgVgCfAfB/CSHSge6iQRhY1IHN9/agozULAaCjNduwPxKEkOSQFsLX8XpAZRgCwO1yhNQBOm9Gv+d4sXXPUaWRYnxCKg1DQDRZLd28N8PELbZlkOt0i68JR5va69C1uZO337+sldXrPkzv0+28oN+ZEkYZ9US1fbLz0EnXBbdXlumw29uPnG56o2NiUipli0JvvMo0nZ9MZAsiv27OdZsjvPTBdG416TOvuUfF+IS6f3VlJHFu8NxWJqV8C8Bbxb/fF0L8EICbVePzAP5GSnkJwBtCiB8D+BSAAyHIW7c0aupwQkhyWbl4rvKt0MrFc2OQJhzcHgy4XY6QZOPcShb0HC+SEtNRdy9h3KOdoLEtw2gnq4xqy3K73qsO5/Eg7RH0O9Nzk6KTYVNtn1Q7FsJub79yBqlHdU0UeuNVpun8ZCJbLfTepCxTfTKVK8j85rctkjY3+ApILYToArAIwKHiod8RQrwihPimEGJm8VgHALvZ7hQUxiQhxBohxJAQYujs2bPOrwkhhFTJpoEerO7rLHkKpYVo6O1XYS+4CCHhYuK1GIZnY1JiOtbKe1N3v17tEEY7WWVUW9ac1qznfZjep9t5Qb8zJYwy6olq+6TasRB2e/uV001v/NQRhd54lWk6P5nIVgu9NynLVJ9M5Qoyv/lti6TNDcbGISHExwB8B8BDUsr3APwZgG4AC1HwLPq6dari8oondinlk1LKXillb3t7u2/BCSGEeLNpoAfHNt+D41uW4djmexrWMASEv+Batf0AutbtLv1btb2pHWAJqRoTr8UwPBsHl85HJlU5H2TSArNntCiviSKrpe5ewvbeHFw6H9lMeQSHbCaNwaXzA12ny+ybdrSpvQ5dmzuZPaNFK6vXfZjep9t5Qb8zJYwy6olq+2Tl4rkVx+14ZZkOu739yOmmNzrSKaGULQq98SrTdH4ykS2I/Lo5122O8NIH07nVpM+85h4VmbS6f3VlJHFuMMpWJoTIoGAY2iGlfBYApJRv277fDuC7xY+nANh75gYAp0ORlpA6oVbZSeKkGe6RJI/+7jbt1rIwF1yq2Eb7j53Dqu0HYsl6SUgjYP1GRJ2tzNrGH3e2Muf9RvVbad2v3yw4btf5zVamanMnJpmI3O7D9D5Nzgv6nRdB+6JeCaNPLL0Kkq0s7PY2kdNLb4JkK4tCb7zKNJ2fTGQLIr9bhmHdHDGwqCOUbGUqed2ylenO95OtrF7mBpNsZQLAXwE4J6V8yHb8+mI8Igghfh/AYinlrwohbgXw1yjEGZoD4P8BME9KqY3i1OjZykhzUavsJHHSDPeYVOohDWbU1CJbWaNmeosTZitLJnwGI4QQQhob02cwE8+hfgC/DmBECHG4eOyPAKwUQixEYcvYcQC/DQBSyleFEM8A+BcUMp192c0wREij4Rb9v1EMJ81wjzqchrEw33Z74UyDmR8dw/pnCxm6mslARM8dQgghhBBCwsUkW9k/QB1H6HmXax4D8FgVchFSt9QqO0mcNMM9qlB5TEmgdCxqA5FbGsxmMg4RQgghhBBCwsVXtjJCiDe1yk4SJ81wjyp0HlNe34VFvaTBbAR0QQ+9giESQgghhBBSj9A4REjI1Co7SZw0wz2qcPOMqsZrKjecR/+Wfbhp3W70b9mH3HBeeV69pMFsBHY8eEeFIcgkOCYhhBBCCCH1iFG2MkKIObXKThInzXCPKtJCaI1AQb2m/MQRGlw6v+xcIJlpMBsFGoJIUhFCfBPAZwGckVJ+onisDcDTALpQiAX5gJTyfFwyEkIIIaS+8MxWVguYKYMQUg/osrQBwbNl9W/ZV5G+FQA6WrPYv+6uiuPMVkbqFWYrCw8hxC8C+ADAt2zGof/DjuYrAAAgAElEQVSMQmbZLUKIdQBmSin/0KusqJ7B/M5V9vPtqaAFCrHdgEKq4GW3Xa9NN2wih5WuOD86VlH2xs/dis3P/wvefv9y6frZM1pw6OG7A8+91bRDtXO8PR29Cqc3pNv5VjuEgb0ek5dLix/bq+wTO7p2U/W/Pf10SgCTsvCbG6Stndkz+7vbsLy3M1Af+m0X1TV9N8/E8XfGXOt2tknXtdmKDKBuadftde84eKI0hkyu0cngdzzZn5uqeUEZRA5Vn4f9MimILkRVl2peMJFJ1bZDb55T6mt+dKz0AtZtLLr1l9t9+OnnqObsuJ/fTZ/BaBwihBAfhJ2t7KZ1u6GahQWAN5gynTQQNA6FixCiC8B3bcahowCWSCnfEkJcD+BFKaWnW2EUz2BOj0ig4OW4+V71wlF1vilRlWvn6qlpjE/C+H7c6vcrr0k9KtxeZtixFrYm54dhINLVo3vB4jQMqWTRtdt9t3fgOy/ljfvfb1s7jQQWlsHJT7l+28XtGjvOuv2MiXRK4OvLFyjl1tXtdo1FUD33kt3vS7ogcuj6PEwDURBdiKouLx3TyaRq2xSASUO5VP3g1l9Db57T3kfvjW3G/RzVnB3m3B4U02cwxhwihBAfbBrowfEty0r/3tiyrKofa8YRIoSExGwp5VsAUPx/VlyCuGVWND3flKjKtfPepQlf9+NWv195TepRYZokwVrompyvMtL4RVeP7riuTvtxXbvtPHTSV//7bWuVkQAoNwyZluu3Xby+09XtZ0xMTEqt3Lq63a5xkyHoeDKRKUw5dH2uOx6EILoQVV1edeq+V7WtqWEIUPeDW3+53Yeffo5qzg5zbo8axhwihJAYYRyh+qcWLuaEhIkQYg2ANQDQ2dkZevl+MytWm3ExrkyOXuWH1Q5B7sNvkoRqkiqEUU819evaJ0iZUemMV7lB2sX0/ux1+72/IG0b9rgw/d5vfyc1A2wUYyRoXV516r4Pow2dZbj1l07KCSl99XNUc3ZSdU0FPYcIISRGBhZ1YPO9PehozUKgEPeglm6mpDpULub7j53Dqu0HYpKINDFvF7eTofj/Gd2JUsonpZS9Usre9vb20AXx6xFZradkXB6YXuWH1Q5B7sNvkoSgSRX8oqunmvp17ROkzKh0xqvcIO1ien/2uv3eX5C2DXtcmH7vt7+T6rkdxRgJWpdXnbrvw2hDZxlu/eV2H376Oao5O6m6poLGIUJI1ZimYg/Cqu0H0LVud+lfIy66BxZ1YP+6u/DGlmXYv+4uGobqiFq4mBNiyHMAvlj8+4sA/i4uQQaXzkc2ky475uYRqTrflKjKtXP11LSv+3Gr36+8QT1JVy6ea3Ref3eb8fmzZ7T4lsOJrh7dcV2d9uO6dlu5eK6v/vfb1lbbOUk51qkm5fptF6/vdHX7GRPplNDKravb7Ro3GYKOJxOZwpRD1+e640EIogtR1eVVp+57Vdv6MTqo+sGtv9zuw08/RzVnhzm3R036kUceiVsGPPnkk4+sWbMmbjEIIQGwgqydu1DY///+xSv4X//fWdwwM4tbrr+6qrJVXhknz4/hn954B/fdHv6PJCF+eeJ/vqb97qFf+rkaSpJ8Hn300bceeeSRJ+OWoxEQQuwEsAlA56OPPvrbjz766LsA/hzAukcfffQ/ArgOwO898sgjnj7rUTyD3XL91bhhZhYj+XfxwcUr6GjN4quf+7jW8O08vzWbQbYljYvjk7Cvs2dOy+D+22/AOx9cDlRuR2sWn184B+98cBnvX7xSUfaWe2/D8Ilz+PDyR9t8Z89owctfXerrfsJqB9N6VNx1y2z89INLeDX/nnbLhX0LrNf5YWUrc9aTFsI1qcODv9iNnYferOgTuyy6dvt3d/6ssv9PvHMBF68UIqCkRCFjXZC2vu/2ufinN97ByfMfDbP+7jasvXu+7z702y66a36huw2TEtq6VW11W8fVZfcAFDKPfe2+27RyW3WPnHrX+Bo3GfyOp/cvXikdN2mrsOTQ9XmYW8mD6EJUdenmBS+ZVG37x5//BK6b0aLU1/cvXkFaCNex6NZfbvfhp5+jmrPDnNuDYvoMxmxlhJCq8JuK3Q9d63ZrvzvOTF4kASRdR+NOnWqH2cqSCZ/BCCGEkMbG9BmMAalJ05CkRVIjUU9B1pLEhtwIdh46iQkpkRYCKxfPjeStEImW/u42bVrbuHGmTs2PjmH9syMAwLmPEEIIIYSUQeMQaQpquUhqtkX/nNas0nMoiUHW4kClDwDw1METpXMmpCx9bmRdUVHvRtsdD96R2GxlbqlT66mNCSGEEEJI9NA41MTU+6LMD7VaJG3IjTTdoj/KVOw6r4ypU1K4ad3uxOutTh90uR92HjrZsHqiolE8W5JgCFJBrz5CCCGEEGJKU2QrizKTUr1iLcryo2OQ+GhR1qhtU6tF0s5DJ30drwUbciPoXv88utbtRvf657EhNxJq+VGmYt/x4B0V23NSArh0ZbIu9FbX77pIbxMJiAFXS9yMtqR66il1KiGEEEIIiRdPzyEhxFwA3wLwMwAmATwppfyGEKINwNMAugAcB/CAlPK8EEIA+AaAewBcAPAbUsqXoxHfm0Z5Mx02zbbdoFZbn3SL+7gW/bXyZBpY1BGZ3ti9MlTBr8PS2yg86fz2e1rofIoaE3q2REuUXn2EVIvfOVd3fm44j7VPH8ak7dwpKYE/Wb6gojyrjPzoGNJClM3Rzm3gzi3BfTfPxPF3xpAfHYPAR0b+mdMy2Pi5W7Wy2+s0wXQ7em44j8FdhzE+WfmdJV9rNgMhgPMXxstkBoBMCpiQwKQsnD+tJY0Llycq+sLe7tcUyxu9MF46D0BZv3Rdm8XB18+XbaXuvbHNta9zw3k88tyrGB0bLx1LCeDXFheyDQXdrq9qo6lTUljeewNe+NFZV90z1U/VeQDK7seuI/bzr8qkcOnKJCYVjwrTW9J47As9+K8vvIbXznxYOj5v1nTsXbuk7Fx7+6hw9r1pv0SBKgtth4/xbx+7HQ4dVH1nlelsI1Wb9N08E6+efr/Ub9MyKUzNpDF6YRxTUqjQIyszm0p//TAlJTAxKQP3gbP+lCiMa5N2BSr7xL413ln2zGkZLLvterzwo7Pa9i6svV/B2PhHGQF/bXEn3jj7QUU9N7V/zFV3AWj7FPAO5WH/3j7P2cee6rqgawI/1yV1B49ntjIhxPUArpdSviyEmAHgJQADAH4DwDkp5RYhxDoAM6WUfyiEuAfA76JgHFoM4BtSysVudUSZKSPKTEr1gmrg7Dh4Qum9IAC8kYAMO9Wgut/eG9uUi6SwPFwsutc/r5zg0kLg2OZ7QqunXuWplpvW7Y5Eb51GZCAc/dC1v47VEaUqTSqcn6MnSQ8fzFaWTOLIVuZ3ztWdf9/tHWUvQOwIAI+vWFhm5HCWoWJ1XycAaMtVkUkLbL1fbYwyqVMnh+73IDecx0NPH/ZdpilWXwBwlT+TEoAAxifcf+fSxcWvs3xrITm46wjGVRYSFIwhduOIhdfvpZ82cuqeqX6qzsukCgtZ5+1k0gIrfn4uvvNSPpA+2LEbiJwvAf3g1i9RoDIMmdTtNo7cdNAqc+jNc4HbyA3L6PH0D05q9dcvfvvAa/x4lafrk/7uNizv7XQtW1XXfbd34K8PnoDCZh0K9vvR6b41N/gdG9Z1QdcEfq6Lat3hhukzmOe2MinlW5bnj5TyfQA/BNAB4PMA/qp42l+hYDBC8fi3ZIGDAFqLBqZYaPY309bAsBaolufItJa08vx6326gu9+hN89FtvXJjhVs2PR41CTNk6laotomE9X2Jl2/r+7rxOq+zpKnUFqIpjMMAQXPlmymfC6iZ0u4DCzqwP51d+GNLcuwf91diXgrRYjfOVd3vtuWbVm8zq0MFTsPnfS9FXx8QiplN61TJ4eOqLfeWn3hJf/4pPQ0DAEoM0DYywcK9+K2+FQZhgDv7fp+2sipe6b6qTpvfLLSMAQUdGTnoZNVG4aA8japJmyBW79Egc4w5FW3mx666aBVZlShHSZlof3DMgwB/vvAa/x4lafrk/3HznmWrapr56GTkRmGrDqs+/EK5eG3363zg64J/FyX5LAKvgJSCyG6ACwCcAjAbCnlW0DBgCSEmFU8rQOAvTdOFY+9Va2wQYgzk1IS3tjqBsaFyxPIZtINt93AbaLYNBCdNdbCWtwnJVuZ023efrweiWqbTFRGZC99aDZjkBNrPMY9T+potsyDhNQKv3Ou7rjXiw77dabzedCXJ6ryq/kNcZOjFi84o67DKj9oPX763o88btc6j/utI4oXc2GXGefL8yiexU4XY6tGRRR96ud+Tc4N2n5BrqvFy2dLLq8X4H5lsc4Pqod+rkuy84qxcUgI8TEA3wHwkJTyPaFfXKq+qOgdIcQaAGsAoLOz01QM3/hZTIZpzElKrCPdwJAANt/bk9hFWVCS4CmzaaAnMQvIlYvnKl0q4/JkqpaojAlRGpGTpA9JJMp4VdXQqJkHk/DSghC/c67ufN0LEFV5ujJUZQL+nxtUspvW6SaHrq6g5Zpi3U9U9VjlB70Xr5dcfss10RVnH/utw0tfgxB2mXHuIPA7/k3L/Mm7FyNbB0TRp376wKRtgvZpkHaPoj2cWPfj9QLcryzWdUHXBH6ui9N5xQujbGVCiAwKhqEdUspni4fftraLFf8/Uzx+CoB95XkDgNPOMqWUT0ope6WUve3t7UHl98TKpJTNfHSrl65MYOjNcje6sLN3JcVdTPfjmRaiIbcbuN1vM7JpoMf39qXFj+1F17rdpX+LH9tbK3GNiEJvub2JOKk28+Cq7QfKxtGq7QfCFC8QzZalkiQXv3Ou7ny3Fx2ieJ1bGSpWLp7r+wVKJi2UspvWqZNDR9S/TVZfeMmfSQlk0t7PV+lU+Tn2vh5cOr8QN0bDvFnTlce9+shPGzl1z1Q/VedlUgKq28mkC96nQfXBjr1NqnnZ59YvUeDMPmtat5seuumgVWZUL0RTotD+bvrrF7994DV+vMrT9Ul/d5tn2aq6Vi6eG2kqdPv9eIXy8Nvv1vlB1wR+rkvyusOz/4rZx/4CwA+llNtsXz0H4IvFv78I4O9sx/+NKNAH4F1r+1lcDL15rhQxHSjsEX3q4ImylN5hG3OS4i6WtBg41eKVlr3R7jcMNg304Njme3B8yzL03TwTTx08oV2wLn5sL95+/3LZsbffv5w4A1HYWEbkqGNSNSq54Tz6t+zDTet2o3/LvoYwNlTjhagK8Lj/2LlQDURB2jwpLy0I8Tvn6s7fNNCDJ1YsrHiYnZISZcGonWUAlS+N7C9PVC9W+rvbStfar5w5LaMMRq2q0wSTlzgDizrwxIqFyGie4i35WrMZzJyWqZAZKGQrs9Z9AoXsWM6+cLa7VZ513tblC7D1/gVl/dLf3VbxQurryxdo+3pgUQe2Ll+A1mymTL6UKMTn27t2SaAYfbo2mjolhdV9na66Z6qfqvO2Ll+AbQ8sLLsfS0c2DfSUnZ/NpJSGJBT744kVCyuMY85sZU5dVeH8xqRfomDHg3cojRF+xr8lv3WdXQed31llqtpI1Sb93W1l/TYtkyrpu0qPtj2wEJsGepT664cpKRG4D1Tjx9Ipk/JUfWJlK1OVPXNapjR+gMr23jTQg20rFpY5ZVhjWVWPl+6q6rDux+sFuPN7+zxnH3vO64KuCfxcl+R1h0m2sv8NwP8LYAQoxZj6IxTiDj0DoBPACQDLpZTnisakPwXwGRRS2f+mlNI1DUbUmTJMMjaFnQUpSVl4oo6bUau4HF5R6WstT73hlpHASlnZtW639vrjdZ7FLip0KTubRefiyLgQFLd0rU6qyfQX9TjStbmcnMRFW2DOq6em8cqjnyl9jirbnynMVpZM4shWRgghhJDaYfoM5hlzSEr5D1DHEQKATyvOlwC+7ClhDTF5Axz23r+oAucGIcqYJ7WMy6HbzvHUwROlOq3FXrMszP3glpGABCM3nMfapw+XZWawPBOB+o5NY4qbN0qSjENu3jwqA1GS43Xp2tzJe5cmcNvG75UMREne404IIYQQQuLFV7ayesUkY1PYxpykZ+EJC6/sYNWg2uLkhdtiD/DnOdBs+NkKRO+sAlv3HNWm7PQzBuo5SHBSttB64dc4mrTMg3b8tO17lz76TUvSSwtCCCGEEJIsmsI4ZPoGeOqUVOmheea0DDZ+7taqFmhJzcITJlFlBwtiGLLQLfb8eg40E9Y2FR2zZ7SU/g7TW6zejXVui3TTMZCUzIZB8fJGcfbx1CkpfO2+2+ri3oJ6XfZ3t2m3cIZB0MwtzfLSghBCCCGE+CfKgOKJwStglbU4Gx0bL11zcVznD0DsRJUdLKhhyI1m31bllpFAtU3FYvaMFhx6+O7S52qzOFnUImhv1LhtxzEdA/UeJNgt44Kqjy9dmcTaZw43RNBqHW4BHoOwITeCm2yZz86+f9EoQ5CKRsxSSQghhBBCqqcpPIcA9zfA9RIzI4nEEZdDt02QuLPjwTu0njo3aQLoCqDMMASE5y3WCMa6waXzK2IOWZiOgaRuy9LpimpL4eZ7e/DIc6+WDOxXFbNU6PpyUqLm82vU3jxA5XZLk4w6JmU659jLExICBQ/X0QvjmNOaxTvvXywLRm1x9dTq0yaT+kII8fsA/i0AiUIykd+UUl6MU6bccL5sjgAKwfsnZSFRx523tOM7L50qyywrAKwqjiHV9Xamt6Tx2Bd6MPTmOe1WUGcZM6dlsOy26/HCj84iPzpWerboaM3iysRExUuqTArQvTcUxftRDMEyrPnG63dO5b1+97YX8dqZD0ufZ89owZR0WukF6PS+nj2jBevv+bjSa9C+rfmqTKqiD36huw3H3xkL5K1oMS2TwtRMGucvjEMAysD4U6ekcPnKZEm2XUMnPNupozWLrmuzOPj6edftv86t213XZrVlW/IJFNLPX7Z1qm5XgTMxhVu59s/TWtL48PJEme4NLp2v1GOgfJvzdR/LVPTxTz8YL7um98Y2V09RZ7vceUu7cjxYxy0duXRlEpMSZe1t//2z0I2ZtBDou3kmjr8zVibb0JvnsOPgiVI7mYxrN5xj3ppz7AR9cbMhN6KU1akbznZx6kGHol+c4zclgDtubsM/HjunTSphHc+kCvOQs3/sqEIZAGZexapnkpQAtj1QyA7p/N4+j6vq7ro2W3ZfqnZ09qM1n1jPP05ZVc+pzrFw5y3t+O6Rt8p+U9zGtx+Pa13oDZ0+qnQgTjyzldWCuDNlxJ3Bpd6JIv6MSbYfk+xbfstMIrWIR+Mnu141WZzs1Gt/OKk2W1lcmQ3dxq1ubM2e0aL06uvvbsPLJ96tiGWj80YD4plfo9zGaJpN0S+68QZU6shtG79XFmPIma0sbpitLHqEEB0A/gHAx6WUY0KIZwA8L6X8S901UT+D5YbzGNx1BOPOVZkh/d1t+MEb5wNfv7qvE703tlUlQxxk0gJb71+AgUUdFYYhFVaWyM3P/4uR93U2k8Z9t3fgOy/lXefqekS1OyCse7T3i1W+7iVREFJAaGWlUwITNp23ZxINs13mzZruqZ9e+L1vr99WP/OO32cB3e99OiXw9eUf6YbuPCf2fqkmrIYOr/GQSQlAAOMTal2x8Lof3Us4S4beG9uMdM7ejib9aJfVrW8mDHRBNb79ZOTV1e/1O1aLLL+hZStrBqrJ4BJVYN56ClAbRTY03SLUHvvGzRNGRS08B8JGFY/moacP46GnD4eqb34C1SY5i1McVBtbLI4gwV5xo3Q/7roHFtX5Xj/+cWTIijKeVVTB+d088pzeZUkyBJFYmQIgK4QYBzANwOk4hdm652hVRplqvUl3HjqJF350tq4MQ0BhoWZ5WJosvC2Pd9OF5dj4RIWnR6Ngn3fdts0Hwd4vVvlhBqIIsyznYti+KyLMdqnWMAT4v2+v31Y/847fOUb3ez8xWa4bpuEW7P0SRVgNr/GgaifVDhqv+3FrR2seNtE5ezua9KNdVre+MUE1vv3sLtLV76VjSdqxROMQgi/OokrjXu8BasPg0MN3K92inVuc/Cz2/BqTwiaIIdHtxzssfQP8BaoNK4uTzlg3dUoKueF80+i6s+2npAo/EpYRMAodjTLLoAkpgUDGL+fb83mzpmPv2iUhShacqILzu22jZQp64kRKmRdC/AmAEwDGAHxfSvl953lCiDUA1gBAZ2dnpDLFvUV2QsrYZQiKX7n9nt+IhiGg/L6i6Ht7mfWmW5a89Sa3Ey/djfL+TF/a+BlftZLXTz3Oc6uZL/zOw3711DovjDnNZHzrjldTf1LGJI1DCJ7BJaoFFmMgFXAagsLAZJEdhQEpqCHRZKIIa0HvxwMmDG8xlbEOKAQsbjZjqNX2uiDd9i14YRhE3AwZ3eufr6psOx2K2A5Bs5WptlW8duZD3L3txUQYiHRGnGqD8+s89dIpwRT0pAIhxEwAnwdwE4BRALuEEKullE/Zz5NSPgngSaCwrSxKmYJm1wuLtBD4mWuuilWGoPg1APtt60aN4Wifd6PQP3u/xK3ffrFkrze5nXj9tkZ5f6YvbfyMryhf9gQdD06Zqpkv/M7DfvXUOj+MOc1kfOv6q5r6k/LCrymylZkQJINLWG+KN+RG0L3+eXSt243u9c9rB0FSLIqNTFQZtIJm+DKZKOr5wW7Hg3egQ3GP9ZStK0xMXJstg0gQrLnGDTd9sm/rtNPf3abNWLbjwTtwfMuy0r+jm/51IKOfzm09DHd2ExY/treULaxr3W4sfmxv2fe6bZXVbre0sm3aH4Ont6TL4hoQYuOXALwhpTwrpRwH8CyAX4hToMGl8wsxLQLS391W1fUrF8+tWoY4yKQ/MgDPmzXd83xrztXN06rzVy6eWzF3NwL2eVeVUbMa7P1ilR/mYirMstIOnbfvigizXUz00wu/9+312+pnzPsNL6Gr2/nSxvT3394vpuPXD17jIZMSFRlQVTtovO7HrR2tedhE5+ztaNKPdlnd+sYE1fjWPd+q0NXv9TsWdTgJP9A4VAVhpHG3PEqsBZnbwiwpFsVGJqoMWkENiSYTabWeCXGT1GxdbuSG8+jfsg83rduN/i37ap6WPYhBxDnX+MXa1rm6r7Okc1ZGrh0P3oHN9/agozULgYLHUNSB9arFaZTfkBvRnqsKEPn2+5fLDESbBnqU6evD8OrbNNCDN2wGtlf/+DOJblsSKycA9AkhpgkhBIBPA/hhnAINLOrA1uUL0JrNlB23npM7WrNY3deJbKb8kVQApflFdb2d6S1pPLFioXJ+2jTQo5Rh5rQMVvd1ll5QWNd1tGaVC7SMyxOzAJA2+Cnu724zWojOnJYpC4q6d+2SigX47Bktyjn30MN3V8g/e0YLnlixsOL8TQM9ZXO3qg/6u9uUL3H8MC2TwsxpmVKZKqZOSZVke2LFQqN26mjNor+7TdnnFgOLOip+n9zKFrb/Wxyd6uwXq/xtKxZWtJ2uXPvn6S3pktzW/WzT6LHzmKqPndd8ffkC7e+yql1048E6bumINXatevauXVImn4WuSdJClPTKqtu6b+eLELdx7YZqzKvW5kF2B5i+tLHOs7eLUwRnv6jGb0oU5NSNHfvxTAoV/eM1HrYuX4Ct9+t1xXk/TlICeGLFQux48I6K76153JqHVWPRrR1V/WjNJypZnW2uGwur+zorflN049vP862uftXvmP03MEnPzMxWZogqXgyAqrPTuGWisVOLKOYkugxaQTN8eaXvBarPhhQ3cWXrCorfzAV+cNM/J3710W2uMXWDjVPXwhybfjOLmdQdpV40CsxWVhuEEI8CWAHgCoBhAP9WSnlJd349PIMRQgghJDimz2D0HDJA5d1jLSyCWLPtuC3I6uktfNKJ29MjyJYTa7FpNwyl4P5GoB7x67IZN24xwaolysx5bnPNysVzjTzQTDNvRIHObT2IO3vQbZ5uRKkXhPhBSrlRSnmLlPITUspfdzMMEUIIIYRYMCC1AW4LiWOb76lqce4WxDSJXhP1iJ/sb1Gluw+S4Uu12JwE0HFNMj1qghI0ILwpG3Ij2HHwBKxRNr0ljce+ENzYGuU2uB0P3qEMvOwkjP39dnYeOqkNfGwnzvhWe9cuCS1bWRSZxepxeyQhhBBCCCEWNA4ZEFWKYkCfiabaIKbkI/xkf4sy3b3fDF/1tNhUbbv0c69+MqX5lcs5vj68PIGv7DpSqtcvfjMX+CE3nMep8xddz4kiffuElBUGTBVxx7cK6779ZhabPaOlIuaQddwiSr2ohtxwPjLDKyGEEEIIaRxoHDIgqhTFQDCPEj9Uu2hPAtUaa/waWcIwBIWBbrF5jUtQzjhwGmDs2y7j1jWd19/EpFQaB00YXDpfGVsmjG1wKkOmnWq2EXa4pAO15jLLgKmLydMoRmu/RvlDD99dEZTaCtBtEaVeBMWP1yQhhBBCCGluaBwyIGrvHr8eJaYkedFuiltqeVMjjs7IMiXhEbcGl87H4K4jGJ8sN0x+ePkKcsP5xCzudAaYpw6eQO+NbbHK6ebdF9QDK8ptcF4y7Tx0MvDYHVw6H2ufPoxJxXfOuSxqo3XcBLk/uyHImpesQNV2g3WtvXTcXgD48ZokhBBCCCHNjadxSAjxTQCfBXBGSvmJ4rFHADwI4GzxtD+SUj5f/G49gC8BmADw76WUeyKQu6bU60LJLVZS0mW3CCO1/ODS+Xjo6cMVx8cn4cvIVGsGFnXg0f/+Ks5fKM9UNj4R3OslCtwMMIPfDr59KwzcsnBVs90nqm1wOkOmRTVbWS151z/7CsbGCyailAB+bbHaGykqo3VSCHp/XgbrWuq61wuAetqaShoLazuj23ymYt6s6fjynfPKjKx33tKO7x55qyJr59QpKfTe2IqDr5/Xzo1TUgKLb5pZMWZb0gKXJz66Zua0DJbddj1e+NHZMuPurqETZddOnZLC1+67rcLFusMAACAASURBVDTO7cZZCwFgWksaFy5PlBmJnYbcm9un4fWzFyqeK1Vl2svO2GTPpIAJCUzKwu9d380zcfydMdd2FwBUrSUArCp6p+aG8xjcdRjjqrcJANKiUK8bzi3QbmUKFH6PdGW2ZjMQAhi9MF5q06E3z5XFE7Tuq6M1i65rsyW9cLa1Cme7AgWd2Pi5Wyt0wC9XT03jvUsfGemnTknh0pVJzyyhaQFcnc2U3fN/feE117h7JhluvfDqC6t9//HYOWXb33lLO1740VnfYz+orM5+qzX2PnBu477zlnZ856VTpWcuO1b/d2j02U42k1KW4YcpKYGJSYlrbGNpSgpl49Gpm9a4s69D0kIgkwIuGrT5tEwK4xOTFWPeWY+9vstXJnBBc6/TW9L42NS0cps/UDmv27Fe4qn6SKWvbs/HdnRb91XPiqZEET7CD56p7IUQvwjgAwDfchiHPpBS/onj3I8D2AngUwDmAPifAH5OSqnfJwGmUY2KqNKy15Kw7qFe2+Kmdbu1D3FvJERutxTpQLwp6XXbo9Ipga8vX2C0kI8qBpUKVTp0O2khcGzzPZHUTcxI0lziNvYKD/jqxUdcY5Kp7JNJ2M9gXvOYFzrjRVJICWDbAwsx9OY5zyD+QGF76Sc7rzFaKMybNd0zIUHU6BJzBMFa5OSG88qXdEFJAUov2GbE3sYqb3MSPZZRO+i8R32uDfNmTcep8xd99ZFbOAfVb102k8YNM6+qeh6PwkBk+gzm6Tkkpfx7IUSXYb2fB/A3xbSpbwghfoyCoeiA4fWhE2cwzrjj/QSNlVTLxXA946VbYeheUoPc2vHKcuX0UqjlmLTGW9BsZWFsa/SDyrvHTtwxfxjcOFm4GWWl5vu44yCRxscrdpoXSV/aTsrCPf7kXffkARZj4xPGxpa4DUOAP89sL6z72brnaGhlAlxI27G3MQ1D8fDamQ+rmveoz7UhyPzqtttGt3U/jHk8zt+CamIO/Y4Q4t8AGALwFSnleQAdAA7azjlVPBYLcQbjTEK8H7dYSTrDleliuFaGr6hSy1eLl26FpXtJDHLrZNNAD/725Tw+vKz+UbQbskzaJax05Xb5gupmGNsa/WJtWYtyjAUpm8GNk4fXtgT7eZNS0qBHakIzbFs8PTqWeCNWkmgGnYgbtnG8sP0bkyhilyadoMahPwPwn1B4wfOfAHwdwG+h4A3sRNmqQog1ANYAQGdnZ0Ax3IkzGOcOjSfFzkMn0XtjW03evutiJQHQGq5MFsNRGr5Ui1Zn/UE8mXSu2vNmTQ8kp5duhaV7UQQ/jsL747Ev9CjdmTNpUWbI8moXp2EIKFjP7972oi8DUdxeezr8tH3SAtUnKbhxnP2bJIO1l9eexaSUidmGShofr9hpjcCc1ix+8u7FquLANRPNoBNxwzaOF7Z/Y+K226ZR+zyQcUhK+bb1txBiO4DvFj+eAmDf93ADgNOaMp4E8CRQ2O8eRA4v/AbjDGvRvCE3on2jNCFlTd++qxaY3eufV56rC2BtcdO63ZjTmsVb76rbr9pA17pF6+q+zqq38Fy4rHba1B33wku3wgwEG2bw46i8P6xr7YEQrWCO9nK92kXnRunHvTIJXnsqauV542U4CRqoPinBjePu3x0P3pGYrbfOFwA6krQNlTQ+Ko9XP9RDzCErgCxjDrljvYDTJQYJCmO0fIS9jRlzKB4Yc6g+CBJzyC2cg253R1gxh+IiUDJvIcT1to9fAPDPxb+fA/CrQoipQoibAMwD8IPqRAyO7oFYddxauOWLrsLWwi03nMeq7QfQtW536d+q7e4hlLyMLLq3737JDeex8NHvl8m26I+/j9xw3vU63SLC6w2Y1S66351q36C5LVotcsN59G/Zh5vW7Ub/ln2e92oR9sLWS7f86F4tcfP+qJaBRR04vPGXcXzLMhzfsgzDX/3lCqNHLdrFRI9M0XmDBPESibLtLSzDiTUWLcPJhtxI6Zyg4z8pOh1m/zYCmwZ6cGzzPTi+ZRmeWLEQ2Uy67PukbUMljc/Aog5svrcHHQHmhnmzpuPxFQvR0ZqFQCF4+uq+TrRmMxXnTp2SQn93m+ub3SkpoZyvW9Ll18yclsHqvs6yep9YsbDi2qlTUtj2wEIMLOrApoEerO7rrKhfoBDbzipn87092PHgHWXnpoXAvFnTyz6v7uvE3rVLlGXay7bLnkkVjFVWGf3dbZ7trmstAZRexj2xYiEyLiuEtHvoSgDlW8IHFnW4lik8ymzNZjBzWqbUpttWLMTqvs6ye7H+7mjNlumFs6119at0QqUDfrl6avmcPHVKqiSXG2mBsnt+YsXCigWjs423Ll+gHCt+8OoLq311bW+No1qg6rdaY/WBfd6zz11ZjdJb/a/TZzu6MvwwJSUgUD6WnMU6ddM61yn3VYZtPi2TUo55Zz32+qa53Ov0ljRmz2jRfu+mC/3dbdi7domyj1T6mhLuwagBKPt887092Lt2SVXzRj1kK9sJYAmA6wC8DWBj8fNCFOwFxwH8tpTyreL5D6OwxewKgIeklP/DS4iospXpoohvvrcyGG3/ln1K17CMI81f6XyXN8Vu2Wx0+M0+5ZaVIJMW2Hq/PhOTLsONlQY16BujajMpeWUB8tOfTnT9GzRrj5cs1cgaJX6zn23IjQQK5qzzXvFqlzAyQbmVIQDfXoFheYnUIvOc29i2xqZbhqsnVizUtosqHlkcOh13tjBdetKkBO5PctBwZitLJswYSwghhDQ2YWYrW6k4/Bcu5z8G4DGvcmuBn3gtOg8SlWEI0Mfmsb+hV9ERUvYpt6wE4xPSNQ6IW6BqXVBqE7wyKXltd/HKrlZNzJOwAzt76VYUsYLCwE/2M1Ua+A8vT+Aru44A0G+FMtn2o2uXMGJDuQXptXsFut2DnbAW/LXIPGfiFeQWp0Y3ljbkRpRzwic7r6m5TgfNwhgWcQQpt+Nl/AlzGyohhBBCCGkeqslWVhe4PSjbH7JThllf3MrxCkq1uq8TvTe2hWKk8NoO5fa9LlC1dXx5byeOvzPm2i7ZTAqXr0jjgLA6g8FTB0+go7jAcTNa5Ybz2vbV3avTyDVv1nRcuDwZmrHGaxFmqnu1NBz5MZLptulMTLobH71i2ri1y961S4yylbkZGk2C9MYRSLkWmedMDCebBnq07aMbY7o+3X/sXJknz9VT03jl0c/4Edk3bvNEo2MatyopMZEIIYQQQkj90PDGIR3Oh2y1ASTtGbRKtU1GhXPfYrWGAa8I6V7eCLpMSKbt4ncriVs8EGuBs/letdHKMqjpUN2ryvvptTMfFgItBtxGFpYxx3SBF0VGJj8eTUHTNwaNaWPhNARZsbWsQNfOrZ4qQyPgHaS31oGUw/Qm0+mGqeFE58EIALd+9XsVWwdN++69SxO4beP3IjUQeRm3qyHJW7IAM+9J1dy3/9g5rNp+gAYiQgghhBCipWmNQ6qHbKDwhn1SytLCYNfQCde0xbpynNgXLmG4/btlJbCnD/e72Hn4b90NXR0BF0xei0trgbN/3V0Vi7z+Lfu0Muk8L4Js/TCNk2O6LUnX9iYLvCgyMjnvb5VHoDW37Vluxscwt/2oYmvptnoC5YZG69508abiCA4extg30Q0vw4lb5pgPL09U6LebLjh571KwDEV+0Bm3q8F0nJukso/KyGQSWD/ubW8kfoQQrQD+HMAnUNhN+1tSSvdMGoQQQghpeprWOKR7yJ6Usiw47MCiDlcXfRPvgyhiYQws6sDQm+fKggVbjE9IfOWZI9g1dAIvn3jX2KixITeCDy+7G4aCeN0AZotLv9vGAFQdDFe3JdBacB96/R1cuDzpO9aR20LTZIEXNN24jiDGJp0XSjolcOct7ejfsk+5+DX1XjFZQLvF1tLh7JtabOeqJV66YWI4GVjU4ZpW2NmGJlv1TEmqd45pTDO3VPa54Tweee7VkpcbUD72rXqC3nst4laRhuAbAL4npbxfCNECYFqcwqji1/V3t+Gm9o+VPcO0pAWklKUXACkB/NriTgBQJkYAYLSl30KgYCnraM3izlva8d0jb5WNVed5JqSFwHUfy+Dt9y8bna9LcpI0pk5J4Wv33aZ9YeVEd18ChReWlycKLTpzWgbLbrse33npFMaqaAj7i0rT0A7TMilc0NRpv19nIg6nXkaBXTftvwv2l3p+cPtNihoBYFVfJ944+0HglxIpAW1mZL/MntGCKel0xe9u0PiqQbl6alr78sw5Tkwwnc/cmD2jBT/9YLxCv6z2d+rjz67fjSsOEZ9YsRAAQtWzmdMyaEkL43lVV8bGz91aGtMmz7Cmvy3ZTAqXrkxiUhb6ISUAVddNEcDim8tfKKYV586bNR1fvnNeYp6NPbOV1YI4MmXovAlasxkc3vjLVZdjxysVXhBMt7N5YTd0uWUxAqrLqmQ6MJ1tlRvO4yvPHNHKpWtbk4xG1bahsz1MfsStdIlemdOqycik8oDSyeWWYU73wzl7RgveuzjhmonNa0ucaTY3XYYvL5x9k1SDRBDCytbld7w7+9TtWp0cSc3iB/jLJqfSJwCu80lrNoNLVyarundV+6WKQk5KMyN8nPGHmK0seoQQVwM4AuBmafiAF+UzmOlvv18EgClpgXEfiynij5QAtj2wEENvnoukD6slm0njvts78J2X8lU/CwOF+73jZrVnaC2xfheqbfd5s6bj+E8v+H7B1uhkM2ncMPMqZfITUomlj3/wzOEKw5BFmMa8MMmkBT7V5S8Ld1y/Lc6XElE8G4eWrawR2ZAb0XpvfHj5CnLDeePOUHklWFQTC0O1+PhqbiT0LRv2WBRei4pq3k47t7vosHvGWAsh0/PtmGz9MN0SqMPeHqYPwKdHx/D4ioWenixBtmYV2uuVsrdxdg8hFW5tq5tMVZZ8p4eFl/eKqZeGV2wtHU5djTqDUy2NT2Ft2/PyBpIoN0Q557PbNn5POR9dPTWtLbOajINBMY3dZeqVo/IKdPPCslC9UfN77864VVdlUoXxXlQHk7fLjD/U8NwM4CyA/yaEWADgJQC/J6WMZSXkFm+wGiRAw1DETMrCXPOTdy/GLYqSsfGJQF41OiZlMrbfWr8L1bY7jR9qxsYn2DY+sPRRZxgCkmkYAgq/EX7HdFy/Lc4a40icY9F0xiGvRbxXGnirDOcPkrVgCxqTx07QxUdQrIHj9tY5jG04doOBzvvBXr+J8UYnr9vWDws/AYmdwcmd7WH6ADynNYtdQyfKypo6JVVhHfabkekjnfHn9xzmlkc/7Wm6hdAttpbuTUWtt4wFiUlVTbDxsLJ1WfWptqaqcG5FfOXRz1QYiK6emsavLOooeSU5781kS2U1OI10Xddmy+YAt+2UptsPveKy+cXvvdsNnd3rnw9UZxIWQCQypgD4JIDflVIeEkJ8A8A6AP/RfpIQYg2ANQDQ2dkZmTBhLdxJPJweHQvkvVsrGlW/kt7upLmodQIXUiCudm8645DJIt6tM3TGpQkpSwuJaq181Xq0BEW36LT2YIaZVcnE+6HaeE5eb8ZLb90N6th8b4+rZ4jJA0o2k8a0llTFwuzSlUnsGjphvLdfRVCdCTP9t+Vh4WWU25DTZ567Jpsp+2y1iX0vs30fMWDmtRNF5jcLU2+YxY/tVXpd+Q02rgs6DaDCKNN7Y5tr21gGW9OYDVa9lgzOrGResa2ijJmjMtLp7kflcWiSTc4rLpuObCaNqzIpnL9Q6T1Uzb17be1z24JofeccT6TuOQXglJTyUPHzt1EwDpUhpXwSwJNAYVtZVML4CWZPksec1ix+8u7FxPZho+pX0tudNBdBvfhJdcQVT7LpjEMmE61bZ7gZl8JyAQvTUjh1SiFolglhp4h2WyiaeD+YTEbVGDdM22Xl4rme25LcHlAEUFpo6jzALGOKl2FIt43OS2ecsYdM+la3NU+FZRg1SaPtNoZU2zq92t4Kzr7z0EnkR8fwlWeOYOjNc6V7Czvzm2nsHXuf6AxDdryCjesMXCqdse5x5w9OYqLoXuXm0WS1sZsxwV622z243VuUwcH9GEh196DStbu3vViVC7plfAEqYxJVe+9u+mfSlwBw/sI4Br99BIB79kVSH0gpfyKEOCmEmC+lPArg0wD+JS55wgxmb4cxh6InJQoelYw5VFus3wXGHIoGxhzyh6WPjDkULaqYQ3Elzmk645DXWwavzvAyLlkLQueCYt6s6di7dgkAb8+Kaiy01v1Zi8cdBj8s9jg8YaaI1i0Udxw8UQrw6masGFw6H7//9GGta221gb69JjKnTG7eJ7oHYKeMXtsDvTzb3FLLu+mMJatXezm9cGbPaPE0amRSBcOo2705t/boMNnW6URn/Nl56IQye4BFkMxvqrp0WEbmDbkRo4wLbmXp7vHQ6++4PuBMOJR8bHwCg7sOV2UEcPPW092DddzNO6fauE1+jOqm2ymrNQw9sWKhMgOfyT2qDK3O34qwFt5Bxh1JNL8LYEcxU9nrAH4zLkGsOZbZygrUY7Yya15IarYyy0O20bKVWe3ObGXVwWxlevxkKxtY1MFsZUWYrawG1DJbmZuCmMQL8srw09GaxbSWlO8Fhf2hv5osWs4sQSbZ1PxkOPKD25tr1aJJhfNHGggvgruuL1UZvHR6Yzf+mGxd8so05fW238qc5Kyr7+aZePnEuxU6Yz1cmxhBwsqAp8PSs7Cz4nmVZyKT6bYz07qCZBtxyxxXzT2q0GWsMjGGuGW78jOm7ISRxcxkrrNY3dfpueUOMPe+0dUR1Hjt9tCq2qYZRlDWarJRGtfBbGWJJI6MsYQQQgipHcxWpsB6iLYTVjBYC7etQ27YFwLWAiWMINSDS+fjK7uOVHgQ2Olatzv0OCyAu5eW6VvqTQM9Ros4v+SG82iZIjA2XimfaquamxfUCz86W5Lt6w8scJXNK4ual2dbKiUqFo4TshCNv7+7DcffGStrJ2u71VMHT3j2ca1iXXmNIbdtnSqPvKCLYst7xM+2My/DsFNHv/LMEWN53LZIhh13QGd42Lt2iaeB6AfHz2szOt7cPk15rdf2T13cpoeePoyHnj5slH5dt2Xtk53X4ODr5ytiMfkNIm5KGHOp29tM+3e54Ty+e+StUPQjrr3thBBCCCEkGTSNcUjn+eH3IX7TQI/rwnZgUUcoRh1rYennoX/2jJaKY0NvnnM1DFm4LYiDBvN1MwKcHh2rcHfVBUYNOw25LrOXm5eNrh8kPsq8lR8d84zd4ZVFzctwMjGpT8t48PXzZd4ZfmPt+I115czg5sa8WdNLf3tlycqPjqFr3e4Kg4DKaFHNlh/LYKEz/D118AR2HDxRZuxxC6S+f91dFcdNxq9qTDnHnJ9tDn5QbeWytr/qPHF0W5A25EaU/TFv1nTXe1u5eK6n7pmkXzcJKG3Rv2WfURBxL7zmwygDoVe75c1OJi1i29tOCCGEEEKSQdMYh/76kHrBvePgCd8P6yaZtsLALY7N3ld/UrYXc/aMFhx6+O6Kc01TrNvPdy7kTAwMqkXmpoEe/O3LeWV2H4lKz6haBUbVechcf01WqwumGTHGJyQe/e+vuspv3z64dc9R/OOxc+jfsq/UZoD73n4dTvm8ggM78RPrqsMjwLaTU+cvlnmbmGTJchoEwloIW9vwnjp4wrOdLeOf5cGiQ+cZ46Y3ujHrJ67RvFnTA7eLKruX3XvGzWCj+k6nb6+fvVD622nQsOYTk8D5llFUpzOWMdFk7tDdm/O4rn3tMeR0hB0I3c6q7QdCGw/MVkYIIYQQQgAgFbcAtSA3nNcGNpMoLFi61u0u/Vu1/YBrebqFoHXc7iVhij0otMWmgR6s7ussGZ3SQpTiWBx6+G4c37Ks9M++yMwN59G/ZR9uWrfb93YDPwYGe33rnx1BfnSstJhe/+wIcsN5PPaFHmQzaeP6La+EKDFdGNrpu3mmcfmqdNVO3NrsjbMfGNdlx2mc9AoO7GRw6XzPvspm0nhixULsX3cXBhZ1KPVWheWV4WRgUYerx4JpsMDVfZ3G5339gQVVByG0Wto+JlXo5orVfZ1KwxDgbtB1zgV71y4pmyNMmTdrOr7yzBHtVq6udbtdPZVUW5C89E3nWQSYZw60jxsn+4+dQ9e63diQG/EsR7eFynl879olFfO5iWEIMJs7dai8QC38ZBF0Iy0Ejm9ZhuGv/jINQ4QQQgghpDk8h7yMDc4Fi9cWBq+U73vXLsFtG7+njUrvxC2ehluGKdWWhWqyAwDlcVjcgpzaj+vihTzy3KuYPnWK7zg2frc3+UXnIaNbMOaG83j5xLu+6nBumZuWKdhhrUwZqm1ClgElaKY6pyHCzWvFGTMmN5zH4K7Drtk4pk5JVQQIVm2T06HqV2ux74XXgn/TQI+rvtq3DPZv2edZnxcpjwDLdrkA96x8TtwMuqo67V5YJgHF582ajlPnL1YVp+atd8ewITdSdh9eHpV+vRhVmMTFsrYDrnIx2qniEwFA17WVc4CbIcht25hf46y9TF2GDuu3oppA2RZecaAIqQaT35QkIQD8QjFun9tvcNAtvlFtDbaYlklhaiZt9HLKIi0KMk3KwjytixkXhKunpvHh5cnQ4+VVS0oA7R8rz8Iadd/UI7NntODM+5dr1i6qDE5JgjoSLgLANdlMaNnNrAx5hXiSr1SVCREoZC1T7XqpFSYJsqLE0zgkhPgmgM8COCOl/ETxWBuApwF0ATgO4AEp5XkhhADwDQD3ALgA4DeklC9HI7oZueF8oMW212LXKy34h5f1iumVxcYtpbPOaGPfslANKxfPNUr5Z/dU0BlzRsfGAw38IIFR/cT20AWuvfOWdvRv2YfTo2O4JpuBEMDohXGkDLeU2XFuQXKmT9WV5jcdt10ua5uUlZLVLX6RfftQbjhvtD3s0pVJ7Bo6UTFZWelSLZ3VtZe9X/1mWPr/27v/ICnOMz/g32eHWZhdwS0rA5FWrBFrSrIUTnDeMqvbVMq2YmMLW1rLJ2NOJKqLS/rHqVhnFxewyUlKSYEUsYxTcVwl2UnkQDCSLK/lEzFW6UelggUyEliYSBwsQgsrHWy0LJJhBLOzT/6Y7qF3trunu6d7unvm+6na2p3Zmel35u3ued+n3/d5qwUW+je/UGUFNMFbo3/0taKVGz/7g9u5wi7/VNBpq15z7tjl2/FrUjFtipTT/mYGIWrtJPT3dOK3HgPfiunlsxpY3oUn9w9PO8/vGRqbFvRyUm3aWNB6dNrXBaVjzcvIKCunpNzWaZ1JWDqVGofX75QkUXgbqRr0LBZ1x/JCYdJxmXYn1s54UTW0wBAAzxdH621SMS34zk7/dLUsIR5EkgNDAPeRsCkQWmDIfD0vqSK8ijMwBIS7SEoQXkYO/XcA/xnATy33rQfwvKpuFpH1xu1/A+ALAJYYPysA/Mj4HQuvoxJq3YZdUuWgHSG3PCB+lsWuxq3D0LNhV9XnW686+8lVU02QxKh+c3uYB5r1qma+UJzyGtaTVj2vfHW0ZT1f+SuqYs7MzLRG2MWJSXzriYN45KvLANjnL7Im3/Uzjc+u8Vy5zzp9XqffL+Ud8rMfm9PWqtVBtf3PXNEtLNYOftCkw3YjrvYMjWHB7FbbhpmXkR5ekrcHGZnXIrCdmrtt7/CU975kfjuOj16w/Sy85u2yY46Y8Rvcc8qvBZQSuFd7jlvduiUyf/b1d9G3eK7tPletHt2S3y//d7+pen4wRxCMXyi4Bnyq5ZwiCirqqeFERESNLMgiKWGpGhxS1f8tIosq7r4dwKeMvx8H8BJKwaHbAfxUVRXAXhHpEJGrVPXdsArsR9RLc5eGTf8eBUuvyUyq7NSZAi5HN+1yVzhN0dqy+wj+4dyHoZTbbnqQlVsHzq7z6zRFwy9rYlSvK5kB/hMvA8CT+4cTOdxd1V9OEaerc5Na2pc+ff08x+eaQYJap/F5Pc4mJtXX1WTrdMtaAgtR6Fs81zZQ4RSYtAsyONXx6Q8uYW1fd+BVruy2BcDXSK1Kbgsemq9pXnl2GhlZbSU+q4wIvvfVm8qjWk68V1rd0O+5xu39Vpv2VRm8q6xbt9c+e6GAV06cRX9Pp20APigvgeMLhUkoBN9fvcy1UeH2XcPgENUi6qnhREREjS6u79KgOYcWmAEfVX1XROYb93cBsPbUTxn3TQsOici9AO4FgO5ub8lk/ar2obp1OL0kld6y+8iUwJCpUFTksi1V5zwePXMeH/vOLhQntXyF16nMYY3MAUojS9wa/27TIexynthNZ7lwacJTR8Zu+Xi3oJt1e0ApmbhbJ+/a9c/aXj0PcxRJmM7lC77y+LgZGc+7dsbNaV5BR365rTRWq60VHVs/gYWo9fd04rXhc64BisrRJ04j25xUm7bqJMi2gMsjCcM4LpyCsn4Srfctnms7qmXTHUux6Y6lnve7yilc1mlUbgYPjDh+Fl6HLheKihPv5W3PmVFP5/IS5AmSmJ/IizBHExMRETWjIGlWwhB2Qmq7ZAq2PXdVfRTAowDQ29sbyZAApwZKRy6LixOTrp2746MXpiXtreTWiP6wMOkpgdmEEQAxOz9+phVFYfDACFpnCPKF6SV3mw5ROZ3Fa3LcyuXj3fIdFYqlkSdbdh/BupXX4W8HD1Wd1165FLl1JIVf9Ri9okB5Wfvt99yMa6usGhVULpspT9/zsyS9Oc1r4+AhbN87HEnZOnLZacddZWLnOJ14L191vy6q4uP/9n8FTopn5r7yGzjwm/RZgCnbuPFvf20719pLsNtkrR9rEMRLrZnH54tvjjomuT94/9TVtdzOGYvntU0pi9dRR2HlSxkZz0+rSwCu07m6QupYVwvy+E3MT+SVn+8UIiIimsraT6u3oEvZnxaRqwDA+H3GuP8UAGvP+xoA7wQvXm3slubOZTMQgafOnbm0uBO3RvTVHTnc5XF5bVO+UIQqfC39HqbLnaepncAWqZ5Eu9LA8i5sYvoBkwAAHPZJREFUumMpujpythFDk7UD4yURNlDqTH1r58FACQ9rSdz9va/e5HvJcJO5WpkX1mXtg3bUWqoU0zqtcGB5F7auXgYvRTSTT0cVGMplM3jgthtt//fQwFIMbboVJzavwtbVyyLYenUzZ7R47rgHDQy1SGkfMAOb1c5DVn4DZ29tXoU96z9T3hce/vJS23Pmpjv+1PU4tjKPEeuy815KtbavG0ObbsVDA0tdk9wv3vAsFq1/Fj0bdpWTRzsdl8dHL5T/jnqasR3B9Lp88FeHHadzAaXvrWy1A9iDaucOp+/HuBoj1Dj8fKckhaB08aOrynET9Mis/Yh215Ztwdy2rK/nZORyW8HMGReWOTMzgdtLUWqR0kpcVskrZfwWzG6t6+eSSXglJLx4qSMoXQgO8/XW9nVj6+plyIXwxdPeGk8/3NTVkXNN/xK1oCOHngFwN4DNxu9fWu7/VyLyM5QSUZ+LK98Q4Lx6j9crWtWG5q9bed206U/A5aTK5vP8jHYYzxewdfWymq+6ndi8ynFqkjn6o5JT56lydI9X1tFETolkrR0YP6Me4kgXtP/tMd/Tm7o6ctiz/jMA4Gv5aXPf85JjpTIptbla2bef+L3jfvfDF49iYHnXlPw0Am/LNz74q8ORBIbaWzN4+MveToYDy7tcj5ETm1dNe29AbStOtEhpSmZYMiK2SYsrZ6p6zQPjNYBk9dlHXpqS98xtxTOv5yRzZJ6fYIzgcnLrNSsWuk5LMT8fa6DXy7LxUU5zybaI7RTjynvyhaLjZ2KWz6wDa861tmwLCpOKgmVJl1LQrnRetlt90S7IUzmd7Suf6MKLb45ytTIKnZfk+ERERGHi907tvCxlvwOl5NMfEZFTAO5HKSj0hIh8HcAwgDuNh+9CaRn7YygtZf9XEZTZF7vpTl6me5lGxvNY9uBvysuaWxvQdo34ysTJZu6QFQ8/52lpSLMT62UKk1PiYjP4Y5e7xprkF/A27cOuU1UtZ0ZlUty+xXMxdv6Sawcm7ulC1ezYdxJDm27FvuPveV7y1ToCwk+iafO5lZ31GS2Ykki7sj6t7JbqNh09c35asEpRffnGjYOHIpv2eP5SEfvfHgvtxG7N21PrEvZRTCk0gyhe9omR8Xy5vhbMbsW+737W80g7N0fPnEfPhl1TEiU7deqqfQaVCZf95K4xX9UM+PT3dHqurx37TnpaNj6qaaEZEWy586Zp3wNBjhNrjrSD938OgP2qdl0259xqOYzsVif7+asjsV6dIiIiIqLk8LJa2RqHf91i81gF8I1aCxWlLbuP+B49YF3WvDI/hNerYzMy3oaoqVFGLyNUvAR/nAIHgL8cHJXPs46YGhnPY92Tl5NF2yXF3TM0hv6eTpx4L5/aq9RF1XJHesXiK6cEv2ZkxHZUiXVklFN9nXgv7zqqamB5F57cP4yR8Xw5MOQWFAJKdfTa8Lmgb9VR1Emht+8drmk1Jye1JNnNZTO+jpFqo6+sQZSeDbt8l+f0B5dw/Xd34cOitzNZtfI4rbBWye2cZB0hZ6olKe1vh8ZcV3y0Kqpiyfx224CtNb9YVMHnoiq27D6CB26buppikICkdfoZ4BzgXXRlbsq2vHwPcXUyIiIiInITdkLqxKkc4RLG1IIgDWo/ndOR8bznlX3cAgTV+Jn20bNhVzkQkmnBtCkUhUnFA88cxsDyLmx36EDuPX7WduUeL5ymbdSb2ZE286OY7AJtlSOjBg+M4MR7+WlJgKs9127kwJ6hMdz12MvlHECVowbiyK8SBsXl6VHVRkK4jcQyc9GYajn2Z2VbMCvb4nkkiFsgZsHsVpz+4JLnFa+ceA0MVSuPlTmt0xr0NINYgwdG8OKbo47PtTu/2U2JNKdCmXXpNNVSAfiJ5Rw9cx5L5rfj+OgFx2Xjw0r0bKfyogHg/P4vThSrBr3M7xin8u4ZGpu2j1fD1cmIiIiIyE1DB4fshtH7mVLmxm8nw+8qZNWmmsyokh3Ny1LJfjoF5lX3oiqKDn3N8XwBGwcPOX6+blfu3ab7+VkpqV627R1G70c7pyR1BpwDGnb74oanD+GHLx6dNuKhcsqI076wZ2jM8XXTGBgyPfDM4SmrCdp1vIFSYNRpuua2vcPYd/y9crCgFmFNozMDQ16Fda7yqjJZu3n7rdE/4rXhc677lKIU6DGDMr0f7SwHKM3pXHZTodymevmdBnZ89IJr8Nkt31y2YrpmEOZqatXOCfvfHvOceN/Ntr3D5cTwGREsntfmGhzj6mRERERE5Kahg0N2oyfC6mwJYLvUvV1QBgD++OFESFsumdDLV93NKUbmtis7ACPjeax76vK0L7OcUfQ+/S6lbXJLclzvwNDMGS3o/WgH9h4/69o5rQxYVE7tuOuxl12T+OYLRdupMJVTRtzYJZ22dsjTyDqN0+Q0Wu/MH52DLV7zQtWLn8AQUNtopzD5yZNlBpR2vHISxcnLAWVzJFxl/blNVfO7/zo93pr/zElYp5jxfGHK94LddK8gCxU4seZqsu7vdlMFnZLbj4zn0b/5hdRN8yV3IpIBsB/AiKp+Me7yEBERUfI1dHAoyuHyZm6gymTXdqM4ZhkrzURlz9AYPvvISzh19kPHq/uFouLBXx0uT2Na9+TvfU3b8CJoEtbBAyOhjc4IY+rIxYlJTx1i6/LTlQFBt2TQ1fjtjDvdH/dUPK85Y7yyq9eUxr88SUJgKKji5PSApV1w76GBpfjFayO2U9+CHMvWhM52+c/qwfo+Ky8WfPr6edNWBwOmrzYWlh37Tk5JNL7/7bHyaCMrp9F5lGrfBPAGgDlxbNxptVQiImo+9R4Nn3bm4jNxaIllq3US9XD5yuCTU8LPqFZ3sjp65nzVzoVZji27j4QeNMhmBKv+9KpAzzUDLGFYt/I65LLekn+HwexUjRirvZm3a20Ubxw8VP7bXH3Oj45cFhMxRk4yLYK/XNGNXDa8U4w5Wo/SySlY//CXl047Zs2RRn73fesxOHhgJPBIxlqY79McOWg9N2zbOzztXAEAm+5Yio5cNvSyVAaPX3xz1GWEZhH37TyIReufxV2PvRx6Wah+ROQaAKsA/DiO7TMwREREVgwM+XP6g0tY8fBzsWy7oYNDn75+XqSvbwafBg+MYPH6Z0O90t/f0xkoKOBFGCOq+ns60WW8/4wICkV1TERtfY7V4IGRmpcYt1oyvx0Dy7uw6Y6l6OrIQYBQgxNO7AKCtbJ2aoMkHT+XL4Q6qmZtX7evxxcnS9NawpwSqEC587rkO89i8Qb7ZMaNyNyfu2LID9Pf0xlKwNUpWD+wvAtf+cTU0SofForY//YY7uydvt8tmN2KtX3dU5apr2QGOuKYWjkr24KNg94CxNYRVe0zwx/ImxEpn2ev9fEdZSa8p9TaCuBvAMSSrI+BISIiotr4TUcRloadVjZ4YAQ7X4nuqrF5ZdvLtIWOXHZKgl0vzBxCTsucB2FemQ4jl4m1XGYHzK0bZpcXKewhhs9961MALk+f2LHvZDk4Eca2/C5pXouiquNKTl6E+bnOmZmp+9ScahKWnzxSgstTzOo91WzJ/Hbc2duNw+8cLu/7c9tK5xE/IyJz2QwWXZmbsuqhdSW0yv1LAccV3cwvSzP5dC3HSaUwcnXlC5NVA+VWZrA+imnQRVXXvGdu2MFPJxH5IoAzqvqqiHzK5XH3ArgXALq7/QX/iYiIqDE1bHAoiqlTJgHKV7q9dALG8wUsmd/uK0HuZx95KfSEuuP5QigdKWtn1QvzCn/ltqO6pm8XsFMA7a0Zz8t625kzq37BoaSYMzOD9y8213tOkrjnaB89c35acOHshQLW9nV7Dhh2deSw6MrclGCDdSW0gyfP+S7X9r3D5dw9YQprpJGfVzFHVCUlATmlXj+A20TkVgCzAMwRkW2qutb6IFV9FMCjANDb28sR/0RERNS408qiTka9be8w7tt50HMn4OiZ82ipsvy8ac7MTOJWWjIF6awWVSO/Cm2dPeYUsDt/qei5DioJ4hveV29dHTmc2LwKJzavYmAoZkntse18ZdjXtNeXj9sf/3uGxgIFbM2cPUn9fPyYKJbe/7qV1yXuC9ma+4zSQVU3qOo1qroIwNcAvFAZGIpaVFPiiYiImsWC2a2xbDdpbdHQRJ2MOgivA5mS2iHv6sgltjNmnWbkVsagg8mS+r6jMDKeL+cpSbOujpzvXElJsmB2q6ccQ+2t9UvAbipMlqa+Vsv9A5T2pxgXzUs8M+g8sLwrceeZOBJ6U/ptv+dmBoiIiKgs4LX5phXnamUNO62srdU57mUusS0AWlpk2rLLcQh72e+wzZzRgnUrryvnC0qaah1U8uevfYyKSyIzJ9i3n/h93EUJ7PQHl7B19bKqy5zXMlWyFmHm+qGSpB1zcST0pvCo6ksAXopj20EWUiAiIqJ4NWxwyG1a1vFNq8p/e0koXQ9JDgwBwMWJyXLukbjzoNgxEzgzSBSOpNWvF2Yy4Vy2BRcnioET8dZCAHzMZ34xN0k+5qjx8XxKRERE1DwadlpZNea0mSQEhtImyZ1UXuluPv09nVjb112u+3xhMrZgqwI4ddZ5ZF3QpejTtFc3Uzhhzsxwp/Tlssn6Sl6zYmHcRSAiIiKiOmnYkUNulj34G4znvS/DTBSWE5tXoX/zC4mcmpdWe4bGAic8bwEwWfVR/uQLzq8YZaL8pEhTIKtWYeSHMwOGgwdGan6tsGREsGbFQjw0sDTuohARERFRnSTrMmWI3JIhNnpgaMn8dsxty8ZdDLKxaP2zDRsY8jMqJimjS8IODFWTtET5S+a3c+pQjMzcWBsHD+G+nQddA4v1IgADQ0RERERNqKaRQyJyAsAHAIoAJlS1V0Q6AewEsAjACQBfVdWztRXTv2vnXRH58ulJNTyWx8WJ+DsZ1DzMTq7XRNbZjOBSsZnGmJS8M55He2smtiTSlY6dOY9Z2RbkC81XF0nwlU90YcPTryciKGRSoDzdmgEiqsVdj73ctO0wIiKioNb2dcfWBgtj5NCnVXWZqvYat9cDeF5VlwB43rhdV4MHRmrOJZQRwcwZ6RxYxcAQ1Vu+UMSGp1/3PKWoGQNDQKnjnZTAEFAqT5ICE81m5ysnE/v5cxl7qgUDQ0RERMFs2zuMjYOHYtl2FNGP2wE8bvz9OICBCLbhasvuI6G8DoMsRN4ltZNLlFSFBC9TyeT+VAsGhoiIiIKL6yJdrcEhBfAbEXlVRO417lugqu8CgPF7fo3b8C2MpK9sGBMRUbNiLioiIiKieMQVi6h1tbJ+VX1HROYDeE5E3vT6RCOYdC8AdHd311iMqa7uyDVs0l8iIqKoLZ7XFncRiIiIiJpSXBfpaho5pKrvGL/PAPgFgE8COC0iVwGA8fuMw3MfVdVeVe2dN29eLcWYZt3K60J9PWpsS+a3I9vCq+RERKajZ85j8MBI3MWglHJbMZaIiIjcrVmxMJbtBg4OiUi7iMw2/wbwOQB/APAMgLuNh90N4Je1FjIIdvbJq+OjF5DLpjP5OBFRVMLK30fNZ/s9NzNAREREFECcq5XVMq1sAYBfSGnI0wwA/1NVfy0ivwPwhIh8HcAwgDtrL6Y/W3YfSXSiT0qWoirev5icFaSIiJIgjPx91Ly233Nz3EUgIiIiHwIHh1T1OICbbO5/D8AttRSqVmzQEhER1ebqjlzcRSAiIiKiOmnIuTRs0BIREQUnYP4+IiIiombSkMEhNmiJiIiC+/OeTgws74q7GERERERUJw0ZHGKDloiIKLjfDo1xtTIiIiKiJlJLQupE68hlMZ4vxF0MIiKi1FGUFnfgxZZ0EZGFAH4K4B8BmATwqKr+oN7luOuxl7FnaKzemyUiIkq9/p7O2BZ1aMiRQwDwwG03Nu6bIyIiitgIF3dIowkA31bVjwPoA/ANEbmhngVgYIiIiCi4PUNjuOuxl2PZdsPGTwaWd+GR1cuQbdh3SEREFJ2MSNxFIJ9U9V1Vfc34+wMAbwCo6/AvBoaIiIhqE9d3acOHTgqTcZeAiIgofYqqcReBaiAiiwAsB7DP5n/3ish+Edk/Ojpa76IRERFRAjVscGjj4CHct/Ng3MUgIiIiqisRuQLAzwHcp6rvV/5fVR9V1V5V7Z03b179C0hERESJ05DBoY2Dh7Bt73DcxSAiIiKqKxHJohQY2q6qT9d7+/09nfXeJBERUUOJ67u0IYNDO/adjLsIRERERHUlIgLgJwDeUNVH4ijD9ntuZoCIiIgooDhXK2vIpeyZJ4GIiIiaUD+Afw7gkIiYc+u/o6q76lmIuBq1REREFFxDBocyIgwQERERUVNR1f8DgMvMERERkW8NOa1szYqFcReBiIgo1bIN2UIgIiIiIjsN2fR7aGApZrTwwhkREVFQV8zKxl0EIiIiIqqThgwOAcDEJKeVERERBXX2QiHuIhARERFRnTRkcGjwwAgn3BMREdUgI/wmJSIiImoWkSWkFpHPA/gBgAyAH6vq5qi2VWnL7iPguCEiIqLguLADBbVx8BB27DvJfSiFFsxuxekPLsVdjGkEQIsARe5SrgTAx+a34/joBR5/CSVAqvupS+a3Y/7smdgzNBZ3UVJtblsWH7miFUfPnC/fF+cS9qZIRg6JSAbADwF8AcANANaIyA1RbMvOO+P5em2KiIioIXHgEAWxcfAQtu0dZsc0pZIYGAJKnWkGhqpTAEfPnOfxl2Bpr5mjZ84zMBSCsxcKUwJDALBnaAx3PfZyTCUqiWpa2ScBHFPV46p6CcDPANwe0bamubojV69NERERNST2LSiIHftOxl0EIiKiVIo78BZVcKgLgLV1cMq4r0xE7hWR/SKyf3R0NNSNr1t5XaivR8nAZZWJiIiSjSMWiIiI0imq7rbdYPQprQVVfVRVe1W1d968eaFufGB5F+a2cQneRpFpEWxdvQxH//0qrO3rjrs4DUdQmuOabeEcEiK6jN+jFAQTmRMREaVTVMGhUwAWWm5fA+CdiLZl6/4v3YhcNuP4/4wI+ns6bR/jt1nT1ZHD2r5udHXkIBW3/erIZSMdIdORywZu8LsVy4wrmO+9I1d9Gy0CrO3rxtbVyxwfP7cti+/deRMGlpcGnj00sBRr+7rLjU8B0Jq5XGNt2ZYpn5+XbXjRlm0pf261NHsFQHtrpryfbF29DCc2r5rynpyeZ32fQOm9LZnfXn6euU+b+2Eu24Jq8Z6ujhy+v3oZtt9zM7bcedOUz2huW3ZaXc5ty5bLvHX1sin7vHl/f0/ntO1kWwRtIe3YQetibV831vZ1T3lOe2um6v46c0bLlPfoti8tmN06pT6WzG+3fVzUx3nG5YPp7+n0dSwsmN1qe7+5bwU9Hiqfl3P4QMz9vBrre7Ye906va6o8fzvVzcwZLbblrnwtcx8xz/9OjzeDstbyme/V7jlO9WCyHpvmj9v7n9uWRX9Pp2P5shnB/V+60XWbRHbWrFhY/UFEREQ0jV0/qp5EIxj+KyIzAPw9gFsAjAD4HYC/VNXDdo/v7e3V/fv3h16OwQMj2LL7CN4Zz+PqjhzWrbyuHGRwewwAPPDMYYznCwBKjej7v3TjtOdGVSan54yM55ERQVEVXR6fG3bZ/JQ/yHtNk8r39+nr5+HFN0cb9v364bfuw95X0rLvBdmHwnxv1tf6k1wWlyaKuFCYBBDueS8JrKsnZUSwZsVCPDSwNNBrRb1/VXv9KLZfj2NGRF5V1d5QX5RqFkUbjKuVpRdXK0s3rlaWfFytjID6r1bmtQ0WSXDIKMCtALaitJT9f1XVh50eG1VwiIiIiJKBwaFkYhuMiIiosXltg82IqgCqugvArqhen4iIiIiIiIiIasf1n4iIiIiIiIiImhiDQ0RERERERERETYzBISIiIiIiIiKiJsbgEBERERERERFRE4tstTJfhRAZBfB2RC//EQD/L6LXpmiwztKF9ZUurK/0aZQ6+6iqzou7EDQV22BkwfpKH9ZZurC+0qWR6stTGywRwaEoich+Lp2bLqyzdGF9pQvrK31YZ5RW3HfThfWVPqyzdGF9pUsz1henlRERERERERERNTEGh4iIiIiIiIiImlgzBIcejbsA5BvrLF1YX+nC+kof1hmlFffddGF9pQ/rLF1YX+nSdPXV8DmHiIiIiIiIiIjIWTOMHCIiIiIiIiIiIgcNHRwSkc+LyBEROSYi6+MuT7MSkYUi8qKIvCEih0Xkm8b9nSLynIgcNX7PNe4XEflPRr29LiJ/Znmtu43HHxWRu+N6T81ARDIickBE/s64fa2I7DM++50i0mrcP9O4fcz4/yLLa2ww7j8iIivjeSfNQUQ6ROQpEXnTONZu5jGWXCLy18b58A8iskNEZvEYo0bB9ldysA2WTmyDpQfbX+nDNpizhg0OiUgGwA8BfAHADQDWiMgN8ZaqaU0A+LaqfhxAH4BvGHWxHsDzqroEwPPGbaBUZ0uMn3sB/AgoNWQA3A9gBYBPArjfPNlSJL4J4A3L7f8A4PtGfZ0F8HXj/q8DOKuqHwPwfeNxMOr4awBuBPB5AP/FOC4pGj8A8GtVvR7ATSjVHY+xBBKRLgD/GkCvqv5jABmUjhUeY5R6bH8lDttg6cQ2WHqw/ZUibIO5a9jgEEoH1jFVPa6qlwD8DMDtMZepKanqu6r6mvH3ByidNLtQqo/HjYc9DmDA+Pt2AD/Vkr0AOkTkKgArATynqmOqehbAcygdjBQyEbkGwCoAPzZuC4DPAHjKeEhlfZn1+BSAW4zH3w7gZ6p6UVXfAnAMpeOSQiYicwD8UwA/AQBVvaSq4+AxlmQzAOREZAaANgDvgscYNQa2vxKEbbD0YRssPdj+Si22wRw0cnCoC8BJy+1Txn0UI2Mo3nIA+wAsUNV3gVLjBcB842FOdcc6rZ+tAP4GwKRx+0oA46o6Ydy2fvblejH+f854POurfhYDGAXw34xh6D8WkXbwGEskVR0B8B8BDKPUIDkH4FXwGKPGwP0yodgGSw22wdKD7a+UYRvMXSMHh8TmPi7NFiMRuQLAzwHcp6rvuz3U5j51uZ9CJCJfBHBGVV+13m3zUK3yP9ZX/cwA8GcAfqSqywGcx+UhzHZYZzEyhorfDuBaAFcDaEdpqHklHmOURtwvE4htsHRgGyx12P5KGbbB3DVycOgUgIWW29cAeCemsjQ9Ecmi1CjZrqpPG3efNoZSwvh9xrjfqe5Yp/XRD+A2ETmB0nSAz6B0FavDGH4JTP3sy/Vi/P9PAIyB9VVPpwCcUtV9xu2nUGqs8BhLpn8G4C1VHVXVAoCnAfw5eIxRY+B+mTBsg6UK22DpwvZX+rAN5qKRg0O/A7DEyDzeilLCqGdiLlNTMuZl/gTAG6r6iOVfzwAws/HfDeCXlvv/hZHRvw/AOWNI5m4AnxORuUbU93PGfRQiVd2gqteo6iKUjpsXVPUuAC8C+AvjYZX1ZdbjXxiPV+P+rxlZ/q9FKfneK3V6G01FVf8BwEkRuc646xYA/xc8xpJqGECfiLQZ50ezvniMUSNg+ytB2AZLF7bB0oXtr1RiG8yNqjbsD4BbAfw9gCEA3427PM36A+CfoDTM7nUAB42fW1Gar/k8gKPG707j8YLSSidDAA6hlE3efK1/iVLCr2MA/iru99boPwA+BeDvjL8Xo3TSOwbgSQAzjftnGbePGf9fbHn+d416PALgC3G/n0b+AbAMwH7jOBsEMJfHWHJ/ADwI4E0AfwDwPwDM5DHGn0b5YfsrOT9sg6X3h22wdPyw/ZW+H7bBnH/EeGNERERERERERNSEGnlaGRERERERERERVcHgEBERERERERFRE2NwiIiIiIiIiIioiTE4RERERERERETUxBgcIiIiIiIiIiJqYgwOERERERERERE1MQaHiIiIiIiIiIiaGINDRERERERERERN7P8DpR9imlPGdEAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['wc_text']  = df['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df['wc_title'] = df['title'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(20, 4))\n",
    "ax[0].scatter(range(df.shape[0]), df['wc_text'])\n",
    "ax[1].scatter(range(df.shape[0]), df['wc_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('new_dataset')\n",
    "# train.to_csv('new_dataset/train1.csv', index=None)\n",
    "# test.to_csv('new_dataset/test1.csv', index=None)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: wc_text    min: 1          max: 403        low: 2.0        high: 119.0     \n",
      "Col: wc_title   min: 1          max: 17         low: 1.0        high: 8.0       \n",
      "Col: wc_text    min: 1          max: 403        low: 2.0        high: 144.0     \n",
      "Col: wc_title   min: 1          max: 17         low: 1.0        high: 10.0      \n",
      "Col: wc_text    min: 1          max: 403        low: 2.0        high: 196.0     \n",
      "Col: wc_title   min: 1          max: 17         low: 1.0        high: 12.0      \n"
     ]
    }
   ],
   "source": [
    "def get_quantile(df, col, q1, q2):\n",
    "    \"\"\"compute quantile range\n",
    "    args:\n",
    "        col: col name\n",
    "        q1: lower quantile percentile\n",
    "        q2: upper quantile percentile\n",
    "    \"\"\"\n",
    "    df1 = df[[col]].dropna()\n",
    "    lower_bound = np.percentile(df1, q=q1)\n",
    "    upper_bound = np.percentile(df1, q=q2)\n",
    "    lower_bound = np.round(lower_bound,3)\n",
    "    upper_bound = np.round(upper_bound, 3)\n",
    "    min_ = np.round(np.min(df1[col]), 3)\n",
    "    max_ = np.round(np.max(df1[col]), 3)\n",
    "    print(\"Col: {4:<10} min: {0:<10} max: {1:<10} low: {2:<10} high: {3:<10}\".format(min_, max_, lower_bound, upper_bound, col))\n",
    "\n",
    "get_quantile(df, 'wc_text', 1, 95)\n",
    "get_quantile(df, 'wc_title', 1, 95)\n",
    "\n",
    "get_quantile(df, 'wc_text', 1, 97)\n",
    "get_quantile(df, 'wc_title', 1, 97)\n",
    "\n",
    "get_quantile(df, 'wc_text', 1, 99)\n",
    "get_quantile(df, 'wc_title', 1, 99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias: eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True)+K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "# from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\n",
    "# from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "\n",
    "# max_features = 50000\n",
    "# num_classes  = 21\n",
    "# max_words    = 200\n",
    "# filter_sizes = [1,2,3,5]\n",
    "# num_filters  = 36\n",
    "# inp = Input(shape=(max_words,))\n",
    "# x = Embedding(max_features, 300, trainable=False)(inp)\n",
    "# x = Reshape((max_words, 300, 1))(x)\n",
    "# maxpool_pool = []\n",
    "# for i in range(len(filter_sizes)):\n",
    "#     conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], 300),\n",
    "#                                  kernel_initializer='he_normal', activation='relu')(x)\n",
    "#     maxpool_pool.append(MaxPool2D(pool_size=(max_words - filter_sizes[i] + 1, 1))(conv))\n",
    "# z = Concatenate(axis=1)(maxpool_pool)   \n",
    "# z = Flatten()(z)\n",
    "# z = Dropout(0.1)(z)\n",
    "# outp = Dense(num_classes, activation=\"softmax\")(z)\n",
    "# model = Model(inputs=inp, outputs=outp)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import Model\n",
    "\n",
    "from keras.layers import Bidirectional, CuDNNLSTM, LSTM, CuDNNGRU, GRU, Embedding\n",
    "from keras.layers import Dense, Input, Dropout, Activation, Conv1D, Flatten, Concatenate\n",
    "from keras.layers import SpatialDropout1D, Dropout, GlobalMaxPooling1D, MaxPooling1D\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping,ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9387, 2830)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1 = Tokenizer()\n",
    "tokenizer1.fit_on_texts(list(df['text']))\n",
    "word_index1 = tokenizer1.word_index\n",
    "\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(list(df['title']))\n",
    "word_index2 = tokenizer2.word_index\n",
    "\n",
    "len(word_index1), len(word_index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8512, 200), (8512, 17))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features1 = len(word_index1) + 1\n",
    "max_features2 = len(word_index2) + 1\n",
    "\n",
    "max_words1 = 200\n",
    "max_words2 = 17\n",
    "\n",
    "text = tokenizer1.texts_to_sequences(df['text'])\n",
    "text = pad_sequences(text, maxlen = max_words1)\n",
    "\n",
    "title = tokenizer2.texts_to_sequences(df['title'])\n",
    "title = pad_sequences(title, maxlen = max_words2)\n",
    "\n",
    "text.shape, title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size   = 32\n",
    "epochs       = 40\n",
    "num_classes  = 21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'attention_3/Sum_1:0' shape=(?, 128) dtype=float32>,\n",
       " <tf.Tensor 'attention_4/Sum_1:0' shape=(?, 64) dtype=float32>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp1 = Input(shape=(max_words1,))\n",
    "inp2 = Input(shape=(max_words2,))\n",
    "\n",
    "x1 = Embedding(max_features1, 300, trainable=True)(inp1)\n",
    "x2 = Embedding(max_features2, 300, trainable=True)(inp2)\n",
    "\n",
    "x1 = Bidirectional(LSTM(128, return_sequences=True))(x1)\n",
    "x1 = Bidirectional(LSTM(64, return_sequences=True))(x1)\n",
    "x1 = Attention(max_words1)(x1)\n",
    "\n",
    "x2 = Bidirectional(LSTM(64, return_sequences=True))(x2)\n",
    "x2 = Bidirectional(LSTM(32, return_sequences=True))(x2)\n",
    "x2 = Attention(max_words2)(x2)\n",
    "\n",
    "x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 200, 300)     2816400     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 17, 300)      849300      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 200, 256)     439296      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 17, 128)      186880      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 200, 128)     164352      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 17, 64)       41216       bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 128)          328         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 64)           81          bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192)          0           attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          24704       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 21)           1365        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,532,178\n",
      "Trainable params: 4,532,178\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = Concatenate(axis=-1)([x1, x2])\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "modelATT = Model(inputs=[inp1, inp2], outputs=x)\n",
    "modelATT.compile(loss='categorical_crossentropy', \n",
    "                 optimizer=Adam(lr=1e-2), metrics=['accuracy'])\n",
    "modelATT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5959, 5), (2553, 2))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_len = train.shape[0]\n",
    "ts_len = test.shape[0]\n",
    "# train.shape[0] + test.shape[0], df.shape[0]\n",
    "train1 = df.iloc[:tr_len]\n",
    "test1  = df.iloc[tr_len:]\n",
    "train1.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping(df, col_name):\n",
    "    cat_codes = df[col_name].astype('category')\n",
    "    \n",
    "    class_mapping = {}\n",
    "    i = 0\n",
    "    for col in cat_codes.cat.categories:\n",
    "        class_mapping[col] = i\n",
    "        i += 1\n",
    "    \n",
    "    class_mapping_reverse = {}\n",
    "    for key, value in class_mapping.items():\n",
    "        class_mapping_reverse[value] = key\n",
    "\n",
    "    return class_mapping, class_mapping_reverse\n",
    "\n",
    "cl_map, cl_map_inv = get_mapping(train1, 'topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5959, 217) (2553, 217)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4171, 217), (4171,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1['target'] = train1['topic'].astype('category').cat.codes\n",
    "train1['target'] = train1['target'].astype('int')\n",
    "\n",
    "text_title = np.concatenate([text, title], axis=1)\n",
    "tr_text_title = text_title[:tr_len]\n",
    "ts_text_title = text_title[tr_len:]\n",
    "print(tr_text_title.shape, ts_text_title.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    tr_text_title, train1['target'], shuffle=True,\n",
    "    stratify=train1['target'], test_size=0.3, random_state=1337\n",
    ")\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4171, 200), (4171, 17), (1788, 200), (1788, 17))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_text  = X_train[:,:200]\n",
    "X_tr_title = X_train[:,200:]\n",
    "\n",
    "X_ts_text  = X_test[:,:200]\n",
    "X_ts_title = X_test[:,200:]\n",
    "\n",
    "X_tr_text.shape, X_tr_title.shape, X_ts_text.shape, X_ts_title.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4171, 21), (1788, 21))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "Y_train = to_categorical(Y_train, num_classes=num_classes)\n",
    "Y_test  = to_categorical(Y_test, num_classes=num_classes)\n",
    "Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.60750658e-05, 1.85060810e-05, 1.85060810e-05, ...,\n",
       "       1.95252374e-05, 1.60750658e-05, 5.45514998e-06])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "def get_class_weights(y):\n",
    "    \"\"\" \n",
    "    Example:\n",
    "        model.fit(X_t, y, batch_size=10, epochs=2,validation_split=0.1,sample_weight=sample_wts)\n",
    "    \n",
    "    \"\"\"\n",
    "    return class_weight.compute_sample_weight('balanced', y)\n",
    "\n",
    "cls_wts = get_class_weights(Y_train)\n",
    "cls_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4171,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_wts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelATT.compile(loss='categorical_crossentropy', \n",
    "                 optimizer=Adam(lr=1e-2), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4171 samples, validate on 1788 samples\n",
      "Epoch 1/40\n",
      "4171/4171 [==============================] - 113s 27ms/step - loss: 8.4009e-05 - acc: 0.0173 - val_loss: 3.0743 - val_acc: 0.0358\n",
      "Epoch 2/40\n",
      "4171/4171 [==============================] - 103s 25ms/step - loss: 8.3744e-05 - acc: 0.0254 - val_loss: 3.0808 - val_acc: 0.0229\n",
      "Epoch 00002: early stopping\n",
      "CPU times: user 11min 22s, sys: 58.9 s, total: 12min 21s\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "early_stop = EarlyStopping(monitor=\"val_acc\", patience=1, verbose=1)\n",
    "history    = modelATT.fit(\n",
    "    [X_tr_text, X_tr_title], Y_train,\n",
    "    validation_data = ([X_ts_text, X_ts_title], Y_test),\n",
    "    epochs          = epochs,\n",
    "    batch_size      = batch_size,\n",
    "    verbose         = 1,\n",
    "    sample_weight   = cls_wts,\n",
    "    callbacks       = [early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "\n",
    "def tfidf_feature(train, test, col_name, min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=3, stopwords='english', \n",
    "                  n_component=120, decom_flag=False, which_method='svd', \n",
    "                  max_features=None):\n",
    "    \"\"\"return tfidf feature\n",
    "    Args:\n",
    "        train, test: dataframe\n",
    "        col_name: column name of text feature\n",
    "        min_df: if Int, then it represent count of the minimum words in corpus (remove very rare word)\n",
    "        analyzer: [‘word’, ‘char’]\n",
    "        ngram: max range of ngram\n",
    "        token_pattern: [using: r'\\w{1,}'] [by default: '(?u)\\b\\w\\w+\\b']\n",
    "        stopwords: ['english' or customized by remove specific words]\n",
    "        n_component: n_component of svd feature transform\n",
    "        decom_flag: Wheteher to run svd/nmf on top of that or not (by default: False)\n",
    "        which_method: which to run [svd or nmf] on top of tfidf (by default: False)\n",
    "        max_features: max no of features to keep, based on frequency. It will keep words with higher freq\n",
    "    return:\n",
    "        Transformed feature space of the text data, as well as tfidf function instance\n",
    "        if svd_flag== True : train_tf, test_tf, tfv, svd\n",
    "        else : train_tf, test_tf, tfv\n",
    "    example:\n",
    "        train_tfv, test_tfv, tfv = tfidf_feature(X_train, X_test, ['text'], min_df=3)\n",
    "        train_svd, test_svd, complete_tfv, tfv, svd = tfidf_feature(X_train, X_test, ['text'], \n",
    "            min_df=3, svd_component=3, svd_flag=True)\n",
    "\n",
    "    \"\"\"\n",
    "    tfv = TfidfVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, max_df=1.0, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                stop_words = stopwords)\n",
    "\n",
    "    complete_df = pd.concat([train[col_name], test[col_name]], axis=0)\n",
    "#         return complete_df\n",
    "#         print(complete_df.shape, complete_df.columns)\n",
    "\n",
    "    tfv.fit(list(complete_df[:].values))\n",
    "\n",
    "    if decom_flag is False:\n",
    "        train_tfv =  tfv.transform(train[col_name].values.ravel()) \n",
    "        test_tfv  = tfv.transform(test[col_name].values.ravel())\n",
    "\n",
    "        del complete_df\n",
    "        gc.collect()\n",
    "        return train_tfv, test_tfv, tfv\n",
    "    else:\n",
    "        complete_tfv = tfv.transform(complete_df[:].values.ravel())\n",
    "        \n",
    "        if which_method is 'svd':\n",
    "            svd = TruncatedSVD(n_components=n_component)\n",
    "            svd.fit(complete_tfv)\n",
    "            complete_dec = svd.transform(complete_tfv)\n",
    "        else:\n",
    "            nmf = NMF(n_components=n_component, random_state=1234, alpha=0, l1_ratio=0)\n",
    "            nmf.fit(complete_tfv)            \n",
    "            complete_dec = nmf.fit_transform(complete_tfv)            \n",
    "        \n",
    "        \n",
    "        complete_dec = pd.DataFrame(data=complete_dec)\n",
    "        complete_dec.columns = [which_method+'_'+str(i) for i in range(n_component)]\n",
    "\n",
    "        train_dec = complete_dec.iloc[:train.shape[0]]\n",
    "        test_dec = complete_dec.iloc[train.shape[0]:].reset_index(drop=True)\n",
    "\n",
    "        del complete_dec, complete_df\n",
    "        gc.collect()\n",
    "        return train_dec, test_dec, complete_tfv, tfv\n",
    "\n",
    "def countvect_feature(train, test, col_name, min_df=3, analyzer='word', token_pattern=r'\\w{1,}', ngram=3, stopwords='english', max_features=None):\n",
    "    \"\"\"return CountVectorizer feature\n",
    "    Args:\n",
    "        train, test: dataset\n",
    "        col_name: columns name of the text feature\n",
    "        min_df: if Int, then it represent count of the minimum words in corpus (remove very rare word)\n",
    "        analyzer: [‘word’, ‘char’]\n",
    "        ngram: max range of ngram\n",
    "        token_pattern: [using: r'\\w{1,}'] [by default: '(?u)\\b\\w\\w+\\b']\n",
    "        stopwords: ['english' or customized by remove specific words]\n",
    "        max_features: max no of features to keep, based on frequency. It will keep words with higher freq\n",
    "    return:\n",
    "        Count feature space of the text data, as well as its function instance\n",
    "    \"\"\"\n",
    "    ctv = CountVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                stop_words = stopwords)\n",
    "\n",
    "    complete_df = pd.concat([train[col_name], test[col_name]], axis=0)\n",
    "    ctv.fit(list(complete_df[:].values))\n",
    "\n",
    "    train_tf =  ctv.transform(train[col_name].values.ravel()) \n",
    "    test_tf  = ctv.transform(test[col_name].values.ravel())\n",
    "\n",
    "    del complete_df\n",
    "    gc.collect()\n",
    "    return train_tf, test_tf, ctv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidfs = []\n",
    "for ngram in [1,2,3,4,5]:\n",
    "    out_tfidfs.append(tfidf_feature(train, test, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidf1 = tfidf_feature(train, test, 'text', ngram=1)\n",
    "out_tfidf2 = tfidf_feature(train, test, 'text', ngram=2)\n",
    "out_tfidf3 = tfidf_feature(train, test, 'text', ngram=3)\n",
    "out_tfidf4 = tfidf_feature(train, test, 'text', ngram=5)\n",
    "\n",
    "out_vect1 = countvect_feature(train, test, 'text', ngram=1)\n",
    "out_vect2 = countvect_feature(train, test, 'text', ngram=2)\n",
    "out_vect3 = countvect_feature(train, test, 'text', ngram=3)\n",
    "out_vect4 = countvect_feature(train, test, 'text', ngram=5)\n",
    "\n",
    "print('tf-idf features: ', out_tfidf1[0].shape, \n",
    "     out_tfidf2[0].shape, out_tfidf3[0].shape, out_tfidf4[0].shape)\n",
    "print('count-vect features: ', out_vect1[0].shape, \n",
    "     out_vect2[0].shape, out_vect3[0].shape, out_vect4[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = train['topic'].astype('category').cat.codes\n",
    "train['target'] = train['target'].astype('int')\n",
    "all_class = list(train['target'].unique())\n",
    "print(len(all_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['target'] == 1].shape, out_tfidf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf1[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "# pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf1[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"tfidf1 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf2[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"tfidf2 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf3[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"tfidf3 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf4[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"tfidf4 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "print(\"==\"*25)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect1[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"count-vect1 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect2[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"count-vect2 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect3[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"count-vect3 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect4[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"count-vect4 : \", clf.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "    C=0.01, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "    random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(logistic_reg.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf1[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"tfidf1 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf2[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"tfidf2 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf3[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"tfidf3 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf4[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"tfidf4 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "print(\"==\"*25)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect1[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"count-vect1 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect2[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"count-vect2 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect3[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"count-vect3 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect4[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"count-vect4 : \", logistic_reg.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('Dataset/Sample_Submission.csv')\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg.fit(out_vect3[0], train['target'])\n",
    "pred = logistic_reg.predict(out_vect3[1])\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def category_encoder(df)\n",
    "test.drop('topic', axis=1, inplace=True)\n",
    "test['target'] = pred\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['topic'] = test['target'].apply(lambda x: class_mapping_reverse[str(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('target', axis=1, inplace=True)\n",
    "# os.makedirs('submission')\n",
    "test.to_csv('submission/linear_model1.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping(df, col_name):\n",
    "    cat_codes = df[col_name].astype('category')\n",
    "    \n",
    "    class_mapping = {}\n",
    "    i = 0\n",
    "    for col in cat_codes.cat.categories:\n",
    "        class_mapping[col] = i\n",
    "        i += 1\n",
    "    \n",
    "    class_mapping_reverse = {}\n",
    "    for key, value in class_mapping.items():\n",
    "        class_mapping_reverse[value] = key\n",
    "\n",
    "    return class_mapping, class_mapping_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping, class_mapping_reverse = get_mapping(train, 'topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "params = {}\n",
    "params['alpha'] = 1\n",
    "passive_agg = PassiveAggressiveClassifier(C=params['alpha'], early_stopping=False, validation_fraction=0.3, n_iter_no_change=5, shuffle=True, verbose=0, n_jobs=-1, random_state=1234, loss='hinge', class_weight='balanced', average=False, n_iter=None)\n",
    "ridge_clf = RidgeClassifier(alpha=params['alpha'], fit_intercept=True, normalize=True, class_weight='balanced', random_state=1234)\n",
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, C=params['alpha'], fit_intercept=True, intercept_scaling=1, class_weight='balanced', random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_clf.fit(X_train, Y_train)\n",
    "print(ridge_clf.score(X_test, Y_test))\n",
    "print(\"==============\")\n",
    "\n",
    "passive_agg.fit(X_train, Y_train)\n",
    "print(passive_agg.score(X_test, Y_test))\n",
    "print(\"==============\")\n",
    "\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(logistic_reg.score(X_test, Y_test))\n",
    "print(\"==============\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for alpha in [0.01, 0.1, 0.5, 1, 5, 10]:\n",
    "    logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "        C=alpha, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "        random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "    logistic_reg.fit(X_train, Y_train)\n",
    "    pred = logistic_reg.score(X_test, Y_test)\n",
    "    print(pred)\n",
    "#     print(f1_score(ts_y, pred, average='micro', sample_weight=None))\n",
    "#     print(logistic_reg.score(ts_x, ts_y))\n",
    "print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for alpha in np.linspace(0.0001,0.1,10):\n",
    "    logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "        C=alpha, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "        random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "    logistic_reg.fit(X_train, Y_train)\n",
    "    pred = logistic_reg.score(X_test, Y_test)\n",
    "    print(alpha, \" : \", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Usage: plot_document_classification_20newsgroups.py [options]\n",
    "\n",
    "Options:\n",
    "  -h, --help            show this help message and exit\n",
    "  --report              Print a detailed classification report.\n",
    "  --chi2_select=SELECT_CHI2\n",
    "                        Select some number of features using a chi-squared\n",
    "                        test\n",
    "  --confusion_matrix    Print the confusion matrix.\n",
    "  --top10               Print ten most discriminative terms per class for\n",
    "                        every classifier.\n",
    "  --all_categories      Whether to use all categories or not.\n",
    "  --use_hashing         Use a hashing vectorizer.\n",
    "  --n_features=N_FEATURES\n",
    "                        n_features when using the hashing vectorizer.\n",
    "  --filtered            Remove newsgroup information that is easily overfit:\n",
    "                        headers, signatures, and quoting.\n",
    "\n",
    "Loading 20 newsgroups dataset for categories:\n",
    "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "data loaded\n",
    "2034 documents - 3.980MB (training set)\n",
    "1353 documents - 2.867MB (test set)\n",
    "4 categories\n",
    "\n",
    "Extracting features from the training data using a sparse vectorizer\n",
    "done in 0.412178s at 9.655MB/s\n",
    "n_samples: 2034, n_features: 33809\n",
    "\n",
    "Extracting features from the test data using the same vectorizer\n",
    "done in 0.351330s at 8.162MB/s\n",
    "n_samples: 1353, n_features: 33809\n",
    "\n",
    "================================================================================\n",
    "Ridge Classifier\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "RidgeClassifier(solver='sag', tol=0.01)\n",
    "train time: 0.132s\n",
    "test time:  0.001s\n",
    "accuracy:   0.896\n",
    "dimensionality: 33809\n",
    "density: 1.000000\n",
    "\n",
    "\n",
    "================================================================================\n",
    "Perceptron\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "Perceptron(max_iter=50)\n",
    "train time: 0.017s\n",
    "test time:  0.002s\n",
    "accuracy:   0.888\n",
    "dimensionality: 33809\n",
    "density: 0.255302\n",
    "\n",
    "\n",
    "================================================================================\n",
    "Passive-Aggressive\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "PassiveAggressiveClassifier(max_iter=50)\n",
    "train time: 0.031s\n",
    "test time:  0.002s\n",
    "accuracy:   0.904\n",
    "dimensionality: 33809\n",
    "density: 0.694674\n",
    "\n",
    "\n",
    "================================================================================\n",
    "kNN\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "KNeighborsClassifier(n_neighbors=10)\n",
    "train time: 0.002s\n",
    "test time:  0.317s\n",
    "accuracy:   0.858\n",
    "\n",
    "================================================================================\n",
    "Random forest\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "RandomForestClassifier(n_estimators=100)\n",
    "train time: 1.671s\n",
    "test time:  0.071s\n",
    "accuracy:   0.840\n",
    "\n",
    "================================================================================\n",
    "L2 penalty\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "LinearSVC(dual=False, tol=0.001)\n",
    "train time: 0.145s\n",
    "test time:  0.002s\n",
    "accuracy:   0.900\n",
    "dimensionality: 33809\n",
    "density: 1.000000\n",
    "\n",
    "\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "SGDClassifier(max_iter=50)\n",
    "train time: 0.030s\n",
    "test time:  0.002s\n",
    "accuracy:   0.902\n",
    "dimensionality: 33809\n",
    "density: 0.579380\n",
    "\n",
    "\n",
    "================================================================================\n",
    "L1 penalty\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "LinearSVC(dual=False, penalty='l1', tol=0.001)\n",
    "train time: 0.301s\n",
    "test time:  0.002s\n",
    "accuracy:   0.873\n",
    "dimensionality: 33809\n",
    "density: 0.005553\n",
    "\n",
    "\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "SGDClassifier(max_iter=50, penalty='l1')\n",
    "train time: 0.093s\n",
    "test time:  0.002s\n",
    "accuracy:   0.887\n",
    "dimensionality: 33809\n",
    "density: 0.022901\n",
    "\n",
    "\n",
    "================================================================================\n",
    "Elastic-Net penalty\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "SGDClassifier(max_iter=50, penalty='elasticnet')\n",
    "train time: 0.252s\n",
    "test time:  0.002s\n",
    "accuracy:   0.899\n",
    "dimensionality: 33809\n",
    "density: 0.187472\n",
    "\n",
    "\n",
    "================================================================================\n",
    "NearestCentroid (aka Rocchio classifier)\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "NearestCentroid()\n",
    "train time: 0.004s\n",
    "test time:  0.002s\n",
    "accuracy:   0.855\n",
    "\n",
    "================================================================================\n",
    "Naive Bayes\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "MultinomialNB(alpha=0.01)\n",
    "train time: 0.003s\n",
    "test time:  0.001s\n",
    "accuracy:   0.899\n",
    "dimensionality: 33809\n",
    "density: 1.000000\n",
    "\n",
    "\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "BernoulliNB(alpha=0.01)\n",
    "train time: 0.004s\n",
    "test time:  0.003s\n",
    "accuracy:   0.884\n",
    "dimensionality: 33809\n",
    "density: 1.000000\n",
    "\n",
    "\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "ComplementNB(alpha=0.1)\n",
    "train time: 0.004s\n",
    "test time:  0.001s\n",
    "accuracy:   0.911\n",
    "dimensionality: 33809\n",
    "density: 1.000000\n",
    "\n",
    "\n",
    "================================================================================\n",
    "LinearSVC with L1-based feature selection\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "Pipeline(steps=[('feature_selection',\n",
    "                 SelectFromModel(estimator=LinearSVC(dual=False, penalty='l1',\n",
    "                                                     tol=0.001))),\n",
    "                ('classification', LinearSVC())])\n",
    "train time: 0.252s\n",
    "test time:  0.002s\n",
    "accuracy:   0.880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_train[i] # feature count vector for training case i\n",
    "# y_train[i] # label for training case i\n",
    "\n",
    "# The count vectors are defined as:\n",
    "\n",
    "# p = sum of all feature count vectors with label 1\n",
    "\n",
    "# p = tf_train[y_train==1].sum(0) + 1\n",
    "\n",
    "# q = sum of all feature count vectors with label 0\n",
    "\n",
    "# q = tf_train[y_train==0].sum(0) + 1\n",
    "\n",
    "# Notice that we add 1 to both count vectors to ensure that every token appear at least one time in each class.\n",
    "\n",
    "# The log-count ratio r is:\n",
    "\n",
    "# r = np.log((p/p.sum()) / (q/q.sum()))\n",
    "\n",
    "# And b:\n",
    "\n",
    "# b = np.log(len(p) / len(q))\n",
    "\n",
    "# Just the ratio of number of positive and negative training cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_preds = tf_test @ r.T + b\n",
    "preds = pre_preds.T > 0\n",
    "accuracy = (preds == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidf1[0][train['target'] == 5].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidf1[0][train['target'] == 4].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.equal(out_tfidf1[0][train['target'] == 5].sum(0), \n",
    "                out_tfidf1[0][train['target'] == 3].sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidf1[0][train['target'] == 5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidf1[0][train['target'] == 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = train[train['target'] == 2].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidf1[0][idx].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts):\n",
    "    output = []\n",
    "    for i in texts:\n",
    "        s = [token.lemma_ for token in nlp(i)]\n",
    "        output.append(' '.join(s))\n",
    "    return output\n",
    "\n",
    "# train['text1'] = train['text'].progress_apply(lemmatization)\n",
    "# test['text1']  = test['text'].progress_apply(lemmatization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.append(test, ignore_index=True)\n",
    "df['text1'] = df['text'].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:5]['text'].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(text):\n",
    "    return []\n",
    "    for i in texts:\n",
    "        s = [token.lemma_ for token in nlp(i)]\n",
    "        output.append(' '.join(s))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenize(df1['text'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "porter.stem('helping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in df1['text'][1].split(\" \"):\n",
    "    print(porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del df1, df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.append(test, ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "cv = CountVectorizer(max_df=0.95,min_df=2,stop_words='english')\n",
    "term_matrix = cv.fit_transform(df['text'])\n",
    "# print(term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, n_jobs=4)\n",
    "lda.fit(term_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = lda.components_[0]\n",
    "top_words_indices = topic.argsort()[-10:]\n",
    "for index in top_words_indices:\n",
    "    print(cv.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_dict = {}\n",
    "for index,topic in enumerate(lda.components_):\n",
    "    words = [cv.get_feature_names()[i] for i in topic.argsort()[-10:]]\n",
    "    topic_word_dict[index] = words\n",
    "    print('Top words for topic {}'.format(index))\n",
    "    print(words)\n",
    "    print('-'*120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics = lda.transform(term_matrix)\n",
    "data['topic'] = topics.argmax(axis=1)\n",
    "\n",
    "\n",
    "def assign_topics(row):\n",
    "    topic = row['topic']\n",
    "    words = topic_word_dict[topic]\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "data['topic words'] = data.apply(assign_topics,axis=1)\n",
    "print(data.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
