{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tqdm.pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os, gc\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5959, 3), (2553, 2), (5, 3))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('Dataset/train.csv')\n",
    "test  = pd.read_csv('Dataset/test.csv')\n",
    "sub   = pd.read_csv('Dataset/Sample_Submission.csv')\n",
    "\n",
    "train.shape, test.shape, sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['text', 'title'], dtype='object'),\n",
       " Index(['text', 'title', 'topic'], dtype='object'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.rename(columns={'Review Text':'text', 'Review Title':'title'}, inplace=True)\n",
    "test.rename(columns={'Review Text':'text', 'Review Title':'title'}, inplace=True)\n",
    "test.columns, train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.append(test, ignore_index=True)\n",
    "df['text']  = df['text'].str.lower()\n",
    "df['title'] = df['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('contraction_mapping.txt') as f:\n",
    "    contraction_mapping = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8512/8512 [00:00<00:00, 54357.69it/s]\n",
      "100%|██████████| 8512/8512 [00:00<00:00, 191323.47it/s]\n"
     ]
    }
   ],
   "source": [
    "def correct_contraction(x, dic):\n",
    "    for word in dic.keys():\n",
    "        if word in x:\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "df['text']  = df['text'].progress_apply(lambda x: correct_contraction(x, contraction_mapping))\n",
    "df['title'] = df['title'].progress_apply(lambda x: correct_contraction(x, contraction_mapping))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8512/8512 [00:00<00:00, 98500.81it/s]\n",
      "100%|██████████| 8512/8512 [00:00<00:00, 168589.76it/s]\n"
     ]
    }
   ],
   "source": [
    "import os,operator\n",
    "\n",
    "extra_punct = [\n",
    "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
    "\n",
    "\n",
    "import string\n",
    "my_punct = list(string.punctuation)\n",
    "all_punct = list(set(my_punct + extra_punct))\n",
    "\n",
    "special_punc_mappings = {\"—\": \"-\", \"–\": \"-\", \"_\": \"-\", '”': '\"', \"″\": '\"', '“': '\"', '•': '.', '−': '-',\n",
    "                         \"’\": \"'\", \"‘\": \"'\", \"´\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','،':'','„':'',\n",
    "                         '…': ' ... ', '\\ufeff': ''}\n",
    "\n",
    "def spacing_punctuation(text):\n",
    "    \"\"\"\n",
    "    add space before and after punctuation and symbols\n",
    "    \"\"\"\n",
    "    for punc in all_punct:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, f' {punc} ')\n",
    "    return text\n",
    "\n",
    "def clean_special_punctuations(text):\n",
    "    for punc in special_punc_mappings:\n",
    "        if punc in text:\n",
    "#             print(punc)\n",
    "            text = text.replace(punc, special_punc_mappings[punc])\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = spacing_punctuation(text)\n",
    "    text = clean_special_punctuations(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"text\"] = df[\"text\"].progress_apply(preprocess)\n",
    "df['text'] = df['text'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')\n",
    "df['text'].replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "df['text'].replace({'  ':' '}, regex=True, inplace=True)\n",
    "\n",
    "df[\"title\"] = df[\"title\"].progress_apply(preprocess)\n",
    "df['title'] = df['title'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')\n",
    "df['title'].replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "df['title'].replace({'  ':' '}, regex=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8512/8512 [00:00<00:00, 25281.17it/s]\n",
      "100%|██████████| 8512/8512 [00:00<00:00, 113227.62it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@[a-zA-Z0-9_]+', '', text)   \n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)   \n",
    "    text = re.sub(r'www.[^ ]+', '', text)  \n",
    "    text = re.sub(r'[a-zA-Z0-9]*www[a-zA-Z0-9]*com[a-zA-Z0-9]*', '', text)  \n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)   \n",
    "    text = [token for token in text.split() if len(token) > 2]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "df['text']   = df['text'].progress_apply(clean_text)\n",
    "df['title']  = df['title'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8512/8512 [00:00<00:00, 100302.34it/s]\n",
      "100%|██████████| 8512/8512 [00:00<00:00, 141017.31it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x\n",
    "\n",
    "df['text'] = df['text'].progress_apply(clean_text)\n",
    "df['title'] = df['title'].progress_apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'My': 1, 'name': 2, 'is': 2, 'z': 2})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "sent = \"My name is z name is z\"\n",
    "# for word in sent.split(\" \"):\n",
    "# c = [collections.Counter(word) for word in sent.split(\" \")]\n",
    "collections.Counter(sent.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for sent in df['title']:\n",
    "    for word in sent.split(\" \"):\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2831"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('not', 2041),\n",
       " ('taste', 759),\n",
       " ('the', 740),\n",
       " ('and', 665),\n",
       " ('bad', 610),\n",
       " ('for', 547),\n",
       " ('product', 486),\n",
       " ('good', 469),\n",
       " ('but', 399),\n",
       " ('flavor', 363),\n",
       " ('did', 308),\n",
       " ('this', 282),\n",
       " ('like', 265),\n",
       " ('work', 257),\n",
       " ('buy', 255),\n",
       " ('too', 245),\n",
       " ('very', 231),\n",
       " ('does', 209),\n",
       " ('received', 200),\n",
       " ('horrible', 199),\n",
       " ('tastes', 187),\n",
       " ('smell', 187),\n",
       " ('broken', 187),\n",
       " ('terrible', 183),\n",
       " ('wrong', 174),\n",
       " ('texture', 172),\n",
       " ('was', 167),\n",
       " ('with', 165),\n",
       " ('you', 161),\n",
       " ('never', 159),\n",
       " ('great', 156),\n",
       " ('item', 150),\n",
       " ('awful', 146),\n",
       " ('these', 142),\n",
       " ('are', 138),\n",
       " ('have', 136),\n",
       " ('packaging', 135),\n",
       " ('gross', 131),\n",
       " ('money', 129),\n",
       " ('they', 127),\n",
       " ('just', 124),\n",
       " ('sweet', 122),\n",
       " ('arrived', 121),\n",
       " ('quality', 120),\n",
       " ('brand', 120),\n",
       " ('bottle', 118),\n",
       " ('all', 118),\n",
       " ('poor', 115),\n",
       " ('what', 115),\n",
       " ('order', 112),\n",
       " ('pills', 107),\n",
       " ('weird', 107),\n",
       " ('from', 105),\n",
       " ('again', 102),\n",
       " ('get', 98),\n",
       " ('delivery', 96),\n",
       " ('stomach', 96),\n",
       " ('waste', 90),\n",
       " ('your', 88),\n",
       " ('disgusting', 88),\n",
       " ('made', 86),\n",
       " ('after', 84),\n",
       " ('really', 82),\n",
       " ('smells', 79),\n",
       " ('big', 76),\n",
       " ('that', 76),\n",
       " ('expired', 76),\n",
       " ('will', 76),\n",
       " ('sent', 76),\n",
       " ('shipping', 74),\n",
       " ('beware', 74),\n",
       " ('', 74),\n",
       " ('better', 72),\n",
       " ('sticky', 70),\n",
       " ('price', 69),\n",
       " ('disappointed', 69),\n",
       " ('fish', 69),\n",
       " ('well', 68),\n",
       " ('damaged', 67),\n",
       " ('receive', 67),\n",
       " ('package', 66),\n",
       " ('nothing', 64),\n",
       " ('would', 64),\n",
       " ('reaction', 63),\n",
       " ('fishy', 63),\n",
       " ('delivered', 63),\n",
       " ('has', 63),\n",
       " ('much', 62),\n",
       " ('one', 62),\n",
       " ('hard', 62),\n",
       " ('them', 61),\n",
       " ('date', 60),\n",
       " ('cannot', 59),\n",
       " ('allergic', 59),\n",
       " ('seal', 59),\n",
       " ('ingredients', 59),\n",
       " ('vitamin', 58),\n",
       " ('sick', 58),\n",
       " ('swallow', 58),\n",
       " ('time', 57),\n",
       " ('oil', 57),\n",
       " ('nasty', 57),\n",
       " ('sugar', 56),\n",
       " ('okay', 56),\n",
       " ('seller', 56),\n",
       " ('works', 54),\n",
       " ('yuck', 53),\n",
       " ('fake', 52),\n",
       " ('expiration', 51),\n",
       " ('vitamins', 51),\n",
       " ('old', 50),\n",
       " ('than', 50),\n",
       " ('difference', 50),\n",
       " ('worth', 49),\n",
       " ('only', 48),\n",
       " ('aftertaste', 48),\n",
       " ('got', 48),\n",
       " ('return', 48),\n",
       " ('huge', 47),\n",
       " ('out', 47),\n",
       " ('take', 47),\n",
       " ('service', 47),\n",
       " ('had', 46),\n",
       " ('effective', 46),\n",
       " ('other', 45),\n",
       " ('open', 44),\n",
       " ('side', 44),\n",
       " ('protein', 42),\n",
       " ('stuck', 42),\n",
       " ('together', 42),\n",
       " ('tasting', 42),\n",
       " ('use', 42),\n",
       " ('misleading', 41),\n",
       " ('pill', 41),\n",
       " ('melted', 40),\n",
       " ('way', 40),\n",
       " ('came', 39),\n",
       " ('gave', 39),\n",
       " ('when', 39),\n",
       " ('something', 38),\n",
       " ('chalky', 38),\n",
       " ('changed', 38),\n",
       " ('contains', 38),\n",
       " ('gummies', 37),\n",
       " ('gummy', 37),\n",
       " ('chocolate', 36),\n",
       " ('can', 36),\n",
       " ('were', 35),\n",
       " ('opened', 35),\n",
       " ('more', 35),\n",
       " ('ordered', 35),\n",
       " ('missing', 35),\n",
       " ('effects', 35),\n",
       " ('help', 35),\n",
       " ('expensive', 34),\n",
       " ('new', 34),\n",
       " ('batch', 34),\n",
       " ('formula', 34),\n",
       " ('plastic', 34),\n",
       " ('caused', 33),\n",
       " ('love', 33),\n",
       " ('customer', 33),\n",
       " ('best', 33),\n",
       " ('used', 33),\n",
       " ('strong', 32),\n",
       " ('stale', 32),\n",
       " ('makes', 32),\n",
       " ('some', 32),\n",
       " ('low', 31),\n",
       " ('box', 31),\n",
       " ('purchase', 31),\n",
       " ('large', 31),\n",
       " ('nope', 31),\n",
       " ('size', 30),\n",
       " ('now', 30),\n",
       " ('day', 30),\n",
       " ('different', 30),\n",
       " ('products', 30),\n",
       " ('skin', 30),\n",
       " ('refund', 30),\n",
       " ('tried', 29),\n",
       " ('little', 29),\n",
       " ('meh', 29),\n",
       " ('off', 29),\n",
       " ('rancid', 29),\n",
       " ('powder', 29),\n",
       " ('kids', 29),\n",
       " ('two', 29),\n",
       " ('dont', 28),\n",
       " ('could', 28),\n",
       " ('supplement', 28),\n",
       " ('issue', 28),\n",
       " ('why', 28),\n",
       " ('upset', 28),\n",
       " ('upon', 28),\n",
       " ('sure', 28),\n",
       " ('chemical', 28),\n",
       " ('first', 27),\n",
       " ('high', 27),\n",
       " ('most', 27),\n",
       " ('working', 27),\n",
       " ('effect', 27),\n",
       " ('capsules', 27),\n",
       " ('horse', 27),\n",
       " ('disappointing', 27),\n",
       " ('worst', 27),\n",
       " ('over', 27),\n",
       " ('ever', 27),\n",
       " ('expected', 27),\n",
       " ('its', 27),\n",
       " ('value', 27),\n",
       " ('recommend', 26),\n",
       " ('back', 26),\n",
       " ('food', 26),\n",
       " ('still', 26),\n",
       " ('vanilla', 26),\n",
       " ('even', 26),\n",
       " ('fine', 26),\n",
       " ('useless', 25),\n",
       " ('fan', 25),\n",
       " ('before', 25),\n",
       " ('long', 25),\n",
       " ('control', 25),\n",
       " ('ineffective', 25),\n",
       " ('shipped', 25),\n",
       " ('buying', 25),\n",
       " ('worse', 25),\n",
       " ('careful', 25),\n",
       " ('feel', 25),\n",
       " ('contain', 25),\n",
       " ('may', 24),\n",
       " ('bottles', 24),\n",
       " ('empty', 24),\n",
       " ('maybe', 24),\n",
       " ('sour', 24),\n",
       " ('gritty', 24),\n",
       " ('about', 24),\n",
       " ('buyer', 24),\n",
       " ('flavors', 24),\n",
       " ('any', 23),\n",
       " ('change', 23),\n",
       " ('less', 23),\n",
       " ('cheap', 23),\n",
       " ('stevia', 23),\n",
       " ('months', 23),\n",
       " ('anything', 23),\n",
       " ('results', 22),\n",
       " ('super', 22),\n",
       " ('away', 22),\n",
       " ('hot', 22),\n",
       " ('many', 21),\n",
       " ('where', 21),\n",
       " ('energy', 21),\n",
       " ('stuff', 21),\n",
       " ('bitter', 21),\n",
       " ('extremely', 21),\n",
       " ('decent', 21),\n",
       " ('arrival', 21),\n",
       " ('mix', 21),\n",
       " ('cap', 21),\n",
       " ('tasty', 21),\n",
       " ('iron', 21),\n",
       " ('chew', 21),\n",
       " ('container', 20),\n",
       " ('issues', 20),\n",
       " ('dissolve', 20),\n",
       " ('pretty', 20),\n",
       " ('same', 20),\n",
       " ('full', 20),\n",
       " ('read', 20),\n",
       " ('glass', 20),\n",
       " ('organic', 20),\n",
       " ('impressed', 19),\n",
       " ('look', 19),\n",
       " ('bag', 19),\n",
       " ('instead', 19),\n",
       " ('cause', 19),\n",
       " ('cookies', 19),\n",
       " ('natural', 19),\n",
       " ('enough', 19),\n",
       " ('headache', 19),\n",
       " ('fast', 19),\n",
       " ('twice', 19),\n",
       " ('mess', 19),\n",
       " ('brands', 19),\n",
       " ('odor', 19),\n",
       " ('need', 18),\n",
       " ('dry', 18),\n",
       " ('thick', 18),\n",
       " ('rash', 18),\n",
       " ('almost', 18),\n",
       " ('hate', 18),\n",
       " ('happy', 18),\n",
       " ('description', 18),\n",
       " ('please', 18),\n",
       " ('diarrhea', 18),\n",
       " ('want', 18),\n",
       " ('hair', 18),\n",
       " ('vegan', 18),\n",
       " ('weight', 18),\n",
       " ('items', 18),\n",
       " ('unsealed', 17),\n",
       " ('leaked', 17),\n",
       " ('lid', 17),\n",
       " ('sleep', 17),\n",
       " ('pain', 17),\n",
       " ('past', 17),\n",
       " ('drink', 17),\n",
       " ('absolutely', 17),\n",
       " ('try', 17),\n",
       " ('tablets', 17),\n",
       " ('odd', 17),\n",
       " ('scent', 17),\n",
       " ('check', 16),\n",
       " ('send', 16),\n",
       " ('easy', 16),\n",
       " ('been', 16),\n",
       " ('easily', 16),\n",
       " ('might', 16),\n",
       " ('nauseous', 16),\n",
       " ('acid', 16),\n",
       " ('burns', 16),\n",
       " ('trash', 15),\n",
       " ('lost', 15),\n",
       " ('nice', 15),\n",
       " ('else', 15),\n",
       " ('consistency', 15),\n",
       " ('month', 15),\n",
       " ('last', 15),\n",
       " ('safety', 15),\n",
       " ('constipation', 15),\n",
       " ('water', 15),\n",
       " ('stick', 15),\n",
       " ('how', 15),\n",
       " ('save', 15),\n",
       " ('artificial', 15),\n",
       " ('bought', 15),\n",
       " ('stars', 15),\n",
       " ('strawberry', 15),\n",
       " ('toxic', 15),\n",
       " ('expire', 15),\n",
       " ('lot', 15),\n",
       " ('here', 15),\n",
       " ('eyes', 15),\n",
       " ('smelled', 14),\n",
       " ('yucky', 14),\n",
       " ('shipment', 14),\n",
       " ('gas', 14),\n",
       " ('thin', 14),\n",
       " ('alcohol', 14),\n",
       " ('milk', 14),\n",
       " ('three', 14),\n",
       " ('right', 14),\n",
       " ('there', 14),\n",
       " ('sensitive', 14),\n",
       " ('candy', 14),\n",
       " ('store', 14),\n",
       " ('soon', 14),\n",
       " ('using', 14),\n",
       " ('improvement', 14),\n",
       " ('severe', 14),\n",
       " ('everyone', 14),\n",
       " ('warm', 14),\n",
       " ('mouth', 14),\n",
       " ('rotten', 14),\n",
       " ('others', 14),\n",
       " ('flavored', 14),\n",
       " ('nausea', 13),\n",
       " ('pack', 13),\n",
       " ('serving', 13),\n",
       " ('difficult', 13),\n",
       " ('sealed', 13),\n",
       " ('deal', 13),\n",
       " ('see', 13),\n",
       " ('allergy', 13),\n",
       " ('every', 13),\n",
       " ('noticeable', 13),\n",
       " ('cetaphil', 13),\n",
       " ('taking', 13),\n",
       " ('should', 13),\n",
       " ('reviews', 13),\n",
       " ('wipes', 13),\n",
       " ('star', 13),\n",
       " ('tablet', 13),\n",
       " ('coated', 13),\n",
       " ('favorite', 13),\n",
       " ('correct', 13),\n",
       " ('condition', 13),\n",
       " ('mold', 13),\n",
       " ('days', 13),\n",
       " ('inside', 13),\n",
       " ('wet', 13),\n",
       " ('teeth', 13),\n",
       " ('started', 13),\n",
       " ('care', 13),\n",
       " ('stinky', 13),\n",
       " ('life', 13),\n",
       " ('half', 13),\n",
       " ('per', 12),\n",
       " ('cost', 12),\n",
       " ('hit', 12),\n",
       " ('tasted', 12),\n",
       " ('chemicals', 12),\n",
       " ('compared', 12),\n",
       " ('chewy', 12),\n",
       " ('experience', 12),\n",
       " ('sugary', 12),\n",
       " ('headaches', 12),\n",
       " ('soft', 12),\n",
       " ('packaged', 12),\n",
       " ('coffee', 12),\n",
       " ('sucralose', 12),\n",
       " ('face', 12),\n",
       " ('wish', 12),\n",
       " ('real', 12),\n",
       " ('week', 12),\n",
       " ('know', 12),\n",
       " ('sweetener', 12),\n",
       " ('false', 12),\n",
       " ('thing', 12),\n",
       " ('scoop', 12),\n",
       " ('cinnamon', 12),\n",
       " ('completely', 12),\n",
       " ('pure', 12),\n",
       " ('synthetic', 12),\n",
       " ('warning', 12),\n",
       " ('corn', 12),\n",
       " ('loss', 12),\n",
       " ('soy', 12),\n",
       " ('took', 12),\n",
       " ('fresh', 12),\n",
       " ('available', 12),\n",
       " ('review', 12),\n",
       " ('centrum', 12),\n",
       " ('done', 12),\n",
       " ('bags', 11),\n",
       " ('increase', 11),\n",
       " ('always', 11),\n",
       " ('dha', 11),\n",
       " ('cold', 11),\n",
       " ('free', 11),\n",
       " ('yet', 11),\n",
       " ('looks', 11),\n",
       " ('clumpy', 11),\n",
       " ('zinc', 11),\n",
       " ('causes', 11),\n",
       " ('sucks', 11),\n",
       " ('delicious', 11),\n",
       " ('everywhere', 11),\n",
       " ('option', 11),\n",
       " ('yikes', 11),\n",
       " ('defective', 11),\n",
       " ('who', 11),\n",
       " ('make', 11),\n",
       " ('nestle', 11),\n",
       " ('rather', 11),\n",
       " ('mushy', 11),\n",
       " ('leaky', 11),\n",
       " ('during', 11),\n",
       " ('problems', 11),\n",
       " ('pre', 11),\n",
       " ('lemon', 11),\n",
       " ('their', 11),\n",
       " ('garden', 11),\n",
       " ('switch', 11),\n",
       " ('aware', 11),\n",
       " ('watch', 11),\n",
       " ('folic', 11),\n",
       " ('stay', 11),\n",
       " ('spots', 11),\n",
       " ('people', 11),\n",
       " ('name', 11),\n",
       " ('totally', 10),\n",
       " ('trust', 10),\n",
       " ('notice', 10),\n",
       " ('priced', 10),\n",
       " ('fruit', 10),\n",
       " ('broke', 10),\n",
       " ('red', 10),\n",
       " ('leaks', 10),\n",
       " ('company', 10),\n",
       " ('needs', 10),\n",
       " ('otherwise', 10),\n",
       " ('our', 10),\n",
       " ('dreams', 10),\n",
       " ('heartburn', 10),\n",
       " ('advertising', 10),\n",
       " ('down', 10),\n",
       " ('went', 10),\n",
       " ('smelling', 10),\n",
       " ('sold', 10),\n",
       " ('poison', 10),\n",
       " ('ingredient', 10),\n",
       " ('list', 10),\n",
       " ('gentle', 10),\n",
       " ('flavorless', 10),\n",
       " ('bland', 10),\n",
       " ('expires', 10),\n",
       " ('non', 10),\n",
       " ('choice', 10),\n",
       " ('overpriced', 10),\n",
       " ('transit', 10),\n",
       " ('cramps', 10),\n",
       " ('cherry', 10),\n",
       " ('things', 10),\n",
       " ('horrid', 9),\n",
       " ('slow', 9),\n",
       " ('symptoms', 9),\n",
       " ('thought', 9),\n",
       " ('also', 9),\n",
       " ('hives', 9),\n",
       " ('miss', 9),\n",
       " ('color', 9),\n",
       " ('formulation', 9),\n",
       " ('contact', 9),\n",
       " ('regular', 9),\n",
       " ('capsule', 9),\n",
       " ('version', 9),\n",
       " ('hype', 9),\n",
       " ('rip', 9),\n",
       " ('kind', 9),\n",
       " ('nutrition', 9),\n",
       " ('caution', 9),\n",
       " ('average', 9),\n",
       " ('deliver', 9),\n",
       " ('weeks', 9),\n",
       " ('because', 9),\n",
       " ('cake', 9),\n",
       " ('felt', 9),\n",
       " ('sticks', 9),\n",
       " ('health', 9),\n",
       " ('arrive', 9),\n",
       " ('small', 9),\n",
       " ('advertised', 9),\n",
       " ('slightly', 9),\n",
       " ('job', 9),\n",
       " ('ruined', 9),\n",
       " ('diapers', 9),\n",
       " ('opening', 9),\n",
       " ('greasy', 9),\n",
       " ('ugh', 9),\n",
       " ('melatonin', 9),\n",
       " ('unbearable', 9),\n",
       " ('cream', 9),\n",
       " ('bring', 8),\n",
       " ('servings', 8),\n",
       " ('almond', 8),\n",
       " ('discolored', 8),\n",
       " ('nearly', 8),\n",
       " ('skip', 8),\n",
       " ('vega', 8),\n",
       " ('unable', 8),\n",
       " ('without', 8),\n",
       " ('probably', 8),\n",
       " ('allergies', 8),\n",
       " ('syrup', 8),\n",
       " ('unpleasant', 8),\n",
       " ('takes', 8),\n",
       " ('dangerous', 8),\n",
       " ('mint', 8),\n",
       " ('hoping', 8),\n",
       " ('itchy', 8),\n",
       " ('paid', 8),\n",
       " ('ice', 8),\n",
       " ('stink', 8),\n",
       " ('close', 8),\n",
       " ('hated', 8),\n",
       " ('absorb', 8),\n",
       " ('costco', 8),\n",
       " ('returned', 8),\n",
       " ('lotion', 8),\n",
       " ('strange', 8),\n",
       " ('online', 8),\n",
       " ('multivitamin', 8),\n",
       " ('ripped', 8),\n",
       " ('guess', 8),\n",
       " ('spoiled', 8),\n",
       " ('quite', 8),\n",
       " ('tea', 8),\n",
       " ('true', 8),\n",
       " ('didnt', 8),\n",
       " ('gives', 8),\n",
       " ('purple', 8),\n",
       " ('bother', 8),\n",
       " ('multivitamins', 8),\n",
       " ('seem', 8),\n",
       " ('wanted', 8),\n",
       " ('inedible', 8),\n",
       " ('actually', 8),\n",
       " ('rubbery', 8),\n",
       " ('pass', 8),\n",
       " ('green', 8),\n",
       " ('getting', 8),\n",
       " ('put', 8),\n",
       " ('weak', 8),\n",
       " ('itself', 8),\n",
       " ('scam', 8),\n",
       " ('messy', 8),\n",
       " ('dose', 7),\n",
       " ('information', 7),\n",
       " ('tummy', 7),\n",
       " ('prenatals', 7),\n",
       " ('bit', 7),\n",
       " ('deceptive', 7),\n",
       " ('late', 7),\n",
       " ('blend', 7),\n",
       " ('tampered', 7),\n",
       " ('tell', 7),\n",
       " ('adhesive', 7),\n",
       " ('break', 7),\n",
       " ('acne', 7),\n",
       " ('creatine', 7),\n",
       " ('pictured', 7),\n",
       " ('left', 7),\n",
       " ('helps', 7),\n",
       " ('chalk', 7),\n",
       " ('gooey', 7),\n",
       " ('folate', 7),\n",
       " ('address', 7),\n",
       " ('short', 7),\n",
       " ('seals', 7),\n",
       " ('garbage', 7),\n",
       " ('workout', 7),\n",
       " ('found', 7),\n",
       " ('sticking', 7),\n",
       " ('quantity', 7),\n",
       " ('original', 7),\n",
       " ('dog', 7),\n",
       " ('avoid', 7),\n",
       " ('find', 7),\n",
       " ('form', 7),\n",
       " ('night', 7),\n",
       " ('lead', 7),\n",
       " ('healthy', 7),\n",
       " ('yes', 7),\n",
       " ('seriously', 7),\n",
       " ('once', 7),\n",
       " ('pregnant', 7),\n",
       " ('replacement', 7),\n",
       " ('solid', 7),\n",
       " ('seems', 7),\n",
       " ('leak', 7),\n",
       " ('expiry', 7),\n",
       " ('whole', 7),\n",
       " ('believe', 7),\n",
       " ('nutritional', 7),\n",
       " ('berry', 7),\n",
       " ('poorly', 7),\n",
       " ('gone', 7),\n",
       " ('probiotics', 7),\n",
       " ('room', 7),\n",
       " ('watermelon', 7),\n",
       " ('recipe', 7),\n",
       " ('aweful', 7),\n",
       " ('prenatal', 7),\n",
       " ('spilled', 7),\n",
       " ('within', 7),\n",
       " ('ache', 7),\n",
       " ('filling', 7),\n",
       " ('potent', 7),\n",
       " ('batches', 7),\n",
       " ('research', 7),\n",
       " ('turbo', 7),\n",
       " ('finish', 7),\n",
       " ('apple', 7),\n",
       " ('intended', 7),\n",
       " ('standard', 7),\n",
       " ('prime', 7),\n",
       " ('arms', 7),\n",
       " ('residue', 7),\n",
       " ('messes', 6),\n",
       " ('bottom', 6),\n",
       " ('increased', 6),\n",
       " ('amazing', 6),\n",
       " ('returns', 6),\n",
       " ('give', 6),\n",
       " ('coconut', 6),\n",
       " ('ick', 6),\n",
       " ('liquid', 6),\n",
       " ('baby', 6),\n",
       " ('gel', 6),\n",
       " ('caps', 6),\n",
       " ('pleasant', 6),\n",
       " ('blue', 6),\n",
       " ('say', 6),\n",
       " ('body', 6),\n",
       " ('both', 6),\n",
       " ('sand', 6),\n",
       " ('exactly', 6),\n",
       " ('mixes', 6),\n",
       " ('doesnt', 6),\n",
       " ('due', 6),\n",
       " ('response', 6),\n",
       " ('literal', 6),\n",
       " ('years', 6),\n",
       " ('noticed', 6),\n",
       " ('mango', 6),\n",
       " ('daily', 6),\n",
       " ('pains', 6),\n",
       " ('far', 6),\n",
       " ('bars', 6),\n",
       " ('cut', 6),\n",
       " ('rate', 6),\n",
       " ('pecans', 6),\n",
       " ('supplements', 6),\n",
       " ('think', 6),\n",
       " ('shakes', 6),\n",
       " ('morning', 6),\n",
       " ('sickness', 6),\n",
       " ('threw', 6),\n",
       " ('possible', 6),\n",
       " ('moldy', 6),\n",
       " ('mixed', 6),\n",
       " ('major', 6),\n",
       " ('breakouts', 6),\n",
       " ('hurt', 6),\n",
       " ('into', 6),\n",
       " ('concerned', 6),\n",
       " ('envelope', 6),\n",
       " ('opinion', 6),\n",
       " ('contaminated', 6),\n",
       " ('tasteless', 6),\n",
       " ('spend', 6),\n",
       " ('constipated', 6),\n",
       " ('label', 6),\n",
       " ('packing', 6),\n",
       " ('nauseating', 6),\n",
       " ('biotin', 6),\n",
       " ('problem', 6),\n",
       " ('wash', 6),\n",
       " ('tampering', 6),\n",
       " ('levels', 6),\n",
       " ('multi', 6),\n",
       " ('painful', 6),\n",
       " ('poisoning', 6),\n",
       " ('comparable', 6),\n",
       " ('gain', 6),\n",
       " ('counterfeit', 6),\n",
       " ('then', 6),\n",
       " ('near', 6),\n",
       " ('gag', 6),\n",
       " ('gold', 6),\n",
       " ('vegetarian', 6),\n",
       " ('ships', 6),\n",
       " ('unbearably', 6),\n",
       " ('unflavored', 6),\n",
       " ('blends', 6),\n",
       " ('unexplained', 6),\n",
       " ('multiple', 5),\n",
       " ('lots', 5),\n",
       " ('entire', 5),\n",
       " ('until', 5),\n",
       " ('hopes', 5),\n",
       " ('packages', 5),\n",
       " ('fragrance', 5),\n",
       " ('digestion', 5),\n",
       " ('maple', 5),\n",
       " ('impossible', 5),\n",
       " ('indigestion', 5),\n",
       " ('boost', 5),\n",
       " ('dosage', 5),\n",
       " ('throw', 5),\n",
       " ('trip', 5),\n",
       " ('cans', 5),\n",
       " ('pick', 5),\n",
       " ('china', 5),\n",
       " ('grape', 5),\n",
       " ('diaper', 5),\n",
       " ('tuna', 5),\n",
       " ('row', 5),\n",
       " ('muscle', 5),\n",
       " ('changes', 5),\n",
       " ('cup', 5),\n",
       " ('unusable', 5),\n",
       " ('recieved', 5),\n",
       " ('salted', 5),\n",
       " ('digestive', 5),\n",
       " ('matters', 5),\n",
       " ('update', 5),\n",
       " ('satisfied', 5),\n",
       " ('son', 5),\n",
       " ('honest', 5),\n",
       " ('nails', 5),\n",
       " ('looking', 5),\n",
       " ('runs', 5),\n",
       " ('peach', 5),\n",
       " ('bloated', 5),\n",
       " ('created', 5),\n",
       " ('times', 5),\n",
       " ('said', 5),\n",
       " ('french', 5),\n",
       " ('turns', 5),\n",
       " ('labeling', 5),\n",
       " ('honey', 5),\n",
       " ('drying', 5),\n",
       " ('stomachache', 5),\n",
       " ('omega', 5),\n",
       " ('itching', 5),\n",
       " ('while', 5),\n",
       " ('acidic', 5),\n",
       " ('acceptable', 5),\n",
       " ('apart', 5),\n",
       " ('pregnancy', 5),\n",
       " ('blech', 5),\n",
       " ('amount', 5),\n",
       " ('significant', 5),\n",
       " ('having', 5),\n",
       " ('under', 5),\n",
       " ('chewable', 5),\n",
       " ('hope', 5),\n",
       " ('march', 5),\n",
       " ('trying', 5),\n",
       " ('policy', 5),\n",
       " ('oyster', 5),\n",
       " ('shellfish', 5),\n",
       " ('ship', 5),\n",
       " ('returnable', 5),\n",
       " ('amounts', 5),\n",
       " ('tons', 5),\n",
       " ('excessive', 5),\n",
       " ('going', 5),\n",
       " ('smaller', 5),\n",
       " ('rough', 5),\n",
       " ('mild', 5),\n",
       " ('blended', 5),\n",
       " ('truly', 5),\n",
       " ('egg', 5),\n",
       " ('already', 5),\n",
       " ('nature', 5),\n",
       " ('powdery', 5),\n",
       " ('magnesium', 5),\n",
       " ('trimester', 5),\n",
       " ('sometimes', 5),\n",
       " ('lacks', 5),\n",
       " ('black', 5),\n",
       " ('brain', 5),\n",
       " ('swallowing', 5),\n",
       " ('cant', 5),\n",
       " ('medicine', 5),\n",
       " ('separates', 5),\n",
       " ('costs', 5),\n",
       " ('wonderful', 5),\n",
       " ('anymore', 5),\n",
       " ('canister', 5),\n",
       " ('smelly', 5),\n",
       " ('blob', 5),\n",
       " ('coating', 5),\n",
       " ('through', 5),\n",
       " ('pissed', 5),\n",
       " ('gluten', 5),\n",
       " ('mask', 5),\n",
       " ('general', 5),\n",
       " ('gummie', 5),\n",
       " ('knockoff', 5),\n",
       " ('dairy', 5),\n",
       " ('questionable', 5),\n",
       " ('harsher', 5),\n",
       " ('taffy', 5),\n",
       " ('edible', 5),\n",
       " ('heavy', 5),\n",
       " ('pieces', 5),\n",
       " ('ammonia', 4),\n",
       " ('calories', 4),\n",
       " ('ibs', 4),\n",
       " ('another', 4),\n",
       " ('second', 4),\n",
       " ('clump', 4),\n",
       " ('urine', 4),\n",
       " ('bloating', 4),\n",
       " ('unscented', 4),\n",
       " ('mean', 4),\n",
       " ('promised', 4),\n",
       " ('maltodextrin', 4),\n",
       " ('heat', 4),\n",
       " ('stung', 4),\n",
       " ('quickly', 4),\n",
       " ('vendor', 4),\n",
       " ('natures', 4),\n",
       " ('bounty', 4),\n",
       " ('optimum', 4),\n",
       " ('infused', 4),\n",
       " ('blah', 4),\n",
       " ('shattered', 4),\n",
       " ('managing', 4),\n",
       " ('flavour', 4),\n",
       " ('calcium', 4),\n",
       " ('vivid', 4),\n",
       " ('according', 4),\n",
       " ('quick', 4),\n",
       " ('deals', 4),\n",
       " ('nail', 4),\n",
       " ('selling', 4),\n",
       " ('highly', 4),\n",
       " ('important', 4),\n",
       " ('site', 4),\n",
       " ('feeling', 4),\n",
       " ('described', 4),\n",
       " ('nose', 4),\n",
       " ('caramel', 4),\n",
       " ('solimo', 4),\n",
       " ('future', 4),\n",
       " ('reason', 4),\n",
       " ('daughter', 4),\n",
       " ('storage', 4),\n",
       " ('fully', 4),\n",
       " ('options', 4),\n",
       " ('cheaper', 4),\n",
       " ('directly', 4),\n",
       " ('settles', 4),\n",
       " ('tiny', 4),\n",
       " ('properly', 4),\n",
       " ('aggressive', 4),\n",
       " ('leaking', 4),\n",
       " ('shit', 4),\n",
       " ('lol', 4),\n",
       " ('burning', 4),\n",
       " ('power', 4),\n",
       " ('own', 4),\n",
       " ('anxiety', 4),\n",
       " ('stolen', 4),\n",
       " ('must', 4),\n",
       " ('kept', 4),\n",
       " ('praline', 4),\n",
       " ('content', 4),\n",
       " ('ascorbic', 4),\n",
       " ('fall', 4),\n",
       " ('hypoallergenic', 4),\n",
       " ('picture', 4),\n",
       " ('replace', 4),\n",
       " ('kinda', 4),\n",
       " ('sell', 4),\n",
       " ('clay', 4),\n",
       " ('batter', 4),\n",
       " ('uses', 4),\n",
       " ('exp', 4),\n",
       " ('inaccurate', 4),\n",
       " ('thanks', 4),\n",
       " ('reactions', 4),\n",
       " ('overrated', 4),\n",
       " ('heart', 4),\n",
       " ('tho', 4),\n",
       " ('lacking', 4),\n",
       " ('tart', 4),\n",
       " ('supposed', 4),\n",
       " ('massive', 4),\n",
       " ('irritated', 4),\n",
       " ('changing', 4),\n",
       " ('bait', 4),\n",
       " ('part', 4),\n",
       " ('live', 4),\n",
       " ('tangerine', 4),\n",
       " ('strength', 4),\n",
       " ('naturally', 4),\n",
       " ('funny', 4),\n",
       " ('says', 4),\n",
       " ('tear', 4),\n",
       " ('stop', 4),\n",
       " ('spray', 4),\n",
       " ('concept', 4),\n",
       " ('sad', 4),\n",
       " ('brown', 4),\n",
       " ('seen', 4),\n",
       " ('pink', 4),\n",
       " ('elderberry', 4),\n",
       " ('recent', 4),\n",
       " ('horrific', 4),\n",
       " ('monster', 4),\n",
       " ('unfortunately', 4),\n",
       " ('initial', 4),\n",
       " ('extreme', 4),\n",
       " ('unlike', 4),\n",
       " ('reviewers', 4),\n",
       " ('treat', 4),\n",
       " ('almonds', 4),\n",
       " ('air', 4),\n",
       " ('ones', 4),\n",
       " ('women', 4),\n",
       " ('term', 4),\n",
       " ('upsets', 4),\n",
       " ('sweetened', 4),\n",
       " ('usps', 4),\n",
       " ('carbs', 4),\n",
       " ('toxicity', 4),\n",
       " ('cholesterol', 4),\n",
       " ('type', 4),\n",
       " ('grainy', 4),\n",
       " ('lemonade', 4),\n",
       " ('facial', 4),\n",
       " ('jar', 4),\n",
       " ('guys', 4),\n",
       " ('play', 4),\n",
       " ('few', 4),\n",
       " ('blood', 4),\n",
       " ('unhealthy', 4),\n",
       " ('happened', 4),\n",
       " ('shake', 4),\n",
       " ('case', 4),\n",
       " ('natal', 4),\n",
       " ('foods', 4),\n",
       " ('failure', 4),\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_dict.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4513,), (8512, 3))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].unique().shape, df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>useless</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trash</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not</td>\n",
       "      <td>2041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>buy</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>these</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  freq\n",
       "0  useless    25\n",
       "1    trash    15\n",
       "2      not  2041\n",
       "3      buy   255\n",
       "4    these   142"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = pd.DataFrame.from_dict(word_dict, orient='index')\n",
    "\n",
    "word_freq = word_freq.reset_index()\n",
    "word_freq.columns = ['word','freq']\n",
    "word_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not</td>\n",
       "      <td>2041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>taste</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>the</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>and</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>bad</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>for</td>\n",
       "      <td>547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>product</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>good</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>but</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>flavor</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>did</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>this</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>like</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>work</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>buy</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>too</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>very</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>does</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>received</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>horrible</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>broken</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>smell</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tastes</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>terrible</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>wrong</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>texture</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>was</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>with</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>you</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>never</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>silica</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>packets</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>tends</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>basement</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>incorrecto</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>yesterday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>meds</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>producto</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>world</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>supplier</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>faster</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>grew</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>loaded</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>print</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>pricing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>idead</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>moth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>balls</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>integrity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>stretch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2079</th>\n",
       "      <td>marks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>hmm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>consistent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>atleast</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>hazardous</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>unsettling</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>stanky</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>mejorar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>unscrupulous</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>paste</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2831 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  freq\n",
       "2              not  2041\n",
       "49           taste   759\n",
       "19             the   740\n",
       "30             and   665\n",
       "41             bad   610\n",
       "33             for   547\n",
       "37         product   486\n",
       "94            good   469\n",
       "113            but   399\n",
       "246         flavor   363\n",
       "96             did   308\n",
       "71            this   282\n",
       "23            like   265\n",
       "58            work   257\n",
       "3              buy   255\n",
       "27             too   245\n",
       "235           very   231\n",
       "91            does   209\n",
       "101       received   200\n",
       "14        horrible   199\n",
       "79          broken   187\n",
       "44           smell   187\n",
       "13          tastes   187\n",
       "118       terrible   183\n",
       "62           wrong   174\n",
       "372        texture   172\n",
       "140            was   167\n",
       "175           with   165\n",
       "290            you   161\n",
       "69           never   159\n",
       "...            ...   ...\n",
       "2057        silica     1\n",
       "2058       packets     1\n",
       "2059         tends     1\n",
       "1208      basement     1\n",
       "1205    incorrecto     1\n",
       "2062     yesterday     1\n",
       "2063          meds     1\n",
       "1204      producto     1\n",
       "1200         world     1\n",
       "1199      supplier     1\n",
       "1195        faster     1\n",
       "1194          grew     1\n",
       "2071        loaded     1\n",
       "2072         print     1\n",
       "1186       pricing     1\n",
       "2074         idead     1\n",
       "2075          moth     1\n",
       "2076         balls     1\n",
       "2077     integrity     1\n",
       "2078       stretch     1\n",
       "2079         marks     1\n",
       "2080           hmm     1\n",
       "1185    consistent     1\n",
       "1176       atleast     1\n",
       "1174     hazardous     1\n",
       "1173    unsettling     1\n",
       "1172        stanky     1\n",
       "2089       mejorar     1\n",
       "1169  unscrupulous     1\n",
       "2830         paste     1\n",
       "\n",
       "[2831 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "\n",
    "def tfidf_feature(train, test, col_name, min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=3, stopwords='english', \n",
    "                  n_component=120, decom_flag=False, which_method='svd', \n",
    "                  max_features=None, feat_col_name='svd'):\n",
    "    \"\"\"return tfidf feature\n",
    "    Args:\n",
    "        train, test: dataframe\n",
    "        col_name: column name of text feature\n",
    "        min_df: if Int, then it represent count of the minimum words in corpus (remove very rare word)\n",
    "        analyzer: [‘word’, ‘char’]\n",
    "        ngram: max range of ngram\n",
    "        token_pattern: [using: r'\\w{1,}'] [by default: '(?u)\\b\\w\\w+\\b']\n",
    "        stopwords: ['english' or customized by remove specific words]\n",
    "        n_component: n_component of svd feature transform\n",
    "        decom_flag: Wheteher to run svd/nmf on top of that or not (by default: False)\n",
    "        which_method: which to run [svd or nmf] on top of tfidf (by default: False)\n",
    "        max_features: max no of features to keep, based on frequency. It will keep words with higher freq\n",
    "    return:\n",
    "        Transformed feature space of the text data, as well as tfidf function instance\n",
    "        if svd_flag== True : train_tf, test_tf, tfv, svd\n",
    "        else : train_tf, test_tf, tfv\n",
    "    example:\n",
    "        train_tfv, test_tfv, tfv = tfidf_feature(X_train, X_test, ['text'], min_df=3)\n",
    "        train_svd, test_svd, complete_tfv, tfv, svd = tfidf_feature(X_train, X_test, ['text'], \n",
    "            min_df=3, svd_component=3, svd_flag=True)\n",
    "\n",
    "    \"\"\"\n",
    "    tfv = TfidfVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, max_df=1.0, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                stop_words = stopwords)\n",
    "\n",
    "    complete_df = pd.concat([train[col_name], test[col_name]], axis=0)\n",
    "#         return complete_df\n",
    "#         print(complete_df.shape, complete_df.columns)\n",
    "\n",
    "    tfv.fit(list(complete_df[:].values))\n",
    "\n",
    "    if decom_flag is False:\n",
    "        train_tfv =  tfv.transform(train[col_name].values.ravel()) \n",
    "        test_tfv  = tfv.transform(test[col_name].values.ravel())\n",
    "\n",
    "        del complete_df\n",
    "        gc.collect()\n",
    "        return train_tfv, test_tfv, tfv\n",
    "    else:\n",
    "        complete_tfv = tfv.transform(complete_df[:].values.ravel())\n",
    "        \n",
    "        if which_method is 'svd':\n",
    "            svd = TruncatedSVD(n_components=n_component)\n",
    "            svd.fit(complete_tfv)\n",
    "            complete_dec = svd.transform(complete_tfv)\n",
    "        else:\n",
    "            nmf = NMF(n_components=n_component, random_state=1234, alpha=0, l1_ratio=0)\n",
    "            nmf.fit(complete_tfv)            \n",
    "            complete_dec = nmf.fit_transform(complete_tfv)            \n",
    "        \n",
    "        \n",
    "        complete_dec = pd.DataFrame(data=complete_dec)\n",
    "        complete_dec.columns = [feat_col_name+'_'+str(i) for i in range(n_component)]\n",
    "\n",
    "        train_dec = complete_dec.iloc[:train.shape[0]]\n",
    "        test_dec = complete_dec.iloc[train.shape[0]:].reset_index(drop=True)\n",
    "\n",
    "        del complete_dec, complete_df\n",
    "        gc.collect()\n",
    "        print(\"=\"*15, \" done \", \"=\"*15)\n",
    "        return train_dec, test_dec, complete_tfv, tfv\n",
    "\n",
    "def countvect_feature(train, test, col_name, min_df=3, \n",
    "                      analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                      ngram=3, stopwords='english', max_features=None):\n",
    "    \"\"\"return CountVectorizer feature\n",
    "    Args:\n",
    "        train, test: dataset\n",
    "        col_name: columns name of the text feature\n",
    "        min_df: if Int, then it represent count of the minimum words in corpus (remove very rare word)\n",
    "        analyzer: [‘word’, ‘char’]\n",
    "        ngram: max range of ngram\n",
    "        token_pattern: [using: r'\\w{1,}'] [by default: '(?u)\\b\\w\\w+\\b']\n",
    "        stopwords: ['english' or customized by remove specific words]\n",
    "        max_features: max no of features to keep, based on frequency. It will keep words with higher freq\n",
    "    return:\n",
    "        Count feature space of the text data, as well as its function instance\n",
    "    \"\"\"\n",
    "    ctv = CountVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                stop_words = stopwords)\n",
    "\n",
    "    complete_df = pd.concat([train[col_name], test[col_name]], axis=0)\n",
    "    ctv.fit(list(complete_df[:].values))\n",
    "\n",
    "    train_tf =  ctv.transform(train[col_name].values.ravel()) \n",
    "    test_tf  = ctv.transform(test[col_name].values.ravel())\n",
    "\n",
    "    del complete_df\n",
    "    gc.collect()\n",
    "    return train_tf, test_tf, ctv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_vectorizer(df, col_name, min_df=3, analyzer='word', stopwords='english', \n",
    "                     token_pattern=r'\\w{1,}', ngram=3, max_features=None):\n",
    "    ctv = CountVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                stop_words = stopwords)\n",
    "\n",
    "    ctv.fit(list(df[col_name].values))\n",
    "\n",
    "    df_new =  ctv.transform(df[col_name].values.ravel()) \n",
    "    return df_new\n",
    "\n",
    "\n",
    "def get_tfidf_feature(df, col_name, min_df=3, analyzer='word', stopwords='english',\n",
    "                  token_pattern=r'\\w{1,}', ngram=3, max_features=None):\n",
    "\n",
    "    tfv = TfidfVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, max_df=1.0, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                stop_words = stopwords)\n",
    "\n",
    "    tfv.fit(list(df[col_name].values))\n",
    "    df_new =  tfv.transform(df[col_name].values.ravel()) \n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8512, 14131)\n",
      "(8512, 6430)\n",
      "(8512, 2958)\n",
      "(8512, 1650)\n",
      "(8512, 1155)\n"
     ]
    }
   ],
   "source": [
    "count_vect = []\n",
    "for ngram in [1,2,3,4,5]:\n",
    "    cvect = get_count_vectorizer(df, 'title', ngram)\n",
    "    print(cvect.shape)\n",
    "    count_vect.append(cvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8512, 1183)\n",
      "(8512, 2369)\n",
      "(8512, 2958)\n",
      "(8512, 3240)\n",
      "(8512, 3382)\n"
     ]
    }
   ],
   "source": [
    "tfidf_store = []\n",
    "for ngram in [1,2,3,4,5]:\n",
    "    tfidf = get_tfidf_feature(df, 'title', ngram=ngram)\n",
    "    print(tfidf.shape)\n",
    "    tfidf_store.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_len = train.shape[0]\n",
    "ts_len = test.shape[0]\n",
    "# train.shape[0] + test.shape[0], df.shape[0]\n",
    "train1 = df.iloc[:tr_len]\n",
    "test1  = df.iloc[tr_len:]\n",
    "train1.shape, test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = train['topic'].astype('category').cat.codes\n",
    "train['target'] = train['target'].astype('int')\n",
    "\n",
    "def get_mapping(df, col_name):\n",
    "    cat_codes = df[col_name].astype('category')\n",
    "    \n",
    "    class_mapping = {}\n",
    "    i = 0\n",
    "    for col in cat_codes.cat.categories:\n",
    "        class_mapping[col] = i\n",
    "        i += 1\n",
    "    \n",
    "    class_mapping_reverse = {}\n",
    "    for key, value in class_mapping.items():\n",
    "        class_mapping_reverse[value] = key\n",
    "\n",
    "    return class_mapping, class_mapping_reverse\n",
    "\n",
    "cl_map, cl_map_inv = get_mapping(train, 'topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf 0 0.387248322147651\n",
      "tfidf 1 0.3731543624161074\n",
      "tfidf 2 0.37919463087248323\n",
      "tfidf 3 0.3597315436241611\n",
      "tfidf 4 0.3818791946308725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split    \n",
    "\n",
    "for i, data in enumerate(tfidf_store):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = MultinomialNB().fit(X_train, Y_train)\n",
    "    print(\"tfidf \"+str(i), clf.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count-vect 0 0.37651006711409396\n",
      "count-vect 1 0.34563758389261745\n",
      "count-vect 2 0.4\n",
      "count-vect 3 0.4167785234899329\n",
      "count-vect 4 0.39798657718120806\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split    \n",
    "\n",
    "for i, data in enumerate(count_vect):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = MultinomialNB().fit(X_train, Y_train)\n",
    "    print(\"count-vect \"+str(i), clf.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5948, 4), (5959, 4), (1776, 2), (2553, 2))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.drop_duplicates().shape, train.shape, test.drop_duplicates().shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(logistic_reg.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-reg 0 0.3697986577181208\n",
      "log-reg 1 0.387248322147651\n",
      "log-reg 2 0.3711409395973154\n",
      "log-reg 3 0.361744966442953\n",
      "log-reg 4 0.3718120805369127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split    \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "    C=0.01, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "    random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "\n",
    "for i, data in enumerate(count_vect):\n",
    "    train_ = data[:train.shape[0]]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        train_, train['target'], \n",
    "        stratify=train['target'], \n",
    "        test_size=0.25\n",
    "    )\n",
    "    clf = logistic_reg.fit(X_train, Y_train)\n",
    "    print(\"log-reg \"+str(i), clf.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "alpha:  0.01\n",
      "log-reg 0 0.32348993288590605\n",
      "log-reg 1 0.2979865771812081\n",
      "log-reg 2 0.2771812080536913\n",
      "log-reg 3 0.2912751677852349\n",
      "log-reg 4 0.29261744966442954\n",
      "=========================\n",
      "alpha:  0.05\n",
      "log-reg 0 0.4080536912751678\n",
      "log-reg 1 0.4053691275167785\n",
      "log-reg 2 0.3859060402684564\n",
      "log-reg 3 0.40268456375838924\n",
      "log-reg 4 0.39731543624161075\n",
      "=========================\n",
      "alpha:  0.1\n",
      "log-reg 0 0.38993288590604025\n",
      "log-reg 1 0.3939597315436242\n",
      "log-reg 2 0.3932885906040268\n",
      "log-reg 3 0.40939597315436244\n",
      "log-reg 4 0.4120805369127517\n",
      "=========================\n",
      "alpha:  0.5\n",
      "log-reg 0 0.38456375838926177\n",
      "log-reg 1 0.38926174496644295\n",
      "log-reg 2 0.4087248322147651\n",
      "log-reg 3 0.4040268456375839\n",
      "log-reg 4 0.39731543624161075\n",
      "=========================\n",
      "alpha:  1\n",
      "log-reg 0 0.4033557046979866\n",
      "log-reg 1 0.38456375838926177\n",
      "log-reg 2 0.3838926174496644\n",
      "log-reg 3 0.3912751677852349\n",
      "log-reg 4 0.38523489932885907\n",
      "=========================\n",
      "alpha:  2\n",
      "log-reg 0 0.3959731543624161\n",
      "log-reg 1 0.36644295302013424\n",
      "log-reg 2 0.3812080536912752\n",
      "log-reg 3 0.3838926174496644\n",
      "log-reg 4 0.39865771812080536\n",
      "=========================\n",
      "alpha:  5\n",
      "log-reg 0 0.3651006711409396\n",
      "log-reg 1 0.36577181208053694\n",
      "log-reg 2 0.36308724832214767\n",
      "log-reg 3 0.33691275167785234\n",
      "log-reg 4 0.35503355704697986\n",
      "=========================\n",
      "alpha:  10\n",
      "log-reg 0 0.34429530201342284\n",
      "log-reg 1 0.3308724832214765\n",
      "log-reg 2 0.33288590604026846\n",
      "log-reg 3 0.33691275167785234\n",
      "log-reg 4 0.3496644295302013\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split    \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "    C=0.1, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "    random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "\n",
    "for alpha in [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10]:\n",
    "    logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "        C=alpha, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "        random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "    print(\"=\"*25)\n",
    "    print(\"alpha: \", alpha)\n",
    "    for i, data in enumerate(tfidf_store):\n",
    "        train_ = data[:train.shape[0]]\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            train_, train['target'], \n",
    "            stratify=train['target'], \n",
    "            test_size=0.25\n",
    "        )\n",
    "        clf = logistic_reg.fit(X_train, Y_train)\n",
    "        print(\"log-reg \"+str(i), clf.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_accuracy(truths, preds, n):\n",
    "    best_n = np.argsort(preds, axis=1)[:,-n:]\n",
    "#     ts = np.argmax(truths, axis=1)\n",
    "    ts = truths\n",
    "    success = 0\n",
    "    for t, p in zip(ts, best_n):\n",
    "        if t in p:\n",
    "            success += 1\n",
    "    return float(success)/preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, gc\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# sklearn.metrics.accuracy_score(y_true, y_pred\n",
    "\n",
    "def train_lgb_model(X_train, y_train, X_valid, y_valid, features, param, num_round):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X_train, X_valid: training and valid data\n",
    "        y_train, y_valid: training and valid target\n",
    "        X_test: test-data\n",
    "        features: training features\n",
    "    Return:\n",
    "        oof-pred, test_preds model, model_imp\n",
    "    \"\"\"\n",
    "    _train = lgb.Dataset(X_train[features], label=y_train, feature_name=list(features))\n",
    "    _valid = lgb.Dataset(X_valid[features], label=y_valid,feature_name=list(features))\n",
    "    \n",
    "    clf = lgb.train(param, _train, num_round, \n",
    "                    valid_sets = [_train, _valid], \n",
    "                    verbose_eval=200, \n",
    "                    early_stopping_rounds = 25)                  \n",
    "    \n",
    "    oof = clf.predict(X_valid[features], num_iteration=clf.best_iteration)\n",
    "#     test_pred = clf.predict(X_test[features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    lgb_imp = pd.DataFrame(data=[clf.feature_name(), list(clf.feature_importance())]).T\n",
    "    lgb_imp.columns = ['feature','imp']\n",
    "    \n",
    "    return oof, clf, lgb_imp\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def run_cv_lgb(train_df, target, leaves=30):\n",
    "\n",
    "    param = {\n",
    "        'bagging_freq'           : 5,\n",
    "        'bagging_fraction'       : 0.8,\n",
    "        'boost_from_average'     : 'false',\n",
    "        'boost'                  : 'gbdt',\n",
    "        'feature_fraction'       : 0.8,\n",
    "        'learning_rate'          : 0.01,\n",
    "        'max_depth'              : -1,\n",
    "        'metric'                 : 'auc',\n",
    "#         'min_data_in_leaf'       : 100,\n",
    "#         'min_sum_hessian_in_leaf': 10.0,\n",
    "        'num_leaves'             : leaves,\n",
    "        'num_threads'            : 4,\n",
    "        'tree_learner'           : 'serial',\n",
    "#         'objective'              : 'binary',\n",
    "        'verbosity'              : 1,\n",
    "    #     'lambda_l1'              : 0.001,\n",
    "        'lambda_l2'              : 0.1,\n",
    "        'objective'              : 'multiclassova',\n",
    "        'is_unbalance'           : True,\n",
    "        'num_class'              : 21,\n",
    "    }   \n",
    "    if leaves is not None:\n",
    "        param['num_leaves'] = leaves\n",
    "        print(\"using leaves: \", param['num_leaves'])\n",
    "\n",
    "    random_seed = 1234\n",
    "    n_splits = 4\n",
    "    num_round = 10000\n",
    "    feature_imp = pd.DataFrame()\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "    oof_lgb = np.zeros((len(train_df), 21))\n",
    "#     predictions = np.zeros((len(test_df),n_splits))\n",
    "\n",
    "    clfs = []\n",
    "    \n",
    "    for fold_, (train_index, valid_index) in enumerate(folds.split(train_df, target)):\n",
    "        print(train_index.shape, valid_index.shape)\n",
    "        print(\"Fold {}\".format(fold_))\n",
    "    \n",
    "        y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n",
    "        X_train, X_valid = train_df.iloc[train_index,:], train_df.iloc[valid_index,:]\n",
    "        features = X_train.columns\n",
    "        \n",
    "\n",
    "        num_round = 10000\n",
    "        oof, clf, lgb_imp = train_lgb_model(X_train, y_train, \n",
    "                                            X_valid, y_valid, \n",
    "                                            features, param, \n",
    "                                            num_round)\n",
    "        lgb_imp['fold'] = fold_\n",
    "        feature_imp = pd.concat([feature_imp, lgb_imp], axis=0)\n",
    "    \n",
    "        oof_lgb[valid_index] = oof\n",
    "#         predictions[:,fold_] = test_pred\n",
    "        clfs.append(clf)\n",
    "        \n",
    "        score = accuracy_score(y_valid, np.argmax(oof, axis=1))\n",
    "#         print(classification_report(y_valid, np.argmax(oof, axis=1)))\n",
    "#         score = roc_auc_score(y_valid, oof)\n",
    "        print( \"  auc = \", score )\n",
    "        print(top_n_accuracy(y_valid, oof, 3))\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "#         break\n",
    "#     return y_valid, oof\n",
    "        \n",
    "    feature_imp.imp = feature_imp.imp.astype('float')\n",
    "    feature_imp = feature_imp.groupby(['feature'])['imp'].mean()\n",
    "    feature_imp = pd.DataFrame(data=[feature_imp.index, feature_imp.values]).T\n",
    "    feature_imp.columns=['feature','imp']\n",
    "    feature_imp = feature_imp.sort_values(by='imp')\n",
    "\n",
    "    return clfs, feature_imp, oof_lgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using leaves:  10000\n",
      "(4463,) (1496,)\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.405608\tvalid_1's auc: 0.417911\n",
      "  auc =  0.16377005347593582\n",
      "0.3235294117647059\n",
      "============================================================\n",
      "(4465,) (1494,)\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.411251\tvalid_1's auc: 0.405284\n",
      "  auc =  0.19143239625167335\n",
      "0.321954484605087\n",
      "============================================================\n",
      "(4470,) (1489,)\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.401348\tvalid_1's auc: 0.436842\n",
      "  auc =  0.1605104096709201\n",
      "0.2807253190060443\n",
      "============================================================\n",
      "(4479,) (1480,)\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\ttraining's auc: 0.395933\tvalid_1's auc: 0.392722\n",
      "  auc =  0.17094594594594595\n",
      "0.2918918918918919\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# _, _, _ = run_cv_lgb(train_sparse_matrix, train['target'], leaves=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using leaves:  10000\n",
      "(4463,) (1496,)\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.408856\tvalid_1's auc: 0.428366\n",
      "  auc =  0.30213903743315507\n",
      "0.5006684491978609\n",
      "============================================================\n",
      "(4465,) (1494,)\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.407776\tvalid_1's auc: 0.406841\n",
      "  auc =  0.30522088353413657\n",
      "0.536144578313253\n",
      "============================================================\n",
      "(4470,) (1489,)\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.413765\tvalid_1's auc: 0.449881\n",
      "  auc =  0.29012760241773\n",
      "0.5023505708529215\n",
      "============================================================\n",
      "(4479,) (1480,)\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.385803\tvalid_1's auc: 0.392989\n",
      "  auc =  0.29864864864864865\n",
      "0.5236486486486487\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "_, _, _ = run_cv_lgb(train_sparse_matrix, train['target'], leaves=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using leaves:  10000\n",
      "(4463,) (1496,)\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.406346\tvalid_1's auc: 0.42111\n",
      "  auc =  0.3054812834224599\n",
      "0.517379679144385\n",
      "============================================================\n",
      "(4465,) (1494,)\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.413985\tvalid_1's auc: 0.415912\n",
      "  auc =  0.3286479250334672\n",
      "0.5401606425702812\n",
      "============================================================\n",
      "(4470,) (1489,)\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.419028\tvalid_1's auc: 0.448459\n",
      "  auc =  0.2995298858294157\n",
      "0.5251846877098724\n",
      "============================================================\n",
      "(4479,) (1480,)\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.399299\tvalid_1's auc: 0.391379\n",
      "  auc =  0.30743243243243246\n",
      "0.5324324324324324\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "_, f, _ = run_cv_lgb(train_sparse_matrix, train['target'], leaves=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f70fa4fed68>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot('imp', data=f.sort_values('imp', ascending=False)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-reg  0.4006711409395973\n"
     ]
    }
   ],
   "source": [
    "train_ = count_vect[-2][:train.shape[0]]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    train_, train['target'], \n",
    "    stratify=train['target'], \n",
    "    test_size=0.25\n",
    ")\n",
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "    C=0.2, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "    random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "\n",
    "clf = logistic_reg.fit(X_train, Y_train)\n",
    "print(\"log-reg \", clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# model = LogisticRegression(solver='sag')\n",
    "sfm = SelectFromModel(logistic_reg, threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse_matrix = sfm.fit_transform(train_, target)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5959, 1486), (5959, 1650))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sparse_matrix.shape, train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse_matrix = train_sparse_matrix.todense()\n",
    "train_sparse_matrix = pd.DataFrame(train_sparse_matrix)\n",
    "train_sparse_matrix.columns = ['col'+str(i) for i in range(train_sparse_matrix.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5959, 3), (2553, 3))"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ = df[:train.shape[0]]\n",
    "test_  = df[train.shape[0]:]\n",
    "train_.shape, test_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============  done  ===============\n",
      "===============  done  ===============\n",
      "===============  done  ===============\n",
      "===============  done  ===============\n",
      "===============  done  ===============\n",
      "===============  done  ===============\n"
     ]
    }
   ],
   "source": [
    "kuch_title1 = tfidf_feature(train, test, 'title', min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=2, stopwords='english', \n",
    "                  n_component=75, decom_flag=True, which_method='svd', \n",
    "                  max_features=None, feat_col_name='svd_title1')\n",
    "\n",
    "kuch_title2 = tfidf_feature(train, test, 'title', min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=3, stopwords='english', \n",
    "                  n_component=75, decom_flag=True, which_method='svd', \n",
    "                  max_features=None, feat_col_name='svd_title2')\n",
    "\n",
    "kuch_text11 = tfidf_feature(train, test, 'text', min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=2, stopwords='english', \n",
    "                  n_component=100, decom_flag=True, which_method='svd', \n",
    "                  max_features=None, feat_col_name='svd_text11')\n",
    "\n",
    "kuch_text21 = tfidf_feature(train, test, 'text', min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=3, stopwords='english', \n",
    "                  n_component=100, decom_flag=True, which_method='svd', \n",
    "                  max_features=None, feat_col_name='svd_text21')\n",
    "\n",
    "\n",
    "kuch_text12 = tfidf_feature(train, test, 'text', min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=2, stopwords='english', \n",
    "                  n_component=50, decom_flag=True, which_method='nmf', \n",
    "                  max_features=None, feat_col_name='svd_text12')\n",
    "\n",
    "kuch_text22 = tfidf_feature(train, test, 'text', min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=3, stopwords='english', \n",
    "                  n_component=50, decom_flag=True, which_method='nmf', \n",
    "                  max_features=None, feat_col_name='svd_text22')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5959, 450), (2553, 450))"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "train_all = pd.concat([kuch_title1[0], kuch_title2[0], kuch_text11[0],\n",
    "                       kuch_text12[0], kuch_text21[0], kuch_text22[0]], axis=1)\n",
    "test_all  = pd.concat([kuch_title1[1], kuch_title2[1], kuch_text11[1],\n",
    "                       kuch_text12[1], kuch_text21[1], kuch_text22[1]], axis=1)\n",
    "\n",
    "train_all.shape, test_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using leaves:  70\n",
      "(4463,) (1496,)\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.147056\tvalid_1's auc: 0.236048\n",
      "  auc =  0.4391711229946524\n",
      "0.7179144385026738\n",
      "============================================================\n",
      "(4465,) (1494,)\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.171365\tvalid_1's auc: 0.218049\n",
      "  auc =  0.4243641231593039\n",
      "0.715528781793842\n",
      "============================================================\n",
      "(4470,) (1489,)\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.175862\tvalid_1's auc: 0.247368\n",
      "  auc =  0.40899932840832776\n",
      "0.6991269308260577\n",
      "============================================================\n",
      "(4479,) (1480,)\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.169342\tvalid_1's auc: 0.221444\n",
      "  auc =  0.4195945945945946\n",
      "0.6925675675675675\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "_, f, _ = run_cv_lgb(train_all, train['target'], leaves=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>svd_title1_0</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>svd_title1_1</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>svd_text11_12</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>svd_text12_39</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>svd_text12_41</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>svd_text12_46</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>svd_text12_28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>svd_text12_38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>svd_title1_5</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>svd_title1_2</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>svd_text22_25</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svd_text11_25</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>svd_text11_9</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>svd_text22_47</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>svd_title1_26</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>svd_text21_97</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>svd_title1_12</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>svd_title2_3</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>svd_text11_2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>svd_text12_17</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>svd_title1_13</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>svd_title1_11</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>svd_text22_7</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>svd_title1_69</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>svd_text12_18</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>svd_title2_0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>svd_text11_47</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>svd_title2_13</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>svd_text22_41</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>svd_title2_38</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>svd_text21_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>svd_title1_71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>svd_text11_64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>svd_text22_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>svd_title2_4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>svd_text12_35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>svd_text11_22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>svd_title1_61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>svd_text11_41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>svd_text11_1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>svd_text12_14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>svd_text21_72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>svd_text12_43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>svd_text11_38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>svd_text11_90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>svd_text11_32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>svd_text11_35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>svd_text21_83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>svd_text11_13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>svd_text12_42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>svd_title2_69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>svd_title1_56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>svd_text21_63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>svd_text12_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>svd_text11_20</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>svd_text11_57</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>svd_title2_6</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>svd_text11_27</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>svd_text11_28</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>svd_title2_51</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature   imp\n",
       "300   svd_title1_0   5.5\n",
       "301   svd_title1_1  3.25\n",
       "4    svd_text11_12   2.5\n",
       "133  svd_text12_39   2.5\n",
       "136  svd_text12_41  2.25\n",
       "141  svd_text12_46  2.25\n",
       "121  svd_text12_28     2\n",
       "132  svd_text12_38     2\n",
       "345   svd_title1_5  1.75\n",
       "312   svd_title1_2  1.75\n",
       "268  svd_text22_25  1.75\n",
       "18   svd_text11_25  1.75\n",
       "89    svd_text11_9  1.75\n",
       "292  svd_text22_47  1.75\n",
       "319  svd_title1_26   1.5\n",
       "247  svd_text21_97   1.5\n",
       "304  svd_title1_12   1.5\n",
       "398   svd_title2_3   1.5\n",
       "12    svd_text11_2   1.5\n",
       "109  svd_text12_17   1.5\n",
       "305  svd_title1_13   1.5\n",
       "303  svd_title1_11   1.5\n",
       "297   svd_text22_7   1.5\n",
       "366  svd_title1_69   1.5\n",
       "110  svd_text12_18   1.5\n",
       "375   svd_title2_0   1.5\n",
       "42   svd_text11_47   1.5\n",
       "380  svd_title2_13   1.5\n",
       "286  svd_text22_41   1.5\n",
       "407  svd_title2_38   1.5\n",
       "..             ...   ...\n",
       "162   svd_text21_2     1\n",
       "369  svd_title1_71     1\n",
       "61   svd_text11_64     1\n",
       "252  svd_text22_10     1\n",
       "409   svd_title2_4     1\n",
       "129  svd_text12_35     1\n",
       "15   svd_text11_22     1\n",
       "358  svd_title1_61     1\n",
       "36   svd_text11_41     1\n",
       "1     svd_text11_1     1\n",
       "106  svd_text12_14     1\n",
       "220  svd_text21_72     1\n",
       "138  svd_text12_43     1\n",
       "32   svd_text11_38     1\n",
       "90   svd_text11_90     1\n",
       "26   svd_text11_32     1\n",
       "29   svd_text11_35     1\n",
       "232  svd_text21_83     1\n",
       "5    svd_text11_13     1\n",
       "137  svd_text12_42     1\n",
       "441  svd_title2_69     1\n",
       "352  svd_title1_56     1\n",
       "210  svd_text21_63     1\n",
       "102  svd_text12_10     1\n",
       "13   svd_text11_20  0.75\n",
       "53   svd_text11_57  0.75\n",
       "431   svd_title2_6  0.75\n",
       "20   svd_text11_27  0.75\n",
       "21   svd_text11_28  0.75\n",
       "422  svd_title2_51  0.75\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.sort_values('imp', ascending=False)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using leaves:  50\n",
      "(4463,) (1496,)\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.147056\tvalid_1's auc: 0.236048\n",
      "  auc =  0.4391711229946524\n",
      "0.7179144385026738\n",
      "============================================================\n",
      "(4465,) (1494,)\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.171365\tvalid_1's auc: 0.218049\n",
      "  auc =  0.4243641231593039\n",
      "0.715528781793842\n",
      "============================================================\n",
      "(4470,) (1489,)\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.175862\tvalid_1's auc: 0.247368\n",
      "  auc =  0.40899932840832776\n",
      "0.6991269308260577\n",
      "============================================================\n",
      "(4479,) (1480,)\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.169342\tvalid_1's auc: 0.221444\n",
      "  auc =  0.4195945945945946\n",
      "0.6925675675675675\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "_, f, _ = run_cv_lgb(train_all, train['target'], leaves=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using leaves:  200\n",
      "(4463,) (1496,)\n",
      "Fold 0\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.0635784\tvalid_1's auc: 0.228486\n",
      "  auc =  0.33890374331550804\n",
      "0.6778074866310161\n",
      "============================================================\n",
      "(4465,) (1494,)\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.088599\tvalid_1's auc: 0.235155\n",
      "  auc =  0.35876840696117807\n",
      "0.6927710843373494\n",
      "============================================================\n",
      "(4470,) (1489,)\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.0824321\tvalid_1's auc: 0.202618\n",
      "  auc =  0.3364674278038952\n",
      "0.6830087306917394\n",
      "============================================================\n",
      "(4479,) (1480,)\n",
      "Fold 3\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.0796541\tvalid_1's auc: 0.219037\n",
      "  auc =  0.37027027027027026\n",
      "0.7047297297297297\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "_, f, _ = run_cv_lgb(train_all, train['target'], leaves=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>svd_title1_1</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>svd_text11_9</td>\n",
       "      <td>6.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>svd_title1_0</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>svd_text12_39</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>svd_text11_12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>svd_text11_2</td>\n",
       "      <td>5.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>svd_text11_3</td>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>svd_title1_3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>svd_text21_7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>svd_text11_7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>svd_title1_2</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>svd_text11_14</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>svd_text12_41</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>svd_text21_9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>svd_text22_18</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>svd_text21_15</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>svd_text21_20</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>svd_title1_8</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>svd_text11_84</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>svd_title2_0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>svd_title1_13</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>svd_text11_33</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>svd_title1_12</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svd_text11_23</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>svd_text22_30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>svd_text11_17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>svd_title1_51</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>svd_text21_87</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svd_text11_10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>svd_text12_17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>svd_text21_66</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>svd_text12_45</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>svd_text11_22</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>svd_text21_28</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>svd_text11_94</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>svd_text12_10</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>svd_text21_29</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>svd_title2_39</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>svd_text21_19</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>svd_text21_31</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>svd_text21_32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>svd_title2_13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>svd_text21_33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>svd_text11_74</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>svd_text21_84</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>svd_text21_6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>svd_text12_35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>svd_text11_72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>svd_text21_23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>svd_title1_60</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>svd_text21_70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>svd_text11_30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>svd_title1_65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>svd_text11_31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>svd_text11_95</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>svd_text21_45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>svd_text11_35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>svd_text21_55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>svd_text11_47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>svd_title1_23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature   imp\n",
       "301   svd_title1_1   7.5\n",
       "89    svd_text11_9  6.75\n",
       "300   svd_title1_0   6.5\n",
       "133  svd_text12_39   6.5\n",
       "4    svd_text11_12     6\n",
       "12    svd_text11_2  5.75\n",
       "23    svd_text11_3  5.25\n",
       "323   svd_title1_3     5\n",
       "217   svd_text21_7     5\n",
       "67    svd_text11_7     5\n",
       "312   svd_title1_2   4.5\n",
       "6    svd_text11_14  4.25\n",
       "136  svd_text12_41  4.25\n",
       "239   svd_text21_9     4\n",
       "260  svd_text22_18  3.75\n",
       "157  svd_text21_15  3.75\n",
       "163  svd_text21_20   3.5\n",
       "373   svd_title1_8   3.5\n",
       "83   svd_text11_84   3.5\n",
       "375   svd_title2_0   3.5\n",
       "305  svd_title1_13   3.5\n",
       "27   svd_text11_33  3.25\n",
       "304  svd_title1_12  3.25\n",
       "16   svd_text11_23  3.25\n",
       "274  svd_text22_30     3\n",
       "9    svd_text11_17     3\n",
       "347  svd_title1_51     3\n",
       "236  svd_text21_87     3\n",
       "2    svd_text11_10     3\n",
       "109  svd_text12_17     3\n",
       "..             ...   ...\n",
       "213  svd_text21_66  2.25\n",
       "140  svd_text12_45  2.25\n",
       "15   svd_text11_22  2.25\n",
       "171  svd_text21_28  2.25\n",
       "94   svd_text11_94  2.25\n",
       "102  svd_text12_10  2.25\n",
       "172  svd_text21_29  2.25\n",
       "408  svd_title2_39  2.25\n",
       "161  svd_text21_19  2.25\n",
       "175  svd_text21_31  2.25\n",
       "176  svd_text21_32     2\n",
       "380  svd_title2_13     2\n",
       "177  svd_text21_33     2\n",
       "72   svd_text11_74     2\n",
       "233  svd_text21_84     2\n",
       "206   svd_text21_6     2\n",
       "129  svd_text12_35     2\n",
       "70   svd_text11_72     2\n",
       "166  svd_text21_23     2\n",
       "357  svd_title1_60     2\n",
       "218  svd_text21_70     2\n",
       "24   svd_text11_30     2\n",
       "362  svd_title1_65     2\n",
       "25   svd_text11_31     2\n",
       "95   svd_text11_95     2\n",
       "190  svd_text21_45     2\n",
       "29   svd_text11_35     2\n",
       "201  svd_text21_55     2\n",
       "42   svd_text11_47     2\n",
       "316  svd_title1_23     2\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.sort_values('imp', ascending=False)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_xgb_model(X_train, y_train, X_valid, y_valid, features, param, \n",
    "                    num_round):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X_train, X_valid: training and valid data\n",
    "        y_train, y_valid: training and valid target\n",
    "        X_test: test-data\n",
    "        features: training features\n",
    "    Return:\n",
    "        oof-pred, test_preds, model, model_imp\n",
    "    \"\"\"\n",
    "    _train = xgb.DMatrix(X_train[features], label=y_train, feature_names=list(features))\n",
    "    _valid = xgb.DMatrix(X_valid[features], label=y_valid,feature_names=list(features))\n",
    "    \n",
    "    watchlist = [(_valid, 'valid')]\n",
    "    clf = xgb.train(dtrain=_train, \n",
    "                    num_boost_round=num_round, \n",
    "                    evals=watchlist,\n",
    "                    early_stopping_rounds=25, \n",
    "                    verbose_eval=200, \n",
    "                    params=param)\n",
    "    \n",
    "    valid_frame = xgb.DMatrix(X_valid[features],feature_names=list(features))\n",
    "    oof  = clf.predict(valid_frame, ntree_limit=clf.best_ntree_limit)\n",
    "\n",
    "\n",
    "#     test_frame = xgb.DMatrix(X_test[features],feature_names=list(features))\n",
    "#     test_pred = clf.predict(test_frame, ntree_limit=clf.best_ntree_limit)\n",
    "\n",
    "    \n",
    "    xgb_imp = pd.DataFrame(data=[list(clf.get_fscore().keys()), \n",
    "                                 list(clf.get_fscore().values())]).T\n",
    "    xgb_imp.columns = ['feature','imp']\n",
    "    xgb_imp.imp = xgb_imp.imp.astype('float')\n",
    "    \n",
    "#     return oof, test_pred, clf, xgb_imp\n",
    "    return oof, clf, xgb_imp\n",
    "\n",
    "\n",
    "# def run_cv_xgb(train_df, target, test_df, depth):\n",
    "def run_cv_xgb(train_df, target, depth):\n",
    "\n",
    "    features = train_df.columns\n",
    "    params = {\n",
    "        'eval_metric'     : 'auc',\n",
    "        'seed'            : 1337,\n",
    "        'eta'             : 0.05,\n",
    "        'subsample'       : 0.7,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'silent'          : 1,\n",
    "        'nthread'         : 4,\n",
    "        'Scale_pos_weight': 3.607,\n",
    "        'objective'       : 'multi:softmax',\n",
    "        'num_class'       : 21,\n",
    "        'max_depth'       : depth,\n",
    "        'alpha'           : 0.05\n",
    "    }\n",
    "    \n",
    "    n_splits = 3\n",
    "    random_seed = 1234\n",
    "    feature_imp = pd.DataFrame()\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "    oof_xgb = np.zeros((len(train_df), 21))\n",
    "#     predictions = np.zeros((len(test_df),n_splits))\n",
    "    clfs = []\n",
    "##########################\n",
    "    for fold_, (train_index, valid_index) in enumerate(folds.split(train_df, target)):\n",
    "        print(train_index.shape, valid_index.shape)\n",
    "        print(\"Fold {}\".format(fold_))\n",
    "    \n",
    "        y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n",
    "        X_train, X_valid = train_df.iloc[train_index,:], train_df.iloc[valid_index,:]\n",
    "        features = X_train.columns\n",
    "        \n",
    "\n",
    "        num_rounds = 10000\n",
    "        oof, test_pred, clf, xgb_imp = train_xgb_model(X_train, y_train, \n",
    "                                                       X_valid, y_valid, \n",
    "                                                       features, params, \n",
    "                                                       num_rounds)\n",
    "        \n",
    "        xgb_imp['fold'] = fold_\n",
    "        feature_imp = pd.concat([feature_imp, xgb_imp], axis=0)\n",
    "    \n",
    "        oof_xgb[valid_index] = oof\n",
    "#         predictions[:,fold_] = test_pred\n",
    "        clfs.append(clf)\n",
    "        \n",
    "        score = accuracy_score(y_valid, np.argmax(oof_xgb, axis=1))\n",
    "#         print(classification_report(y_valid, np.argmax(oof, axis=1)))\n",
    "#         score = roc_auc_score(y_valid, oof)\n",
    "        print( \"  auc = \", score )\n",
    "        print(top_n_accuracy(y_valid, oof_xgb, 3))\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    feature_imp.imp = feature_imp.imp.astype('float')\n",
    "    feature_imp = feature_imp.groupby(['feature'])['imp'].mean()\n",
    "    feature_imp = pd.DataFrame(data=[feature_imp.index, feature_imp.values]).T\n",
    "    feature_imp.columns=['feature','imp']\n",
    "    feature_imp = feature_imp.sort_values(by='imp')\n",
    "\n",
    "\n",
    "    return clfs, feature_imp, oof_xgb#, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_cv_xgb(train_all, target, depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.522 \n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(max_depth=4, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(train_all, target)\n",
    "predictions = clf.predict_proba(train_all)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(target, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4469, 450), (4469,))"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    train_all, train['target'], \n",
    "    stratify=train['target'], \n",
    "    test_size=0.25\n",
    ")\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 2.122 \n",
      "  auc =  0.42348993288590603\n",
      "0.825503355704698\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(max_depth=4, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict_proba(X_test)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "\n",
    "score = accuracy_score(Y_test, np.argmax(predictions, axis=1))\n",
    "print( \"  auc = \", score )\n",
    "print(top_n_accuracy(Y_test, predictions, 3))\n",
    "print(\"=\"*60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.849 \n",
      "  auc =  0.4268456375838926\n",
      "0.8315436241610739\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(max_depth=6, n_estimators=50, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict_proba(X_test)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "\n",
    "score = accuracy_score(Y_test, np.argmax(predictions, axis=1))\n",
    "print( \"  auc = \", score )\n",
    "print(top_n_accuracy(Y_test, predictions, 3))\n",
    "print(\"=\"*60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scl = StandardScaler()\n",
    "scl.fit(train_all)\n",
    "train_all = scl.transform(train_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.881 \n",
      "  auc =  0.4161073825503356\n",
      "0.8422818791946308\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(max_depth=6, n_estimators=50, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict_proba(X_test)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "\n",
    "score = accuracy_score(Y_test, np.argmax(predictions, axis=1))\n",
    "print( \"  auc = \", score )\n",
    "print(top_n_accuracy(Y_test, predictions, 3))\n",
    "print(\"=\"*60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=-1, penalty='l2', random_state=1234,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = MultinomialNB().fit(X_train, Y_train)\n",
    "# predictions = clf.predictions\n",
    "# print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "\n",
    "\n",
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "    C=0.01, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "    random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "# predictions = logistic_reg.pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.968 \n",
      "  auc =  0.5140939597315436\n",
      "0.8100671140939597\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "predictions = logistic_reg.predict_proba(X_test)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "score = accuracy_score(Y_test, np.argmax(predictions, axis=1))\n",
    "print( \"  auc = \", score )\n",
    "print(top_n_accuracy(Y_test, predictions, 3))\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.643 \n",
      "  auc =  0.4791946308724832\n",
      "0.812751677852349\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict_proba(X_test)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "score = accuracy_score(Y_test, np.argmax(predictions, axis=1))\n",
    "print( \"  auc = \", score )\n",
    "print(top_n_accuracy(Y_test, predictions, 3))\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:27, 14394.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('../../ml-toolbox-testing/dataset/glove.6B/glove.6B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4469,), (4469,))"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    df[:train.shape[0]]['title'], train['target'], \n",
    "    stratify=train['target'], \n",
    "    test_size=0.25\n",
    ")\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/4469 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|█▋        | 730/4469 [00:00<00:00, 7298.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 1463/4469 [00:00<00:00, 7306.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 2251/4469 [00:00<00:00, 7467.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████▊   | 3027/4469 [00:00<00:00, 7552.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████▌ | 3840/4469 [00:00<00:00, 7714.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 4469/4469 [00:00<00:00, 7675.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1490 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 786/1490 [00:00<00:00, 7856.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1490/1490 [00:00<00:00, 7561.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()#.decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(X_train)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(X_test)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ankish/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4469, 300), (1490, 300))"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)\n",
    "xtrain_glove.shape, xvalid_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 2.309 \n",
      "  auc =  0.33758389261744964\n",
      "0.6161073825503356\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "# clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf = xgb.XGBClassifier(max_depth=6, n_estimators=50, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_glove, Y_train)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "score = accuracy_score(Y_test, np.argmax(predictions, axis=1))\n",
    "print( \"  auc = \", score )\n",
    "print(top_n_accuracy(Y_test, predictions, 3))\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 3.201 \n",
      "  auc =  0.3308724832214765\n",
      "0.614765100671141\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_glove, Y_train)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(Y_test, predictions))\n",
    "score = accuracy_score(Y_test, np.argmax(predictions, axis=1))\n",
    "print( \"  auc = \", score )\n",
    "print(top_n_accuracy(Y_test, predictions, 3))\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35126"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = StandardScaler()\n",
    "xtrain_glove = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove = scl.transform(xvalid_glove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 21)                6321      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 189,321\n",
      "Trainable params: 188,121\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"no model previously\")\n",
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "yvalid_enc = np_utils.to_categorical(Y_test)\n",
    "\n",
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 1490 samples\n",
      "Epoch 1/3\n",
      "4469/4469 [==============================] - 1s 283us/step - loss: 1.2083 - acc: 0.5963 - val_loss: 2.5464 - val_acc: 0.3530\n",
      "Epoch 2/3\n",
      "4469/4469 [==============================] - 0s 87us/step - loss: 1.1535 - acc: 0.5979 - val_loss: 2.5796 - val_acc: 0.3477\n",
      "Epoch 3/3\n",
      "4469/4469 [==============================] - 0s 88us/step - loss: 1.1485 - acc: 0.6124 - val_loss: 2.5863 - val_acc: 0.3409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f70f44fc4e0>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(xtrain_glove, y=ytrain_enc, batch_size=64, \n",
    "          epochs=3, verbose=1, \n",
    "          validation_data=(xvalid_glove, yvalid_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2367 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 2367/2367 [00:00<00:00, 301247.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 17\n",
    "\n",
    "token.fit_on_texts(list(X_train) + list(X_test))\n",
    "xtrain_seq = token.texts_to_sequences(X_train)\n",
    "xvalid_seq = token.texts_to_sequences(X_test)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index\n",
    "\n",
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 17, 300)           710400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_16 (Spatia (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1024)              103424    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 21)                21525     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 2,045,349\n",
      "Trainable params: 1,334,949\n",
      "Non-trainable params: 710,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except:\n",
    "    print(\"no model previously\")\n",
    "    \n",
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 1490 samples\n",
      "Epoch 1/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.6959 - acc: 0.1821 - val_loss: 2.4985 - val_acc: 0.2322\n",
      "Epoch 2/50\n",
      "4469/4469 [==============================] - 3s 671us/step - loss: 2.5046 - acc: 0.2399 - val_loss: 2.3612 - val_acc: 0.3416\n",
      "Epoch 3/50\n",
      "4469/4469 [==============================] - 3s 600us/step - loss: 2.3438 - acc: 0.2987 - val_loss: 2.2129 - val_acc: 0.3497\n",
      "Epoch 4/50\n",
      "4469/4469 [==============================] - 3s 631us/step - loss: 2.2656 - acc: 0.3269 - val_loss: 2.1535 - val_acc: 0.3658\n",
      "Epoch 5/50\n",
      "4469/4469 [==============================] - 3s 711us/step - loss: 2.2204 - acc: 0.3450 - val_loss: 2.1467 - val_acc: 0.3812\n",
      "Epoch 6/50\n",
      "4469/4469 [==============================] - 3s 599us/step - loss: 2.1601 - acc: 0.3571 - val_loss: 2.1154 - val_acc: 0.3953\n",
      "Epoch 7/50\n",
      "4469/4469 [==============================] - 3s 596us/step - loss: 2.1292 - acc: 0.3808 - val_loss: 2.0727 - val_acc: 0.4020\n",
      "Epoch 8/50\n",
      "4469/4469 [==============================] - 3s 697us/step - loss: 2.1043 - acc: 0.3766 - val_loss: 2.0646 - val_acc: 0.4074\n",
      "Epoch 9/50\n",
      "4469/4469 [==============================] - 3s 737us/step - loss: 2.0765 - acc: 0.3967 - val_loss: 2.0482 - val_acc: 0.4174\n",
      "Epoch 10/50\n",
      "4469/4469 [==============================] - 3s 617us/step - loss: 2.0363 - acc: 0.3972 - val_loss: 2.0292 - val_acc: 0.4235\n",
      "Epoch 11/50\n",
      "4469/4469 [==============================] - 3s 716us/step - loss: 2.0118 - acc: 0.4030 - val_loss: 2.0183 - val_acc: 0.4322\n",
      "Epoch 12/50\n",
      "4469/4469 [==============================] - 3s 776us/step - loss: 2.0032 - acc: 0.4182 - val_loss: 2.0042 - val_acc: 0.4322\n",
      "Epoch 13/50\n",
      "4469/4469 [==============================] - 3s 653us/step - loss: 1.9769 - acc: 0.4276 - val_loss: 2.0034 - val_acc: 0.4396\n",
      "Epoch 14/50\n",
      "4469/4469 [==============================] - 3s 612us/step - loss: 1.9594 - acc: 0.4229 - val_loss: 2.0039 - val_acc: 0.4409\n",
      "Epoch 15/50\n",
      "4469/4469 [==============================] - 3s 674us/step - loss: 1.9272 - acc: 0.4301 - val_loss: 2.0003 - val_acc: 0.4436\n",
      "Epoch 16/50\n",
      "4469/4469 [==============================] - 3s 649us/step - loss: 1.9356 - acc: 0.4332 - val_loss: 1.9934 - val_acc: 0.4329\n",
      "Epoch 17/50\n",
      "4469/4469 [==============================] - 3s 754us/step - loss: 1.9009 - acc: 0.4444 - val_loss: 1.9968 - val_acc: 0.4450\n",
      "Epoch 18/50\n",
      "4469/4469 [==============================] - 3s 657us/step - loss: 1.9000 - acc: 0.4392 - val_loss: 1.9985 - val_acc: 0.4396\n",
      "Epoch 19/50\n",
      "4469/4469 [==============================] - 3s 609us/step - loss: 1.8770 - acc: 0.4410 - val_loss: 1.9915 - val_acc: 0.4450\n",
      "Epoch 20/50\n",
      "4469/4469 [==============================] - 3s 613us/step - loss: 1.8704 - acc: 0.4489 - val_loss: 1.9910 - val_acc: 0.4483\n",
      "Epoch 21/50\n",
      "4469/4469 [==============================] - 3s 602us/step - loss: 1.8421 - acc: 0.4536 - val_loss: 1.9884 - val_acc: 0.4503\n",
      "Epoch 22/50\n",
      "4469/4469 [==============================] - 3s 618us/step - loss: 1.8455 - acc: 0.4558 - val_loss: 1.9858 - val_acc: 0.4450\n",
      "Epoch 23/50\n",
      "4469/4469 [==============================] - 3s 611us/step - loss: 1.8136 - acc: 0.4589 - val_loss: 1.9868 - val_acc: 0.4443\n",
      "Epoch 24/50\n",
      "4469/4469 [==============================] - 3s 612us/step - loss: 1.8146 - acc: 0.4589 - val_loss: 1.9864 - val_acc: 0.4423\n",
      "Epoch 25/50\n",
      "4469/4469 [==============================] - 3s 634us/step - loss: 1.8002 - acc: 0.4654 - val_loss: 1.9973 - val_acc: 0.4477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f70725cf518>"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=batch_size, epochs=epochs, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n",
    "\n",
    "\n",
    "# model.fit(xtrain_pad, y=ytrain_enc, batch_size=64, \n",
    "#           epochs=10, verbose=1, \n",
    "#           validation_data=(xvalid_pad, yvalid_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 17, 300)           710400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_17 (Spatia (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 21)                21525     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 2,810,949\n",
      "Trainable params: 2,100,549\n",
      "Non-trainable params: 710,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except:\n",
    "    print(\"no model previously\")\n",
    "\n",
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 1490 samples\n",
      "Epoch 1/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.6953 - acc: 0.1804 - val_loss: 2.5068 - val_acc: 0.2698\n",
      "Epoch 2/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.4504 - acc: 0.2584 - val_loss: 2.3125 - val_acc: 0.3477\n",
      "Epoch 3/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.3225 - acc: 0.3142 - val_loss: 2.2211 - val_acc: 0.3611\n",
      "Epoch 4/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.2541 - acc: 0.3256 - val_loss: 2.1455 - val_acc: 0.3732\n",
      "Epoch 5/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.2084 - acc: 0.3533 - val_loss: 2.1323 - val_acc: 0.3832\n",
      "Epoch 6/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.1647 - acc: 0.3654 - val_loss: 2.0669 - val_acc: 0.3946\n",
      "Epoch 7/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.1048 - acc: 0.3853 - val_loss: 2.0603 - val_acc: 0.4060\n",
      "Epoch 8/50\n",
      "4469/4469 [==============================] - 8s 2ms/step - loss: 2.0791 - acc: 0.3909 - val_loss: 2.0370 - val_acc: 0.4215\n",
      "Epoch 9/50\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 2.0670 - acc: 0.3905 - val_loss: 2.0429 - val_acc: 0.4195\n",
      "Epoch 10/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.0308 - acc: 0.4146 - val_loss: 2.0278 - val_acc: 0.4255\n",
      "Epoch 11/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 2.0241 - acc: 0.4122 - val_loss: 2.0176 - val_acc: 0.4282\n",
      "Epoch 12/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 1.9877 - acc: 0.4213 - val_loss: 2.0183 - val_acc: 0.4221\n",
      "Epoch 13/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 1.9760 - acc: 0.4316 - val_loss: 2.0176 - val_acc: 0.4369\n",
      "Epoch 14/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 1.9485 - acc: 0.4267 - val_loss: 1.9954 - val_acc: 0.4295\n",
      "Epoch 15/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 1.9440 - acc: 0.4290 - val_loss: 2.0110 - val_acc: 0.4463\n",
      "Epoch 16/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 1.9357 - acc: 0.4301 - val_loss: 1.9959 - val_acc: 0.4383\n",
      "Epoch 17/50\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 1.9074 - acc: 0.4314 - val_loss: 2.0144 - val_acc: 0.4409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f70664002b0>"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=batch_size, epochs=epochs, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 17, 300)           710400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_18 (Spatia (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 600)               1442400   \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1024)              615424    \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 21)                21525     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 3,839,349\n",
      "Trainable params: 3,128,949\n",
      "Non-trainable params: 710,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except:\n",
    "    print(\"no model previously\")\n",
    "\n",
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 1490 samples\n",
      "Epoch 1/50\n",
      "4469/4469 [==============================] - 14s 3ms/step - loss: 2.6930 - val_loss: 2.4781\n",
      "Epoch 2/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.4610 - val_loss: 2.2849\n",
      "Epoch 3/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.3076 - val_loss: 2.1916\n",
      "Epoch 4/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.2434 - val_loss: 2.1729\n",
      "Epoch 5/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.1994 - val_loss: 2.1053\n",
      "Epoch 6/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.1592 - val_loss: 2.0853\n",
      "Epoch 7/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.1100 - val_loss: 2.0549\n",
      "Epoch 8/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.0700 - val_loss: 2.0334\n",
      "Epoch 9/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.0545 - val_loss: 2.0186\n",
      "Epoch 10/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.0127 - val_loss: 2.0102\n",
      "Epoch 11/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.0043 - val_loss: 2.0223\n",
      "Epoch 12/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 1.9885 - val_loss: 2.0118\n",
      "Epoch 13/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 1.9564 - val_loss: 2.0085\n",
      "Epoch 14/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 1.9439 - val_loss: 1.9895\n",
      "Epoch 15/50\n",
      "4469/4469 [==============================] - 11s 2ms/step - loss: 1.9229 - val_loss: 2.0053\n",
      "Epoch 16/50\n",
      "4469/4469 [==============================] - 11s 2ms/step - loss: 1.9026 - val_loss: 2.0005\n",
      "Epoch 17/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 1.8849 - val_loss: 1.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7072636a90>"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=batch_size, epochs=epochs, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 17, 300)           710400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_19 (Spatia (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 17, 300)           540900    \n",
      "_________________________________________________________________\n",
      "gru_16 (GRU)                 (None, 300)               540900    \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 21)                21525     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 3,171,549\n",
      "Trainable params: 2,461,149\n",
      "Non-trainable params: 710,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except:\n",
    "    print(\"no model previously\")\n",
    "    \n",
    "    \n",
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 1490 samples\n",
      "Epoch 1/50\n",
      "4469/4469 [==============================] - 13s 3ms/step - loss: 2.7085 - acc: 0.1716 - val_loss: 2.5526 - val_acc: 0.2732\n",
      "Epoch 2/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.5028 - acc: 0.2352 - val_loss: 2.3511 - val_acc: 0.3315\n",
      "Epoch 3/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.3682 - acc: 0.2998 - val_loss: 2.2801 - val_acc: 0.3423\n",
      "Epoch 4/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.3073 - acc: 0.3242 - val_loss: 2.2014 - val_acc: 0.3617\n",
      "Epoch 5/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.2583 - acc: 0.3381 - val_loss: 2.1720 - val_acc: 0.3819\n",
      "Epoch 6/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.2265 - acc: 0.3394 - val_loss: 2.1587 - val_acc: 0.4067\n",
      "Epoch 7/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1944 - acc: 0.3522 - val_loss: 2.1368 - val_acc: 0.4181\n",
      "Epoch 8/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1634 - acc: 0.3591 - val_loss: 2.0892 - val_acc: 0.4081\n",
      "Epoch 9/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1406 - acc: 0.3737 - val_loss: 2.1075 - val_acc: 0.4181\n",
      "Epoch 10/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1027 - acc: 0.3844 - val_loss: 2.0645 - val_acc: 0.4201\n",
      "Epoch 11/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1001 - acc: 0.3878 - val_loss: 2.0654 - val_acc: 0.4329\n",
      "Epoch 12/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0650 - acc: 0.3929 - val_loss: 2.0559 - val_acc: 0.4295\n",
      "Epoch 13/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0579 - acc: 0.4019 - val_loss: 2.0667 - val_acc: 0.4349\n",
      "Epoch 14/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0564 - acc: 0.3992 - val_loss: 2.0220 - val_acc: 0.4302\n",
      "Epoch 15/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0122 - acc: 0.4086 - val_loss: 2.0187 - val_acc: 0.4396\n",
      "Epoch 16/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0023 - acc: 0.4135 - val_loss: 2.0412 - val_acc: 0.4403\n",
      "Epoch 17/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9985 - acc: 0.4162 - val_loss: 2.0127 - val_acc: 0.4436\n",
      "Epoch 18/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9882 - acc: 0.4236 - val_loss: 2.0198 - val_acc: 0.4409\n",
      "Epoch 19/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 1.9406 - acc: 0.4319 - val_loss: 2.0356 - val_acc: 0.4356\n",
      "Epoch 20/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9362 - acc: 0.4406 - val_loss: 2.0268 - val_acc: 0.4436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f705af79240>"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=batch_size, epochs=epochs, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias: eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True)+K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import Model\n",
    "\n",
    "from keras.layers import Bidirectional, CuDNNLSTM, LSTM, CuDNNGRU, GRU, Embedding\n",
    "from keras.layers import Dense, Input, Dropout, Activation, Conv1D, Flatten, Concatenate\n",
    "from keras.layers import SpatialDropout1D, Dropout, GlobalMaxPooling1D, MaxPooling1D\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping,ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "embedding_21 (Embedding)     (None, 17, 300)           710400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_20 (Spatia (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "gru_17 (GRU)                 (None, 17, 300)           540900    \n",
      "_________________________________________________________________\n",
      "gru_18 (GRU)                 (None, 17, 300)           540900    \n",
      "_________________________________________________________________\n",
      "attention_5 (Attention)      (None, 300)               317       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 21)                21525     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 3,171,866\n",
      "Trainable params: 2,461,466\n",
      "Non-trainable params: 710,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except:\n",
    "    print(\"no model previously\")\n",
    "    \n",
    "\n",
    "# GRU with glove embeddings and two dense layers\n",
    "\n",
    "inp = Input(shape=(max_len,))\n",
    "x = Embedding(  len(word_index) + 1,\n",
    "                300,\n",
    "                weights=[embedding_matrix],\n",
    "                input_length=max_len,\n",
    "                trainable=False)(inp)\n",
    "\n",
    "x = SpatialDropout1D(0.3)(x)\n",
    "x = GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)(x)\n",
    "x = GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)(x)\n",
    "\n",
    "x = Attention(max_len)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.8)(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.8)(x)\n",
    "x = Dense(21)(x)\n",
    "x = Activation('softmax')(x)\n",
    "\n",
    "model = Model(inp, x)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 17, 300)           710400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_21 (Spatia (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "gru_19 (GRU)                 (None, 17, 300)           540900    \n",
      "_________________________________________________________________\n",
      "gru_20 (GRU)                 (None, 17, 300)           540900    \n",
      "_________________________________________________________________\n",
      "attention_6 (Attention)      (None, 300)               317       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 21)                21525     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 3,171,866\n",
      "Trainable params: 2,461,466\n",
      "Non-trainable params: 710,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "\n",
    "model.add(Attention(max_len))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 1490 samples\n",
      "Epoch 1/50\n",
      "4469/4469 [==============================] - 14s 3ms/step - loss: 2.7430 - acc: 0.1707 - val_loss: 2.5905 - val_acc: 0.2020\n",
      "Epoch 2/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.5748 - acc: 0.1987 - val_loss: 2.4684 - val_acc: 0.2322\n",
      "Epoch 3/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.4698 - acc: 0.2493 - val_loss: 2.3137 - val_acc: 0.3188\n",
      "Epoch 4/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.3434 - acc: 0.3048 - val_loss: 2.2061 - val_acc: 0.3570\n",
      "Epoch 5/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 2.2650 - acc: 0.3327 - val_loss: 2.1630 - val_acc: 0.3711\n",
      "Epoch 6/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.2222 - acc: 0.3457 - val_loss: 2.1120 - val_acc: 0.3846\n",
      "Epoch 7/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1889 - acc: 0.3576 - val_loss: 2.0916 - val_acc: 0.3960\n",
      "Epoch 8/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1338 - acc: 0.3775 - val_loss: 2.0642 - val_acc: 0.3933\n",
      "Epoch 9/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.1125 - acc: 0.3815 - val_loss: 2.0648 - val_acc: 0.4027\n",
      "Epoch 10/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0925 - acc: 0.3851 - val_loss: 2.0560 - val_acc: 0.4034\n",
      "Epoch 11/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0833 - acc: 0.3871 - val_loss: 2.0249 - val_acc: 0.4128\n",
      "Epoch 12/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0542 - acc: 0.4008 - val_loss: 2.0197 - val_acc: 0.4215\n",
      "Epoch 13/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0311 - acc: 0.4093 - val_loss: 2.0106 - val_acc: 0.4188\n",
      "Epoch 14/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 2.0160 - acc: 0.4059 - val_loss: 2.0014 - val_acc: 0.4242\n",
      "Epoch 15/50\n",
      "4469/4469 [==============================] - 11s 2ms/step - loss: 2.0009 - acc: 0.4144 - val_loss: 1.9773 - val_acc: 0.4315\n",
      "Epoch 16/50\n",
      "4469/4469 [==============================] - 10s 2ms/step - loss: 1.9918 - acc: 0.4236 - val_loss: 1.9840 - val_acc: 0.4282\n",
      "Epoch 17/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9445 - acc: 0.4296 - val_loss: 1.9710 - val_acc: 0.4396\n",
      "Epoch 18/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9563 - acc: 0.4281 - val_loss: 1.9645 - val_acc: 0.4416\n",
      "Epoch 19/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9301 - acc: 0.4319 - val_loss: 1.9638 - val_acc: 0.4456\n",
      "Epoch 20/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9048 - acc: 0.4408 - val_loss: 1.9783 - val_acc: 0.4369\n",
      "Epoch 21/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.9121 - acc: 0.4375 - val_loss: 1.9695 - val_acc: 0.4349\n",
      "Epoch 22/50\n",
      "4469/4469 [==============================] - 9s 2ms/step - loss: 1.8974 - acc: 0.4473 - val_loss: 1.9835 - val_acc: 0.4409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f704f20c7f0>"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=batch_size, epochs=epochs, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 17, 300)           710400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_22 (Spatia (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 17, 600)           1442400   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 17, 600)           2162400   \n",
      "_________________________________________________________________\n",
      "attention_7 (Attention)      (None, 600)               617       \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 1024)              615424    \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 21)                21525     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 21)                0         \n",
      "=================================================================\n",
      "Total params: 6,002,366\n",
      "Trainable params: 5,291,966\n",
      "Non-trainable params: 710,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "    gc.collect()\n",
    "except:\n",
    "    print(\"no model previously\")\n",
    "    \n",
    "\n",
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))\n",
    "\n",
    "model.add(Attention(max_len))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 1490 samples\n",
      "Epoch 1/50\n",
      "4469/4469 [==============================] - 34s 8ms/step - loss: 2.7301 - acc: 0.1774 - val_loss: 2.5207 - val_acc: 0.2161\n",
      "Epoch 2/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 2.5041 - acc: 0.2298 - val_loss: 2.3269 - val_acc: 0.2993\n",
      "Epoch 3/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 2.3609 - acc: 0.2873 - val_loss: 2.2491 - val_acc: 0.3403\n",
      "Epoch 4/50\n",
      "4469/4469 [==============================] - 26s 6ms/step - loss: 2.2912 - acc: 0.3262 - val_loss: 2.1848 - val_acc: 0.3611\n",
      "Epoch 5/50\n",
      "4469/4469 [==============================] - 26s 6ms/step - loss: 2.2233 - acc: 0.3468 - val_loss: 2.1001 - val_acc: 0.3846\n",
      "Epoch 6/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 2.1735 - acc: 0.3580 - val_loss: 2.0771 - val_acc: 0.3913\n",
      "Epoch 7/50\n",
      "4469/4469 [==============================] - 29s 6ms/step - loss: 2.1306 - acc: 0.3806 - val_loss: 2.0955 - val_acc: 0.4054\n",
      "Epoch 8/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 2.1003 - acc: 0.3846 - val_loss: 2.0544 - val_acc: 0.4040\n",
      "Epoch 9/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 2.0689 - acc: 0.3963 - val_loss: 2.0332 - val_acc: 0.4248\n",
      "Epoch 10/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 2.0659 - acc: 0.4028 - val_loss: 2.0264 - val_acc: 0.4248\n",
      "Epoch 11/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 2.0173 - acc: 0.4155 - val_loss: 2.0041 - val_acc: 0.4289\n",
      "Epoch 12/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 1.9870 - acc: 0.4184 - val_loss: 1.9961 - val_acc: 0.4289\n",
      "Epoch 13/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 1.9608 - acc: 0.4265 - val_loss: 1.9892 - val_acc: 0.4295\n",
      "Epoch 14/50\n",
      "4469/4469 [==============================] - 29s 7ms/step - loss: 1.9517 - acc: 0.4319 - val_loss: 1.9951 - val_acc: 0.4336\n",
      "Epoch 15/50\n",
      "4469/4469 [==============================] - 28s 6ms/step - loss: 1.9205 - acc: 0.4350 - val_loss: 1.9890 - val_acc: 0.4362\n",
      "Epoch 16/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 1.9214 - acc: 0.4457 - val_loss: 1.9853 - val_acc: 0.4403\n",
      "Epoch 17/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 1.9020 - acc: 0.4457 - val_loss: 1.9827 - val_acc: 0.4409\n",
      "Epoch 18/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 1.8673 - acc: 0.4478 - val_loss: 1.9795 - val_acc: 0.4456\n",
      "Epoch 19/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 1.8476 - acc: 0.4547 - val_loss: 1.9906 - val_acc: 0.4436\n",
      "Epoch 20/50\n",
      "4469/4469 [==============================] - 27s 6ms/step - loss: 1.8533 - acc: 0.4567 - val_loss: 1.9873 - val_acc: 0.4510\n",
      "Epoch 21/50\n",
      "4469/4469 [==============================] - 28s 6ms/step - loss: 1.8190 - acc: 0.4659 - val_loss: 2.0006 - val_acc: 0.4483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f704f20c908>"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=batch_size, epochs=epochs, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1490, 17)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xvalid_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2553, 17)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest_seq = token.texts_to_sequences(df[train.shape[0] : ]['title'])\n",
    "xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n",
    "xtest_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21610738233031843,\n",
       " 0.2993288591004058,\n",
       " 0.3402684564958483,\n",
       " 0.36107382562336504,\n",
       " 0.38456375814924304,\n",
       " 0.39127516790524425,\n",
       " 0.4053691279968159,\n",
       " 0.4040268461176213,\n",
       " 0.4248322142450601,\n",
       " 0.4248322146050882,\n",
       " 0.42885906088272197,\n",
       " 0.42885906088272197,\n",
       " 0.4295302011822694,\n",
       " 0.4335570474599032,\n",
       " 0.43624160993819266,\n",
       " 0.44026845685587634,\n",
       " 0.4409395974354456,\n",
       " 0.4456375843726549,\n",
       " 0.443624160913813,\n",
       " 0.45100671088935546,\n",
       " 0.44832214805103787]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2553,)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict_classes(xtest_pad, batch_size=500)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 567, 1194,  234,    8,  239,   31,  136,   37,    6,   44,  216,\n",
       "        611,  467,  107,  715,  390,  123,  410,   97,  228,   99])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([207, 862,  43,   0,  92,   0,  67,   0,   0,   0,  56, 279, 238,\n",
       "        26, 225, 151,  38,  84,  45,  88,  52])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = pd.DataFrame(data=pred, columns=['target'])\n",
    "sub1 = pd.concat([test, pred], axis=1)\n",
    "sub1['topic'] = sub1['target'].apply(lambda x: cl_map_inv[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1.drop('target', axis=1, inplace=True)\n",
    "sub1.to_csv('submission/sub2.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I use chia seed in my protein shakes. These ta...</td>\n",
       "      <td>Bad tast</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I use chia seed in my protein shakes. These ta...</td>\n",
       "      <td>Bad tast</td>\n",
       "      <td>Quality/Contaminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don’t waste your money.</td>\n",
       "      <td>No change. No results.</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I use the book 'Fortify Your Life' by Tieraona...</td>\n",
       "      <td>Good Vegan Choice, Poor Non Vegan Choice</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I use the book 'Fortify Your Life' by Tieraona...</td>\n",
       "      <td>Good Vegan Choice, Poor Non Vegan Choice</td>\n",
       "      <td>Bad Taste/Flavor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review Text  \\\n",
       "0  I use chia seed in my protein shakes. These ta...   \n",
       "1  I use chia seed in my protein shakes. These ta...   \n",
       "2                            Don’t waste your money.   \n",
       "3  I use the book 'Fortify Your Life' by Tieraona...   \n",
       "4  I use the book 'Fortify Your Life' by Tieraona...   \n",
       "\n",
       "                               Review Title                 topic  \n",
       "0                                  Bad tast  Quality/Contaminated  \n",
       "1                                  Bad tast  Quality/Contaminated  \n",
       "2                    No change. No results.         Not Effective  \n",
       "3  Good Vegan Choice, Poor Non Vegan Choice      Bad Taste/Flavor  \n",
       "4  Good Vegan Choice, Poor Non Vegan Choice      Bad Taste/Flavor  "
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1.columns = sub_df.columns\n",
    "sub1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not terrible, but not good. Tastes burnt and a...</td>\n",
       "      <td>Not my cup o’ joe</td>\n",
       "      <td>Burnt/ Over -roast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am so disappointed, it has no flavor, doesn'...</td>\n",
       "      <td>I am so disappointed, it has no flavor</td>\n",
       "      <td>Bad Flavor/Taste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I recently gave up my daily 6 cups of coffee, ...</td>\n",
       "      <td>Flavor was dissapointing</td>\n",
       "      <td>Bitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meh. I've adored Peruvian coffee for 20 years....</td>\n",
       "      <td>Smooth but majorly bland. Won't repurchase.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meh. I've adored Peruvian coffee for 20 years....</td>\n",
       "      <td>Smooth but majorly bland. Won't repurchase.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review Text  \\\n",
       "0  Not terrible, but not good. Tastes burnt and a...   \n",
       "1  I am so disappointed, it has no flavor, doesn'...   \n",
       "2  I recently gave up my daily 6 cups of coffee, ...   \n",
       "3  Meh. I've adored Peruvian coffee for 20 years....   \n",
       "4  Meh. I've adored Peruvian coffee for 20 years....   \n",
       "\n",
       "                                  Review Title               topic  \n",
       "0                            Not my cup o’ joe  Burnt/ Over -roast  \n",
       "1       I am so disappointed, it has no flavor    Bad Flavor/Taste  \n",
       "2                     Flavor was dissapointing              Bitter  \n",
       "3  Smooth but majorly bland. Won't repurchase.                 NaN  \n",
       "4  Smooth but majorly bland. Won't repurchase.                 NaN  "
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.read_csv('Dataset/Sample_Submission.csv')\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1.to_csv('submission/sub2.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f69ebb84c88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['wc_text']  = df['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df['wc_title'] = df['title'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(20, 4))\n",
    "ax[0].scatter(range(df.shape[0]), df['wc_text'])\n",
    "ax[1].scatter(range(df.shape[0]), df['wc_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('new_dataset')\n",
    "# train.to_csv('new_dataset/train1.csv', index=None)\n",
    "# test.to_csv('new_dataset/test1.csv', index=None)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col: wc_text    min: 1          max: 403        low: 2.0        high: 119.0     \n",
      "Col: wc_title   min: 1          max: 17         low: 1.0        high: 8.0       \n",
      "Col: wc_text    min: 1          max: 403        low: 2.0        high: 144.0     \n",
      "Col: wc_title   min: 1          max: 17         low: 1.0        high: 10.0      \n",
      "Col: wc_text    min: 1          max: 403        low: 2.0        high: 196.0     \n",
      "Col: wc_title   min: 1          max: 17         low: 1.0        high: 12.0      \n"
     ]
    }
   ],
   "source": [
    "def get_quantile(df, col, q1, q2):\n",
    "    \"\"\"compute quantile range\n",
    "    args:\n",
    "        col: col name\n",
    "        q1: lower quantile percentile\n",
    "        q2: upper quantile percentile\n",
    "    \"\"\"\n",
    "    df1 = df[[col]].dropna()\n",
    "    lower_bound = np.percentile(df1, q=q1)\n",
    "    upper_bound = np.percentile(df1, q=q2)\n",
    "    lower_bound = np.round(lower_bound,3)\n",
    "    upper_bound = np.round(upper_bound, 3)\n",
    "    min_ = np.round(np.min(df1[col]), 3)\n",
    "    max_ = np.round(np.max(df1[col]), 3)\n",
    "    print(\"Col: {4:<10} min: {0:<10} max: {1:<10} low: {2:<10} high: {3:<10}\".format(min_, max_, lower_bound, upper_bound, col))\n",
    "\n",
    "get_quantile(df, 'wc_text', 1, 95)\n",
    "get_quantile(df, 'wc_title', 1, 95)\n",
    "\n",
    "get_quantile(df, 'wc_text', 1, 97)\n",
    "get_quantile(df, 'wc_title', 1, 97)\n",
    "\n",
    "get_quantile(df, 'wc_text', 1, 99)\n",
    "get_quantile(df, 'wc_title', 1, 99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias: eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True)+K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "# from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\n",
    "# from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "\n",
    "# max_features = 50000\n",
    "# num_classes  = 21\n",
    "# max_words    = 200\n",
    "# filter_sizes = [1,2,3,5]\n",
    "# num_filters  = 36\n",
    "# inp = Input(shape=(max_words,))\n",
    "# x = Embedding(max_features, 300, trainable=False)(inp)\n",
    "# x = Reshape((max_words, 300, 1))(x)\n",
    "# maxpool_pool = []\n",
    "# for i in range(len(filter_sizes)):\n",
    "#     conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], 300),\n",
    "#                                  kernel_initializer='he_normal', activation='relu')(x)\n",
    "#     maxpool_pool.append(MaxPool2D(pool_size=(max_words - filter_sizes[i] + 1, 1))(conv))\n",
    "# z = Concatenate(axis=1)(maxpool_pool)   \n",
    "# z = Flatten()(z)\n",
    "# z = Dropout(0.1)(z)\n",
    "# outp = Dense(num_classes, activation=\"softmax\")(z)\n",
    "# model = Model(inputs=inp, outputs=outp)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import Model\n",
    "\n",
    "from keras.layers import Bidirectional, CuDNNLSTM, LSTM, CuDNNGRU, GRU, Embedding\n",
    "from keras.layers import Dense, Input, Dropout, Activation, Conv1D, Flatten, Concatenate\n",
    "from keras.layers import SpatialDropout1D, Dropout, GlobalMaxPooling1D, MaxPooling1D\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping,ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9387, 2830)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1 = Tokenizer()\n",
    "tokenizer1.fit_on_texts(list(df['text']))\n",
    "word_index1 = tokenizer1.word_index\n",
    "\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(list(df['title']))\n",
    "word_index2 = tokenizer2.word_index\n",
    "\n",
    "len(word_index1), len(word_index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8512, 200), (8512, 17))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features1 = len(word_index1) + 1\n",
    "max_features2 = len(word_index2) + 1\n",
    "\n",
    "max_words1 = 200\n",
    "max_words2 = 17\n",
    "\n",
    "text = tokenizer1.texts_to_sequences(df['text'])\n",
    "text = pad_sequences(text, maxlen = max_words1)\n",
    "\n",
    "title = tokenizer2.texts_to_sequences(df['title'])\n",
    "title = pad_sequences(title, maxlen = max_words2)\n",
    "\n",
    "text.shape, title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size   = 32\n",
    "epochs       = 40\n",
    "num_classes  = 21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'attention_3/Sum_1:0' shape=(?, 128) dtype=float32>,\n",
       " <tf.Tensor 'attention_4/Sum_1:0' shape=(?, 64) dtype=float32>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp1 = Input(shape=(max_words1,))\n",
    "inp2 = Input(shape=(max_words2,))\n",
    "\n",
    "x1 = Embedding(max_features1, 300, trainable=True)(inp1)\n",
    "x2 = Embedding(max_features2, 300, trainable=True)(inp2)\n",
    "\n",
    "x1 = Bidirectional(LSTM(128, return_sequences=True))(x1)\n",
    "x1 = Bidirectional(LSTM(64, return_sequences=True))(x1)\n",
    "x1 = Attention(max_words1)(x1)\n",
    "\n",
    "x2 = Bidirectional(LSTM(64, return_sequences=True))(x2)\n",
    "x2 = Bidirectional(LSTM(32, return_sequences=True))(x2)\n",
    "x2 = Attention(max_words2)(x2)\n",
    "\n",
    "x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 200, 300)     2816400     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 17, 300)      849300      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 200, 256)     439296      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 17, 128)      186880      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 200, 128)     164352      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 17, 64)       41216       bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 128)          328         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 64)           81          bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192)          0           attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          24704       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 21)           1365        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,532,178\n",
      "Trainable params: 4,532,178\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = Concatenate(axis=-1)([x1, x2])\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "modelATT = Model(inputs=[inp1, inp2], outputs=x)\n",
    "modelATT.compile(loss='categorical_crossentropy', \n",
    "                 optimizer=Adam(lr=1e-2), metrics=['accuracy'])\n",
    "modelATT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5959, 5), (2553, 2))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_len = train.shape[0]\n",
    "ts_len = test.shape[0]\n",
    "# train.shape[0] + test.shape[0], df.shape[0]\n",
    "train1 = df.iloc[:tr_len]\n",
    "test1  = df.iloc[tr_len:]\n",
    "train1.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping(df, col_name):\n",
    "    cat_codes = df[col_name].astype('category')\n",
    "    \n",
    "    class_mapping = {}\n",
    "    i = 0\n",
    "    for col in cat_codes.cat.categories:\n",
    "        class_mapping[col] = i\n",
    "        i += 1\n",
    "    \n",
    "    class_mapping_reverse = {}\n",
    "    for key, value in class_mapping.items():\n",
    "        class_mapping_reverse[value] = key\n",
    "\n",
    "    return class_mapping, class_mapping_reverse\n",
    "\n",
    "cl_map, cl_map_inv = get_mapping(train1, 'topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5959, 217) (2553, 217)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4171, 217), (4171,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1['target'] = train1['topic'].astype('category').cat.codes\n",
    "train1['target'] = train1['target'].astype('int')\n",
    "\n",
    "text_title = np.concatenate([text, title], axis=1)\n",
    "tr_text_title = text_title[:tr_len]\n",
    "ts_text_title = text_title[tr_len:]\n",
    "print(tr_text_title.shape, ts_text_title.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    tr_text_title, train1['target'], shuffle=True,\n",
    "    stratify=train1['target'], test_size=0.3, random_state=1337\n",
    ")\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4171, 200), (4171, 17), (1788, 200), (1788, 17))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_text  = X_train[:,:200]\n",
    "X_tr_title = X_train[:,200:]\n",
    "\n",
    "X_ts_text  = X_test[:,:200]\n",
    "X_ts_title = X_test[:,200:]\n",
    "\n",
    "X_tr_text.shape, X_tr_title.shape, X_ts_text.shape, X_ts_title.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4171, 21), (1788, 21))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "Y_train = to_categorical(Y_train, num_classes=num_classes)\n",
    "Y_test  = to_categorical(Y_test, num_classes=num_classes)\n",
    "Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.60750658e-05, 1.85060810e-05, 1.85060810e-05, ...,\n",
       "       1.95252374e-05, 1.60750658e-05, 5.45514998e-06])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "def get_class_weights(y):\n",
    "    \"\"\" \n",
    "    Example:\n",
    "        model.fit(X_t, y, batch_size=10, epochs=2,validation_split=0.1,sample_weight=sample_wts)\n",
    "    \n",
    "    \"\"\"\n",
    "    return class_weight.compute_sample_weight('balanced', y)\n",
    "\n",
    "cls_wts = get_class_weights(Y_train)\n",
    "cls_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4171,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_wts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelATT.compile(loss='categorical_crossentropy', \n",
    "                 optimizer=Adam(lr=1e-2), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4171 samples, validate on 1788 samples\n",
      "Epoch 1/40\n",
      "4171/4171 [==============================] - 113s 27ms/step - loss: 8.4009e-05 - acc: 0.0173 - val_loss: 3.0743 - val_acc: 0.0358\n",
      "Epoch 2/40\n",
      "4171/4171 [==============================] - 103s 25ms/step - loss: 8.3744e-05 - acc: 0.0254 - val_loss: 3.0808 - val_acc: 0.0229\n",
      "Epoch 00002: early stopping\n",
      "CPU times: user 11min 22s, sys: 58.9 s, total: 12min 21s\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "early_stop = EarlyStopping(monitor=\"val_acc\", patience=1, verbose=1)\n",
    "history    = modelATT.fit(\n",
    "    [X_tr_text, X_tr_title], Y_train,\n",
    "    validation_data = ([X_ts_text, X_ts_title], Y_test),\n",
    "    epochs          = epochs,\n",
    "    batch_size      = batch_size,\n",
    "    verbose         = 1,\n",
    "    sample_weight   = cls_wts,\n",
    "    callbacks       = [early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "\n",
    "def tfidf_feature(train, test, col_name, min_df=3, analyzer='word', \n",
    "                  token_pattern=r'\\w{1,}', ngram=3, stopwords='english', \n",
    "                  n_component=120, decom_flag=False, which_method='svd', \n",
    "                  max_features=None):\n",
    "    \"\"\"return tfidf feature\n",
    "    Args:\n",
    "        train, test: dataframe\n",
    "        col_name: column name of text feature\n",
    "        min_df: if Int, then it represent count of the minimum words in corpus (remove very rare word)\n",
    "        analyzer: [‘word’, ‘char’]\n",
    "        ngram: max range of ngram\n",
    "        token_pattern: [using: r'\\w{1,}'] [by default: '(?u)\\b\\w\\w+\\b']\n",
    "        stopwords: ['english' or customized by remove specific words]\n",
    "        n_component: n_component of svd feature transform\n",
    "        decom_flag: Wheteher to run svd/nmf on top of that or not (by default: False)\n",
    "        which_method: which to run [svd or nmf] on top of tfidf (by default: False)\n",
    "        max_features: max no of features to keep, based on frequency. It will keep words with higher freq\n",
    "    return:\n",
    "        Transformed feature space of the text data, as well as tfidf function instance\n",
    "        if svd_flag== True : train_tf, test_tf, tfv, svd\n",
    "        else : train_tf, test_tf, tfv\n",
    "    example:\n",
    "        train_tfv, test_tfv, tfv = tfidf_feature(X_train, X_test, ['text'], min_df=3)\n",
    "        train_svd, test_svd, complete_tfv, tfv, svd = tfidf_feature(X_train, X_test, ['text'], \n",
    "            min_df=3, svd_component=3, svd_flag=True)\n",
    "\n",
    "    \"\"\"\n",
    "    tfv = TfidfVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, max_df=1.0, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                stop_words = stopwords)\n",
    "\n",
    "    complete_df = pd.concat([train[col_name], test[col_name]], axis=0)\n",
    "#         return complete_df\n",
    "#         print(complete_df.shape, complete_df.columns)\n",
    "\n",
    "    tfv.fit(list(complete_df[:].values))\n",
    "\n",
    "    if decom_flag is False:\n",
    "        train_tfv =  tfv.transform(train[col_name].values.ravel()) \n",
    "        test_tfv  = tfv.transform(test[col_name].values.ravel())\n",
    "\n",
    "        del complete_df\n",
    "        gc.collect()\n",
    "        return train_tfv, test_tfv, tfv\n",
    "    else:\n",
    "        complete_tfv = tfv.transform(complete_df[:].values.ravel())\n",
    "        \n",
    "        if which_method is 'svd':\n",
    "            svd = TruncatedSVD(n_components=n_component)\n",
    "            svd.fit(complete_tfv)\n",
    "            complete_dec = svd.transform(complete_tfv)\n",
    "        else:\n",
    "            nmf = NMF(n_components=n_component, random_state=1234, alpha=0, l1_ratio=0)\n",
    "            nmf.fit(complete_tfv)            \n",
    "            complete_dec = nmf.fit_transform(complete_tfv)            \n",
    "        \n",
    "        \n",
    "        complete_dec = pd.DataFrame(data=complete_dec)\n",
    "        complete_dec.columns = [which_method+'_'+str(i) for i in range(n_component)]\n",
    "\n",
    "        train_dec = complete_dec.iloc[:train.shape[0]]\n",
    "        test_dec = complete_dec.iloc[train.shape[0]:].reset_index(drop=True)\n",
    "\n",
    "        del complete_dec, complete_df\n",
    "        gc.collect()\n",
    "        return train_dec, test_dec, complete_tfv, tfv\n",
    "\n",
    "def countvect_feature(train, test, col_name, min_df=3, analyzer='word', token_pattern=r'\\w{1,}', ngram=3, stopwords='english', max_features=None):\n",
    "    \"\"\"return CountVectorizer feature\n",
    "    Args:\n",
    "        train, test: dataset\n",
    "        col_name: columns name of the text feature\n",
    "        min_df: if Int, then it represent count of the minimum words in corpus (remove very rare word)\n",
    "        analyzer: [‘word’, ‘char’]\n",
    "        ngram: max range of ngram\n",
    "        token_pattern: [using: r'\\w{1,}'] [by default: '(?u)\\b\\w\\w+\\b']\n",
    "        stopwords: ['english' or customized by remove specific words]\n",
    "        max_features: max no of features to keep, based on frequency. It will keep words with higher freq\n",
    "    return:\n",
    "        Count feature space of the text data, as well as its function instance\n",
    "    \"\"\"\n",
    "    ctv = CountVectorizer(min_df=min_df,  max_features=max_features, \n",
    "                strip_accents='unicode', analyzer=analyzer, \n",
    "                token_pattern=token_pattern, ngram_range=(1, ngram), \n",
    "                stop_words = stopwords)\n",
    "\n",
    "    complete_df = pd.concat([train[col_name], test[col_name]], axis=0)\n",
    "    ctv.fit(list(complete_df[:].values))\n",
    "\n",
    "    train_tf =  ctv.transform(train[col_name].values.ravel()) \n",
    "    test_tf  = ctv.transform(test[col_name].values.ravel())\n",
    "\n",
    "    del complete_df\n",
    "    gc.collect()\n",
    "    return train_tf, test_tf, ctv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidfs = []\n",
    "for ngram in [1,2,3,4,5]:\n",
    "    out_tfidfs.append(tfidf_feature(train, test, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidf1 = tfidf_feature(train, test, 'text', ngram=1)\n",
    "out_tfidf2 = tfidf_feature(train, test, 'text', ngram=2)\n",
    "out_tfidf3 = tfidf_feature(train, test, 'text', ngram=3)\n",
    "out_tfidf4 = tfidf_feature(train, test, 'text', ngram=5)\n",
    "\n",
    "out_vect1 = countvect_feature(train, test, 'text', ngram=1)\n",
    "out_vect2 = countvect_feature(train, test, 'text', ngram=2)\n",
    "out_vect3 = countvect_feature(train, test, 'text', ngram=3)\n",
    "out_vect4 = countvect_feature(train, test, 'text', ngram=5)\n",
    "\n",
    "print('tf-idf features: ', out_tfidf1[0].shape, \n",
    "     out_tfidf2[0].shape, out_tfidf3[0].shape, out_tfidf4[0].shape)\n",
    "print('count-vect features: ', out_vect1[0].shape, \n",
    "     out_vect2[0].shape, out_vect3[0].shape, out_vect4[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = train['topic'].astype('category').cat.codes\n",
    "train['target'] = train['target'].astype('int')\n",
    "all_class = list(train['target'].unique())\n",
    "print(len(all_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['target'] == 1].shape, out_tfidf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf1[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "# pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf1[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"tfidf1 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf2[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"tfidf2 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf3[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"tfidf3 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf4[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"tfidf4 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "print(\"==\"*25)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect1[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"count-vect1 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect2[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"count-vect2 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect3[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"count-vect3 : \", clf.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect4[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "clf = MultinomialNB().fit(X_train, Y_train)\n",
    "print(\"count-vect4 : \", clf.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "    C=0.01, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "    random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(logistic_reg.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf1[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"tfidf1 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf2[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"tfidf2 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf3[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"tfidf3 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_tfidf4[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"tfidf4 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "print(\"==\"*25)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect1[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"count-vect1 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect2[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"count-vect2 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect3[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"count-vect3 : \", logistic_reg.score(X_test, Y_test))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    out_vect4[0], train['target'], stratify=train['target'], test_size=0.3\n",
    ")\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(\"count-vect4 : \", logistic_reg.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('Dataset/Sample_Submission.csv')\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg.fit(out_vect3[0], train['target'])\n",
    "pred = logistic_reg.predict(out_vect3[1])\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def category_encoder(df)\n",
    "test.drop('topic', axis=1, inplace=True)\n",
    "test['target'] = pred\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['topic'] = test['target'].apply(lambda x: class_mapping_reverse[str(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('target', axis=1, inplace=True)\n",
    "# os.makedirs('submission')\n",
    "test.to_csv('submission/linear_model1.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping(df, col_name):\n",
    "    cat_codes = df[col_name].astype('category')\n",
    "    \n",
    "    class_mapping = {}\n",
    "    i = 0\n",
    "    for col in cat_codes.cat.categories:\n",
    "        class_mapping[col] = i\n",
    "        i += 1\n",
    "    \n",
    "    class_mapping_reverse = {}\n",
    "    for key, value in class_mapping.items():\n",
    "        class_mapping_reverse[value] = key\n",
    "\n",
    "    return class_mapping, class_mapping_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping, class_mapping_reverse = get_mapping(train, 'topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "params = {}\n",
    "params['alpha'] = 1\n",
    "passive_agg = PassiveAggressiveClassifier(C=params['alpha'], early_stopping=False, validation_fraction=0.3, n_iter_no_change=5, shuffle=True, verbose=0, n_jobs=-1, random_state=1234, loss='hinge', class_weight='balanced', average=False, n_iter=None)\n",
    "ridge_clf = RidgeClassifier(alpha=params['alpha'], fit_intercept=True, normalize=True, class_weight='balanced', random_state=1234)\n",
    "logistic_reg = LogisticRegression(penalty='l2', dual=False, C=params['alpha'], fit_intercept=True, intercept_scaling=1, class_weight='balanced', random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_clf.fit(X_train, Y_train)\n",
    "print(ridge_clf.score(X_test, Y_test))\n",
    "print(\"==============\")\n",
    "\n",
    "passive_agg.fit(X_train, Y_train)\n",
    "print(passive_agg.score(X_test, Y_test))\n",
    "print(\"==============\")\n",
    "\n",
    "logistic_reg.fit(X_train, Y_train)\n",
    "print(logistic_reg.score(X_test, Y_test))\n",
    "print(\"==============\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for alpha in [0.01, 0.1, 0.5, 1, 5, 10]:\n",
    "    logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "        C=alpha, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "        random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "    logistic_reg.fit(X_train, Y_train)\n",
    "    pred = logistic_reg.score(X_test, Y_test)\n",
    "    print(pred)\n",
    "#     print(f1_score(ts_y, pred, average='micro', sample_weight=None))\n",
    "#     print(logistic_reg.score(ts_x, ts_y))\n",
    "print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for alpha in np.linspace(0.0001,0.1,10):\n",
    "    logistic_reg = LogisticRegression(penalty='l2', dual=False, \n",
    "        C=alpha, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "        random_state=1234, max_iter=100, multi_class='warn', verbose=0, n_jobs=-1)\n",
    "    logistic_reg.fit(X_train, Y_train)\n",
    "    pred = logistic_reg.score(X_test, Y_test)\n",
    "    print(alpha, \" : \", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Usage: plot_document_classification_20newsgroups.py [options]\n",
    "\n",
    "Options:\n",
    "  -h, --help            show this help message and exit\n",
    "  --report              Print a detailed classification report.\n",
    "  --chi2_select=SELECT_CHI2\n",
    "                        Select some number of features using a chi-squared\n",
    "                        test\n",
    "  --confusion_matrix    Print the confusion matrix.\n",
    "  --top10               Print ten most discriminative terms per class for\n",
    "                        every classifier.\n",
    "  --all_categories      Whether to use all categories or not.\n",
    "  --use_hashing         Use a hashing vectorizer.\n",
    "  --n_features=N_FEATURES\n",
    "                        n_features when using the hashing vectorizer.\n",
    "  --filtered            Remove newsgroup information that is easily overfit:\n",
    "                        headers, signatures, and quoting.\n",
    "\n",
    "Loading 20 newsgroups dataset for categories:\n",
    "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "data loaded\n",
    "2034 documents - 3.980MB (training set)\n",
    "1353 documents - 2.867MB (test set)\n",
    "4 categories\n",
    "\n",
    "Extracting features from the training data using a sparse vectorizer\n",
    "done in 0.412178s at 9.655MB/s\n",
    "n_samples: 2034, n_features: 33809\n",
    "\n",
    "Extracting features from the test data using the same vectorizer\n",
    "done in 0.351330s at 8.162MB/s\n",
    "n_samples: 1353, n_features: 33809\n",
    "\n",
    "================================================================================\n",
    "Ridge Classifier\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "RidgeClassifier(solver='sag', tol=0.01)\n",
    "train time: 0.132s\n",
    "test time:  0.001s\n",
    "accuracy:   0.896\n",
    "dimensionality: 33809\n",
    "density: 1.000000\n",
    "\n",
    "\n",
    "================================================================================\n",
    "Perceptron\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "Perceptron(max_iter=50)\n",
    "train time: 0.017s\n",
    "test time:  0.002s\n",
    "accuracy:   0.888\n",
    "dimensionality: 33809\n",
    "density: 0.255302\n",
    "\n",
    "\n",
    "================================================================================\n",
    "Passive-Aggressive\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "PassiveAggressiveClassifier(max_iter=50)\n",
    "train time: 0.031s\n",
    "test time:  0.002s\n",
    "accuracy:   0.904\n",
    "dimensionality: 33809\n",
    "density: 0.694674\n",
    "\n",
    "\n",
    "================================================================================\n",
    "kNN\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "KNeighborsClassifier(n_neighbors=10)\n",
    "train time: 0.002s\n",
    "test time:  0.317s\n",
    "accuracy:   0.858\n",
    "\n",
    "================================================================================\n",
    "Random forest\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "RandomForestClassifier(n_estimators=100)\n",
    "train time: 1.671s\n",
    "test time:  0.071s\n",
    "accuracy:   0.840\n",
    "\n",
    "================================================================================\n",
    "L2 penalty\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "LinearSVC(dual=False, tol=0.001)\n",
    "train time: 0.145s\n",
    "test time:  0.002s\n",
    "accuracy:   0.900\n",
    "dimensionality: 33809\n",
    "density: 1.000000\n",
    "\n",
    "\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "SGDClassifier(max_iter=50)\n",
    "train time: 0.030s\n",
    "test time:  0.002s\n",
    "accuracy:   0.902\n",
    "dimensionality: 33809\n",
    "density: 0.579380\n",
    "\n",
    "\n",
    "================================================================================\n",
    "L1 penalty\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "LinearSVC(dual=False, penalty='l1', tol=0.001)\n",
    "train time: 0.301s\n",
    "test time:  0.002s\n",
    "accuracy:   0.873\n",
    "dimensionality: 33809\n",
    "density: 0.005553\n",
    "\n",
    "\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "SGDClassifier(max_iter=50, penalty='l1')\n",
    "train time: 0.093s\n",
    "test time:  0.002s\n",
    "accuracy:   0.887\n",
    "dimensionality: 33809\n",
    "density: 0.022901\n",
    "\n",
    "\n",
    "================================================================================\n",
    "Elastic-Net penalty\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "SGDClassifier(max_iter=50, penalty='elasticnet')\n",
    "train time: 0.252s\n",
    "test time:  0.002s\n",
    "accuracy:   0.899\n",
    "dimensionality: 33809\n",
    "density: 0.187472\n",
    "\n",
    "\n",
    "================================================================================\n",
    "NearestCentroid (aka Rocchio classifier)\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "NearestCentroid()\n",
    "train time: 0.004s\n",
    "test time:  0.002s\n",
    "accuracy:   0.855\n",
    "\n",
    "================================================================================\n",
    "Naive Bayes\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "MultinomialNB(alpha=0.01)\n",
    "train time: 0.003s\n",
    "test time:  0.001s\n",
    "accuracy:   0.899\n",
    "dimensionality: 33809\n",
    "density: 1.000000\n",
    "\n",
    "\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "BernoulliNB(alpha=0.01)\n",
    "train time: 0.004s\n",
    "test time:  0.003s\n",
    "accuracy:   0.884\n",
    "dimensionality: 33809\n",
    "density: 1.000000\n",
    "\n",
    "\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "ComplementNB(alpha=0.1)\n",
    "train time: 0.004s\n",
    "test time:  0.001s\n",
    "accuracy:   0.911\n",
    "dimensionality: 33809\n",
    "density: 1.000000\n",
    "\n",
    "\n",
    "================================================================================\n",
    "LinearSVC with L1-based feature selection\n",
    "________________________________________________________________________________\n",
    "Training:\n",
    "Pipeline(steps=[('feature_selection',\n",
    "                 SelectFromModel(estimator=LinearSVC(dual=False, penalty='l1',\n",
    "                                                     tol=0.001))),\n",
    "                ('classification', LinearSVC())])\n",
    "train time: 0.252s\n",
    "test time:  0.002s\n",
    "accuracy:   0.880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_train[i] # feature count vector for training case i\n",
    "# y_train[i] # label for training case i\n",
    "\n",
    "# The count vectors are defined as:\n",
    "\n",
    "# p = sum of all feature count vectors with label 1\n",
    "\n",
    "# p = tf_train[y_train==1].sum(0) + 1\n",
    "\n",
    "# q = sum of all feature count vectors with label 0\n",
    "\n",
    "# q = tf_train[y_train==0].sum(0) + 1\n",
    "\n",
    "# Notice that we add 1 to both count vectors to ensure that every token appear at least one time in each class.\n",
    "\n",
    "# The log-count ratio r is:\n",
    "\n",
    "# r = np.log((p/p.sum()) / (q/q.sum()))\n",
    "\n",
    "# And b:\n",
    "\n",
    "# b = np.log(len(p) / len(q))\n",
    "\n",
    "# Just the ratio of number of positive and negative training cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_preds = tf_test @ r.T + b\n",
    "preds = pre_preds.T > 0\n",
    "accuracy = (preds == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidf1[0][train['target'] == 5].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidf1[0][train['target'] == 4].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.equal(out_tfidf1[0][train['target'] == 5].sum(0), \n",
    "                out_tfidf1[0][train['target'] == 3].sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidf1[0][train['target'] == 5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidf1[0][train['target'] == 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = train[train['target'] == 2].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tfidf1[0][idx].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts):\n",
    "    output = []\n",
    "    for i in texts:\n",
    "        s = [token.lemma_ for token in nlp(i)]\n",
    "        output.append(' '.join(s))\n",
    "    return output\n",
    "\n",
    "# train['text1'] = train['text'].progress_apply(lemmatization)\n",
    "# test['text1']  = test['text'].progress_apply(lemmatization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.append(test, ignore_index=True)\n",
    "df['text1'] = df['text'].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:5]['text'].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(text):\n",
    "    return []\n",
    "    for i in texts:\n",
    "        s = [token.lemma_ for token in nlp(i)]\n",
    "        output.append(' '.join(s))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenize(df1['text'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "porter.stem('helping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in df1['text'][1].split(\" \"):\n",
    "    print(porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del df1, df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.append(test, ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "cv = CountVectorizer(max_df=0.95,min_df=2,stop_words='english')\n",
    "term_matrix = cv.fit_transform(df['text'])\n",
    "# print(term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, n_jobs=4)\n",
    "lda.fit(term_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = lda.components_[0]\n",
    "top_words_indices = topic.argsort()[-10:]\n",
    "for index in top_words_indices:\n",
    "    print(cv.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_dict = {}\n",
    "for index,topic in enumerate(lda.components_):\n",
    "    words = [cv.get_feature_names()[i] for i in topic.argsort()[-10:]]\n",
    "    topic_word_dict[index] = words\n",
    "    print('Top words for topic {}'.format(index))\n",
    "    print(words)\n",
    "    print('-'*120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics = lda.transform(term_matrix)\n",
    "data['topic'] = topics.argmax(axis=1)\n",
    "\n",
    "\n",
    "def assign_topics(row):\n",
    "    topic = row['topic']\n",
    "    words = topic_word_dict[topic]\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "data['topic words'] = data.apply(assign_topics,axis=1)\n",
    "print(data.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
